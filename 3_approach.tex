\section{Approach}
In the following section, we illustrate our real-world use case and the scheduling problem formulation, as well as the data set were are working with. Then, we state our xAI workflow with the aim to fill the research gap.

\subsection{Preliminaries}
To gain a deeper understanding of our use case, we now formulate the scheduling problem and the MDP to address it.

\subsubsection{Real-World Case and Scheduling Problem Formulation}
\begin{figure}[ht!]
    \centering
    \includegraphics[width=0.77\textwidth]{pictures/simulationmodel2_plain.pdf}
    \caption{Model of the considered two-stage flow production system by \captionciteA{müller2024reinforcement}.}
    \label{fig:sim_model}
\end{figure}
\FloatBarrier

The systematic application of xAI techniques is examined in the context of a real-world use case at a large German manufacturer of household appliances. In previous publications, an extensive scheduling model, along with a specialized DRL agent, has been investigated, implemented and tuned \shortcite{müller2024reinforcement,müller2024smaller}. The considered manufacturing process involves a two-stage flow production system. As visualized in Figure \ref{fig:sim_model}, the shopfloor consists of a pre-assembly stage (PAS) and a final assembly stage (FAS). In the PAS, a single station produces eight types of semi-finished products (hereinafter referred to as products), which are then finished at one of four possible FAS stations. Between these two stages, there is a limited intermediate buffer where the products are temporarily stored. According to \citeA{müller2024smaller}, the key model components of the extended permutation flow shop can be defined semi-formally as outlined below. For a comprehensive mathematical description, refer to \citeA{müller2024reinforcement}.

\begin{itemize}
    \item Given a set of specific products to be finalized in the FAS, each based on a standardized product of eight different types produced in the PAS.
    \item All PAS and FAS stations can process only one product at a time and all processing times are deterministic.
    \item Each product must be finalized at exactly one of four FAS stations. In this context, each FAS has a predefined schedule containing the sequence of single products to be manufactured.
    \item A FAS station can only start finishing the product when it is available in the central buffer. If the next required product is not available in the central buffer, the FAS station will incur idle times until it becomes available.
    \item In the PAS, all products required by the FAS that are not initially available in the central buffer must be produced.
    \item Switching from one product type to another may cause sequence-dependent setup efforts in the PAS.
\end{itemize}

These constrains can be summarized as follows:

\begin{table}[ht!]
    \footnotesize
    \centering
    \caption{System Constraints of the Production System}
    \label{t:system_constraints}
    \begin{tabularx}{\textwidth}{lX}
        \toprule
        \textbf{Component}     & \textbf{Description}                             \\
        \midrule
        PAS                    & Produces 8 types of semi-finished products       \\
        FAS                    & 4 stations for finishing products                \\
        Buffer                 & Intermediate storage between PAS and FAS         \\
        Processing Time        & Deterministic                                    \\
        Conflicting Objectives & Minimize idle time (FAS) and setup efforts (PAS) \\
        \bottomrule
    \end{tabularx}
\end{table}
\FloatBarrier


Given these constraints, two conflicting objectives are optimized in lexicographical order: first, minimizing idle times in the FAS, and second, minimizing setup efforts in the PAS. This problem is addressed by determining a central decision variable: the sequence in which products are loaded into the production system in the PAS. This decision is managed by the existing agent, which will be described in detail in the following section.

\subsubsection{Existing RL Approach}
The real world setting stated above needs to be formalized. Several approaches are feasible here. For example, the environment could be modelled using operations research methods. Because of RL-agent's abilities to efficiently find local optima, we chose DRL. The scheduling problem is modelled as an MDP as presented below. For a detailed explanation, refer to \cite{müller2024reinforcement}.

\paragraph{State Space}
The state space is encoded as a vector and comprises following elements:
\begin{itemize}
    \item \textbf{next\_24h\_demand\_prod} for all products: Specifies the demand of the related product for the next 24 hours, taking into account the amount left in the buffer.
    \item  \textbf{end\_of\_planning\_period\_demand} for all products: Specifies the demand of the related product for the planning period left, taking into account the amount left in the buffer.
    \item \textbf{buffer\_content\_duration\_prod} for all products: Specifies how long the amount of an product type in the buffer will suffice to meet the demands of the FAS if this type is no longer produced in the PAS.
    \item  \textbf{buffer\_fill\_level}: Specifies the fill level of the buffer, i.e., the amount of all products in relation to its total capacity.
    \item  \textbf{last\_prod\_type\_is} for all products: Specifies the last type of products manufactured, represented in one-hot encoding.
\end{itemize}

\paragraph{Action Space}
We define the action space to be discrete. It consists of 8 actions representing the 8 different proudct types with 0 centered start (e.g., action 7 = agent recommends to build product 8). Each time the agent selects an action, a lot of 50 units of the corresponding product type is produced.

\paragraph{Reward Function}
The primary objective of the agent is to avoid idle times. Rather than penalizing idle times directly, we penalize \textit{criticality}, which we define to be the ratio of $\texttt{next\_24h\_demand\_prod}$ to $\texttt{buffer\_content\_duration\_prod}$. This has proven to be significantly more effective for training, as criticality provides a richer learning signal compared to idle times \cite{müller2024reinforcement}. Furthermore, we penalize the agent if $\texttt{buffer\_content\_duration\_prod}$ for any product type falls under a threshold of 30 minutes, which encourages the agent to maintain a certain margin for each required product type.
The other objective is to minimize setup efforts. Therefore, we add setup efforts weighted by a factor directly to the reward function. We refer to \cite{müller2024reinforcement} for a detailed explanation of the reward function.

\paragraph{Domain Randomization}
To make the agent decisions more robust, domain randomization (DR) has been used \shortcite{müller2023bridging}.
DR consists of the idea to present not one but many environment distributions to the agent at each episode. The agent is forced to adapt to a wider spectrum of scenarios, making its policy more robust.
In the concrete use case, six different weeks (environments) were presented to the agent at random. Each week had distinct characteristics regarding demand and buffer sizes.

\paragraph{Dataset}
The agent made scheduling decisions based on representative synthetic data that were carefully designed to reflect realistic scenarios. These data were obfuscated to preserve the integrity of the study and protect the interests of the project partner, ensuring no sensitive or proprietary information was compromised. After giving the input values, which consist of checkpoints for the weeks, the agent and environment was initialized and while the agent acted in the environment, we saved the state and action pairs. The generated data frame consists of 103 rows and 35 columns, corresponding to the actions of the agent and the state space.
We started our investigations with an already trained NN; therefore, we consider the whole dataset as a test set. Thus, we achieve the same attribution values for each run through the data in Input X Gradient. Settings where the network is trained again on new data may lead to the necessity to use cross validation and to fit confidence intervals for the attribution values to achieve robustness.
We want to note that we had to rebuild the NN, because the original was not available to use in the xAI methods due to it being a custom class.
Now, how can we systematically explain the data of the agent from our use case using xAI?

\subsection[Domain-knowledge hypotheses]{Domain-Knowledge Hypotheses to combine Falsification, Interpretation and Communication of xAI}
Many xAI efforts, particularly in xRL, provide only zero-order explanations, with few utilizing an agent's objectives or dispositions to create causal explanations \shortcite{dazeley2021levels, dazeley2023explainable, kim2021multi}. Furthermore, domain knowledge is often inadequately integrated in manufacturing contexts (RQ 2.2) \shortcite{chen2023explainable}. Our aim is to leverage the reward function alongside domain knowledge \shortcite{mohseni202124, kim2021multi, heuillet2021explainability, dwivedi2023explainable, milani2024explainable} to develop 'higher-order' explanations \shortcite{dazeley2023explainable}. Following \citeA{palacio2021xai}, we view the causal context and the rationale behind an AI system's behavior as part of the interpretation rather than the explanation itself. Additionally, we emphasize the significance of falsification in xAI \shortcite{leavitt2020towards}.
We hypothesize that these aspects may be combined. To ensure user trust and make explanations interpretable, it is essential to incorporate context and knowledge of the agent's workings \shortcite{dazeley2023explainable, mohseni202124}, while also considering the target audience \shortcite{mohseni202124, kim2021multi, heuillet2021explainability, dwivedi2023explainable, chen2023explainable}. Crucially, we can compile this information before generating an explanation: if we understand what a trained agent should do in a specific use case based on domain knowledge and the agent's objectives (decomposing the reward into meaningful statements), we can formulate a hypothesis \shortcite{mohseni202124, dazeley2023explainable, leavitt2020towards, milani2024explainable}. For instance, one might hypothesize: \textit{In the production line, it is important to maintain buffer levels. An agent is penalized if the buffer content is low; therefore, when buffer levels are low, the agent should produce more products to replenish it.} This hypothesis can inform explanations derived from xAI methods, such as those indicating the significance of specific features in the agent's behavior. If the hypothesis aligns with the explanation, it suggests that the AI system is functioning as expected, thereby generating a comprehensible interpretation involving context and causality. Conversely, if discrepancies arise, it signals an error that necessitates revision of the AI system or further contextual integration. This approach combines falsification through hypotheses \shortcite{leavitt2020towards} with causal interpretation \shortcite{palacio2021xai, kim2021multi} and enhances trust and acceptance \shortcite{dazeley2023explainable}. Additionally, we incorporate the domain knowledge of expert users \shortcite{mohseni202124, kim2021multi, heuillet2021explainability, dwivedi2023explainable, milani2024explainable}. This specific combination represents a novel approach for xRL in real-world business contexts. Our goal is to develop an xAI framework for business and logistics, exemplified through production scheduling, allowing experts to quickly ascertain whether the AI system's outputs align with their domain knowledge.

\subsection{The Choice of a Systematic Workflow}
\label{systematic workflow}
We adopt the systematic workflow outlined by \citeA{tchuente2024methodological}, which provides a robust structure for empirical investigations involving xAI in business applications. This framework guides the application of xAI methods to generate explanations for DRL agents in production scheduling (RQ 1.1), and our contribution enhances it through hypotheses-based validation, ensuring systematic verification of explanations that combine domain expertise with xAI methods.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\linewidth]{pictures/Tchunete_Framework.png}
    \caption{Framework for xAI in business by \citeA{tchuente2024methodological}. Idea, data, and context are presented as three clusters.}
    \label{fig:tchuente}
\end{figure}
\FloatBarrier

\begin{enumerate}
    \item \textbf{Business Question Identification and Importance Scoring}: The workflow starts with identifying the business question and scoring its importance to stakeholders. Techniques are employed to determine target variables, ensuring collaboration among stakeholders.

    \item \textbf{Data Collection}: After clarifying objectives and variables, data collection begins. Data engineering enhances this process through batch data storage and pipeline creation for real-time availability.

    \item \textbf{Data Preprocessing and Feature Engineering}: Addressing missing values is critical before feeding data into the model \shortcite{tchuente2024methodological}. The pipeline should be transparent and well-documented to facilitate understanding of data-related decisions that impact model results.

    \item \textbf{Fitting and Validation of the Model}: Choosing the right model and effectively communicating this choice is crucial. Evaluating model precision is essential, as misconceptions may arise if stakeholders are not engaged with technical details \shortcite{nyawa2023transparent}. A suitable model or ensemble must be selected, with clear explanations of limitations and interpretability.

    \item \textbf{Testing the Model}: The model is tested on a designated test set, ensuring that training, validation, and test segments remain representative.

    \item \textbf{Explanation of Model Outputs}: The modeller selects an explanation method; this step can be bypassed for whitebox models. Various methods exist for explaining model results, and the chosen approach should align with stakeholder objectives. Feedback from domain experts may verify whether results are sufficient for progression \citeA{doshi_velez2017towards}.

    \item \textbf{Robustness Checking}: Initial explanations should undergo verification to ensure consistency across different approaches \shortcite{slack2019fooling}. Validating consistency may involve comparing explanations and assessing the explainer's fidelity to the original model's behavior \shortcite{chi2022quantitative}.

    \item \textbf{Validation of Explanations}: Finally, robust explanations must be validated by domain experts to ensure practical applicability. Given the potential for concept drift to alter model assumptions over time, these last two steps should be iterated regularly.
\end{enumerate}
