\chapter{Introduction}
\label{chap:introduction}

\section{Initial Situation}
The \gls{dt} is a key technology at the forefront of the fourth industrial revolution, often referred to as Industry 4.0.
The latter is characterized by the integration of \gls{cps}, the \gls{iot}, and cloud computing to create smart factories aimed at automation and efficiency \autocite{Oztemel2020}. Companies pursue this vision by trying to remain competitive through the adoption of innovative technologies that promise enhanced productivity and reduced operational costs. One such technology that supports this transformation is the \gls{dt}. It can be defined as a virtual representation of physical assets enabling real-time monitoring and optimization \autocite{Tao2018ijamt}. The \gls{dt} bridges the connection between the two entities with a bidirectional data flow to exchange information and to influence the behaviour of the physical asset \autocite{grieves2014digital}. This technology is central to Industry 4.0, facilitating the connection of physical and digital worlds through real-time data integration, simulation, and optimization \autocite{judijanto2024trends}.

Although this research area is rapidly evolving, a unified definition of \gls{dt} has yet to be established due to the diverse requirements and perspectives across different fields. In engineering, the focus might be on the real-time interaction between physical systems and their digital counterparts, whereas in computer science, the emphasis is often on data integration and simulation capabilities. These varying priorities result in multiple interpretations and applications of the term \gls{dt}. The concept was first introduced by Michael Grieves in 2002, who defined it as a digital representation of a physical object or system \autocite{grieves2014digital}. However, the concept has evolved since, encompassing a broader range of applications and technologies. In the literature, three terms are used to describe similar characteristics of \gls{dt}: \gls{dm}, \gls{ds}, and Digital Twin (\gls{dt}), see \autoref{fig:Kritzinger} \autocite{jones2020characterising,Zhang2021jmsy}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/kritzinger.png}
  \caption[The different Types of Digital Models.]{Comparison of Digital Shadow (\gls{ds}), Digital Model (\gls{dm}) and Digital Twin (\gls{dt}) as presented by Kritzinger (2018). This distinction is crucial for understanding validation requirements across different digital representation types.}
  \label{fig:Kritzinger}
  \caption*{Illustration based on \textcite{kritzinger2018digital} and \textcite{Zhang2021jmsy}}
\end{figure}

The Digital Model (\gls{dm}) represents the most basic form of \gls{dt}. It involves manual data connections between physical and digital entities. These connections can be temporarily shifted or even disconnected. There is no direct control of the digital object over the physical entity. It is primarily a simple or complex model \textit{describing} the physical object. The data flow must be manually triggered by the modeler, who also interprets the results and controls the \gls{dm}.
The Digital Shadow (\gls{ds}) is a more advanced version of the \gls{dm}. It is a digital representation of the physical object that is continuously updated with real-time data, allowing for monitoring, analysis and simulation. While it can predict future states of the physical object based on the current state and historical data, it is unable to influence the physical object without human intervention. The control is, similar to the \gls{dm}, still in the hands of the modeller. A \gls{ds} is frequently used for simulation purposes and is sometimes misclassified as a \gls{dt} in the literature \autocite{kritzinger2018digital,sepasgozar2021differentiating}.
The \gls{dt} is the most advanced version of the three. It offers a digital representation of the physical object, which is also continuously updated with real-time data. The \gls{dt} can be used for monitoring, analysis, and \textit{control} purposes. It can predict the future states of the physical object based on the current state and historical data. The \gls{dt} can also influence the physical object by sending control signals to it. The control is partially or completely in the hands of the \gls{dt}. Thus, the \gls{dt} \textit{can} serve more purposes than modelling or simulating the physical object. It may serve as an autonomous system, updating itself or with minimal human intervention \autocite{kritzinger2018digital}.

\gls{dt}s are applied across various sectors, including manufacturing, defence, automotive, service, finance and healthcare \autocite{Tao2018ijamt}. Manufacturing is particularly notable due to its high potential for process optimization and automation \autocite{Tao2018ijamt}. This thesis focuses on the latter, particularly \gls{dmfs}. These systems process discrete objects (parts) moving along transportation routes or conveyor lines at regular or irregular intervals, integrating both production and logistics operations \autocite{arnold2005materialfluss, schwede2024learning}. A key simplification in their modelling is the abstraction of material flow as a sequence of discrete events, following the principles ofv\gls{des} \autocite{kovacs2016mathematical, robinson2014simulation}. \gls{des} is well-suited for analysing complex systems where state changes occur at discrete points in time, such as arrivals, departures, and processing steps \autocite{robinson2014simulation}.

Historically, \gls{dm} played a crucial role in the design, planning, and control of \gls{dmfs}, primarily through applications like material flow simulations, logistic assistance systems, and digital factory implementations \autocite{Thiede2013}. However, advancements in both \gls{ds} and \gls{dt} have enabled a shift from isolated, use-case-specific models toward digital representations that span the entire lifecycle of \gls{dmfs} \autocite{Abdoune2023}. This transition is largely driven by the growing demand for predictive capabilities by stakeholders and automated decision support in manufacturing systems,
reflecting the core principles of Industry 4.0 \autocite{frank2019industry}. A second driver of \gls{dt} innovation lies in the widely available data from \gls{iot} devices and sensors, which enhances model training and real-time adaptation of \gls{dt}s \autocite{Tao2018ijamt}.

In practice, the automated data transfer between the digital model and the physical system is not always critical for \gls{dmfs} management. Unlike in time-sensitive applications, human decision-makers often remain integral to the control loop, meaning that real-time automation is not always necessary \autocite{schwede2024learning}. \gls{ds} and \gls{dt}s are thus treated as equivalent concepts.

Beyond replicating the current state and managing historical data, \gls{dt}s are essential for predicting system behaviour and evaluating potential modifications. The widespread use of \gls{des} within \gls{dt} modelling highlights the central role of simulation-based \gls{dt}s (\gls{sbdt}s) in \gls{dmfs} \autocite{Lugaresi2021aifac}. As \textcite{schwede2024learning} emphasize, \gls{sbdt}s provide decision support for optimizing costs and performance in highly competitive manufacturing environments. Current \gls{sbdt}s are primarily developed and updated laboursome manually by domain experts. Emerging research explores how \gls{ml} can enhance predictive accuracy and automate model updates by automatically learning model characteristics, thus reducing costs and development time.

The progression from digital models to simulation-based \gls{dt}s reflects an ongoing shift toward data-driven, predictive, and increasingly automated representations of \gls{dmfs}, enabling more informed decision-making throughout the system's lifecycle \autocite{boschert2016digital,lim2020state}.

\section{Problem}
\label{sec:problem}
Despite the transformative potential of \gls{dt}s, their implementation can be challenging. Creating and maintaining accurate \gls{dt}s requires substantial investments in technology and domain knowledge. This investment is wasted if the resulting model fails to accurately represent the physical entity. While automatic generation may seem like an elegant solution, it carries the risks of overfitting or biased predictions \autocite{gemanbias}. Manufacturing data for training must be rigorously cleaned and preprocessed to meet this challenge. Automatically generated \gls{dt}s must also undergo automatic \gls{vvuq} to preserve their cost and time advantages. Manual \gls{vvuq}, which relies on humans in the loop, hinders scalability, automatic synchronization with the physical entity, and depends on costly domain knowledge often provided by experts \autocite{Bitencourt2023}. These hurdles are significant barriers to automatic learning \autocite{ribeiro2016should,zhao2024data}. As industries integrate \gls{dt} into their production processes, establishing trust becomes fundamental as well \autocite{trauer2022digital,arrieta2020explainable}. For widespread acceptance among co-workers, stakeholders, and investors, automatic \gls{dt} creation and \gls{vvuq} must demonstrate clear advantages over manual creation and expert-led \gls{vvuq}. Even when \gls{dt} learning is successfully performed, questions about its correctness, precision, and robustness persist. These concerns are addressed by \gls{vvuq} frameworks \autocite{sel2025survey}. Ensuring the validity, reliability, and accuracy of a \gls{dt} is critical, yet traditional \gls{vvuq} approaches rely heavily on manual expert involvement and case-specific reference values \autocite{Bitencourt2023,hua2022validation}. This leads to inefficiencies, particularly in the context of automated \gls{dt} generation, where such manual processes undermine the goal of reducing development effort. \textcite{hua2022validation} even argue that there are no robust and standardized verification and validation (\gls{vv}) methods for \gls{dt}s. As \textcite{sel2025survey} point out, uncertainty quantification is often overlooked, but addresses an important aspect of assessing low noise in model predictions. One hurdle to standardized \gls{vvuq} frameworks is the lack of a clear definitions for validity and verification in the context of \gls{dt}s \autocite{Bitencourt2023}.

For \gls{dmfs}, these challenges are even more pressing due to their procedural nature and inherent stochasticity. Rigorous \gls{vvuq} is essential to address the risk of manufacturing process failures caused by anomalies, resource constraints, software faults, or human error. This necessity arises because such failures can disrupt the intricate workflows and unpredictable dynamics inherent in \gls{dmfs}, making reliable performance prediction a priority. When \gls{dt}s for these systems are generated automatically, traditional validation methods become problematic, as they negate much of the efficiency gains through automation. This creates a fundamental conflict: while automated \gls{dt} generation reduces initial development and updating efforts, it simultaneously increases the complexity of validation and verification, potentially counteracting its intended efficiency gains.

\section{Objective}

This thesis addresses this conflict by developing a data-driven framework for automated \gls{vvuq} of automatically generated \gls{sbdt}s that have been learned from data. The focus lies on \gls{dmfs} due to their practical relevance and dynamical, procedural nature. The research can further be specified by the following \gls{rq}:

\begin{itemize}
  \label{par:rq1}
  \item \textbf{\gls{rq}1:} How can automated validation and verification processes for \gls{dt}s be efficiently implemented to maintain accuracy?
        \label{par:rq2}
  \item \textbf{\gls{rq}2:} Which data-driven approaches are best suited to identify discrepancies between simulated behaviour and real operational data in discrete material flow systems?
        \label{par:rq3}
  \item \textbf{\gls{rq}3:} To what extent does the developed framework improve the quality and reliability of \gls{dt}s compared to traditional \gls{vv} methods?
\end{itemize}

This thesis proposes that object-centric event logs—commonly used to generate \gls{dt}s in manufacturing—can also serve as the foundation for an automated, use-case-independent validation and verification framework. Such an approach would preserve the efficiency benefits of automated generation while ensuring that the resulting \gls{dt}s meet necessary standards. A key aspect of this approach is the development and monitoring of generic, statistically grounded reference values, which must be quantifiable and have an underlying distribution. The framework will be evaluated using a case study from the discrete material flow domain, providing empirical evidence of its effectiveness in improving model accuracy and efficiency.

\section{Structure and Methodology}

This thesis is organized as follows: \autoref{chap:theory} establishes the theoretical background on \gls{dmfs}, \gls{sbdt}s, Process Mining, and \gls{vvuq}. \autoref{chap:methodology} details the development methodology for the automated \gls{vvuq} framework, including requirements and the \gls{ml}-based validation strategy. \autoref{chap:implementation} describes the technical implementation and system architecture. \autoref{chap:case-study} presents the empirical validation using the \gls{iot} Factory case study, including main results and sanity checks. Finally, \autoref{chap:discussion} discusses the findings and their implications, while \autoref{chap:conclusion} summarizes the key contributions and outlines future research directions.

The thesis follows a Design Science Research approach (DSR). This approach is characterized by the development of artifacts to solve practical problems \autocite{hevner2004design,peffers2007design}. In the sense of DSR, artifacts are created objects or constructs which address the given problem and contribute to both theory and practice. The artifacts are evaluated in a real-world context to demonstrate their effectiveness. The thesis in particular applies the cyclical DSR model (see \autoref{fig:DSR}).

The research paradigm of the thesis is a deductive-theory critical approach \autocite{eberhard1987einfuhrung}. A conceptual \gls{vvuq} framework is developed based on existing theoretical foundations, while deriving new requirements through a requirements analysis. The framework is then applied in a case study to evaluate its effectiveness. The research is critical because it aims to improve the efficiency and effectiveness of \gls{vvuq} for automatically generated \gls{dt}s. Elements of empirical research are included through the case study and the data-driven approach.

