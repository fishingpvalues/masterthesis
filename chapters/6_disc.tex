\chapter{Discussion}
\label{chap:discussion}

This chapter critically examines the findings from the empirical validation of the framework presented in \autoref{chap:case-study}, connecting these results to the broader theoretical foundations established earlier in the thesis, see \autoref{chap:theory}. The IoT Factory case study provided valuable insights into the effectiveness of the machine learning-based approach to VVUQ of simulation-based digital twins, revealing both strengths and limitations of the proposed methodology.

\section{Comparison with Traditional VVUQ Methods}
\label{sec:comparison_manual}

Traditional VVUQ practices, as discussed earlier, distinguish between verification (ensuring the model is built correctly according to specifications) and validation (ensuring the model is an accurate representation of the real system for the intended purpose). Key techniques mentioned included code inspection, debugging, unit tests, statistical tests and sensitivity analysis, see \autoref{sec:ml-approaches}. While essential, applying these techniques comprehensively to complex SBDTs faces several challenges, also highlighted previously.

The proposed ML-based validation framework, utilizing classifiers and permutation testing on event log data, primarily addresses the validation aspect by comparing simulated outputs against real-world operational data. It aims to overcome some of the traditional challenges:

\begin{itemize}
  \item \textbf{Effort and Scalability:} Traditional validation relying on expert consultation, manual log comparison, or setting up numerous simple experiments is often highly labour-intensive, see \autoref{sec:problem}. This manual effort negates some efficiency gains expected from SBDTs, especially if they are automatically generated or frequently updated. The ML approach, while requiring initial setup (data pipeline, feature engineering, model training), automates the core comparison task. Re-running the validation on new data or model versions requires mainly computational resources, addressing the identified key challenge scalability better than manual review which is hard to scale up.

  \item \textbf{Scope, Depth, and Objectivity:} Comparing aggregate KPIs via statistical tests or assessing face validity through expert consultation may fail to capture subtle discrepancies in process dynamics or interactions within the SBDT. The ML approach analyses event data (\autoref{sec:data-pipeline}) and potentially complex feature interactions (\autoref{sec:feature-engineering}), offering a deeper insight into the models dynamic fidelity. The whitebox results (\autoref{sec:results-whitebox}), where the DTree identified statistically significant discrepancies ($RR=1.00$) across all components based on different feature sets, provide empirical evidence (\autoref{sec:results-whitebox}) for this potential depth. Furthermore, the use of permutation testing (\autoref{sec:permtest}) yields objective p-values, contrasting with the subjectivity in expert consultation.

  \item \textbf{Applicability to Black-Box Models:} Techniques like code inspection are difficult for complex simulation platforms. The ML-based approach treats the SBDT primarily as a generator of output data. As long as comparable event logs can be extracted from the simulation and the real system, the validation method can be applied, mitigating the challenge of limited applicability to black-box models. This meets the key challenge of applicability.

  \item \textbf{Continuous Validation:} Manual validation activities are often performed periodically. The automated nature of the ML pipeline lents itself better to more frequent or even continuous validation as new real-world data becomes available or the SBDT is updated, addressing a key limitation of traditional batch validation processes. The key requirement of concurrent validation is fulfilled.

  \item \textbf{Integration:} While integrating any V\&V process presents challenges, embedding an automated data pipeline and ML testing framework might face different integration challenges compared to scheduling manual reviews or setting up traditional statistical reporting. However, once integrated, it can potentially offer more seamless feedback.
\end{itemize}

It is crucial to note that the proposed ML-based validation method primarily focuses on assessing the SBDT's fidelity against real-world data (validation). It does not replace the need for traditional \textit{verification} techniques. Methods like code inspection, unit testing, and debugging remain essential for ensuring that the simulation model itself is implemented correctly and is free from bugs. Verification ensures the model is logically correct while the ML-based validation helps determine if it is the right model relative to observed reality.

\section{Discussion in Context of Research Questions}
\label{sec:discussion_rqs}
The subsequent discussion evaluates the extent to which this objective was met by critically analysing the results of the case study (\autoref{chap:case-study}) in the context of the research questions posed in \autoref{chap:introduction}:
\begin{itemize}
  \item \textbf{RQ1:} How can automated validation and verification processes for DTs be efficiently implemented to maintain accuracy?
  \item \textbf{RQ2:} Which data-driven approaches are best suited to identify discrepancies between simulated behaviour and real operational data in discrete material flow systems?
  \item \textbf{RQ3:} To what extent does the developed framework improve the quality and reliability of DTs compared to traditional V\&V methods?
\end{itemize}

\subsection{RQ1: Efficiency of Automated VVUQ Implementation}
\label{sec:discussion_rq1}
The practical efficiency of the given VVUQ framework depends on the given data quality, system architecture and modelling choices. In the specific IoT Factory use case, data was available and documented. In companies which are not data-driven or where gathering data is difficult, this framework might need time to be created and implemented. The data pipeline needed a lot of fine tuning and data expertise for it to work. Removing processes, false entries, and other data issues took a lot of time. The setup effort was higher than the execution effort of the model. Once the data pipeline was established, model training and inference was not time consuming. The largest amount of time was consumed in creating the hypothesis testing framework and assessing model quality. Referring back to the goal conflicts \autoref{fig:goals} in \autoref{sec:relevant-kpis}, the framework was able to fulfil the scalability requirements with relatively moderate computational costs and accuracy. Regarding the key challenges and requirements laid out in \autoref{sec:requirements-automatically-generated-models}, the problems of UQ and dynamic model adaption can be solved through the blackbox model. The p-values offer a quantitative measurement for UQ, while it is able to process online data (dynamic adaption). Model opacity was a trade-off with the BiLSMT structure, but may be solved if the modeller chooses the whitebox model. The key challenge of data dependency persists. During the development phase, nearly 50 hours were invested in creating the data pipeline and preprocessing. Importantly, the whitebox model could be used as a debugging tool for data pipeline evaluation.

To conclude and refer to \autoref{par:rq1} (also depicted above), the framework tackles the key challenges by providing two models that can be used based on validation goals. The whitebox model is able to provide a high level of interpretability and transparency, while the blackbox model is able to process large amounts of data and adapt to changing data. The effort of \textit{verification} still has to be carried out manually. Regarding the outlined \autoref{par:key-requirements}, the given framework is interpretable through the hypothesis testing and AFS as well as the whitebox model, integratable, scalable and can be continuously validated. The key requirement of upholding data quality was not met, as a lot of effort has been spent on manual implementation and debugging. The two-fold model approach is able to provide a good trade-off between interpretability vs. performance/accuracy. The framework balances the outlined goals, but the data quality and data dependency are still to be addressed.

\subsection{RQ2: Suitability of Data-Driven Approaches for Discrepancy Detection}
\label{sec:discussion_rq2}
A lot of work has been done on data-driven VVUQ approaches for all kinds of models, see \autoref{fig:pchart_knn_dt} for example. The model choice of DTree and BiLSTM seemed arbitrary and inappropriate at first, but the quantitative results indicate that both models complement each other nicely by weighing out the weaknesses of the counterpart. The DTree was not able to capture the sequential patterns in the data quite well, as it splits the data into different branches. The BiLSTM on the other side interpreted the processes with a sequence length of $19$ bidirectionally (past and future) and thus offers deeper insights. The OCEL format developed was able to capture complex dependencies in the data as it is able to store the data in a hierarchical structure and to model AND or OR processes. Different views on the processes are possible because of its object-centricness. Feature engineering highly improved the fitting of a discriminant rule of the DTree and increased the performance of the BiLSTM as well. During the engineering phase, the whitebox model can be plotted \footnote{Scikit-Learn provides a method \texttt{plot\_tree} which takes the DTree object and plots its decision rules} and the feature importance can be calculated. The feature importance of the DTree was able to show which features were important for the model and thus helped to understand the model better. It also facilitated the design of the BiLSTM architecture by showing that temporal features regarding the time model and KPIs were the best features to split the data. The DTree can thus be used as a V\%V tool by itself during the construction phase of the blackbox model by iteratively checking feature importances and its decision rules. This highly data-driven approach is a valuable tool for the modeller to perform a kind of sequential feature selection during feature engineering \autocite{pudil1994floating}. By iteratively adding or removing features, AFS can be performed. Also, it performed as a debugging tool. For example, complete gini impurity could be achieved between the two classes by splitting the data for an order date, an artificial value for the KPI throughput, or processes with ID $\le 27$ which were not present in the simulated data. This indicates that the model was able to detect a bug in the simulation model. The BiLSTM on the other side was of course able to utilize these splits, but is opaque. A solution can be the application of SHAP or LIME.

DTree and BiLSTM both served as appropriate choices for a data-driven VVUQ framework for SBDTs. Permutation testing complements the framework by providing a statistical significance test for the model. The p-values are able to quantify the uncertainty of the prediction. Permutation testing is particularly useful here because the underlying distribution of the OCEL and decision boundaries are unknown and had to be approximated. It was thus perfectly suited for the given data complexity. For non-sequential data, the DTree might be sufficient for VVUQ. Using a whitebox model made it easier to debug and to perform VVUQ. OCED was a suitable data structure to capture the patterns in the data. While some information might not be translatable into the OCEL standard, it serves as a good starting point for modelling efforts. Later on, the framework can be enhanced.

\subsection{RQ3: Improvement Compared to Traditional Methods}
\label{sec:discussion_rq3}

Mainly the validation process improves when using automated VVUQ approaches. The framework is able to ingest and process more information than domain experts. This offers a more comprehensive analysis than traditional checks focusing on aggregate KPIs or limited manual comparisons. The use of permutation testing (\autoref{sec:model-logic}) provides quantitative, objective measures of model fidelity, being in conflict with potentially subjective traditional assessments like face validity. Furthermore, the automated nature of the validation execution addresses scalability issues common in manual methods (\autoref{sec:comparison_manual}). However, this framework primarily enhances validation and supports rather than replaces essential verification techniques outlined in traditional approaches (\autoref{sec:comparison_manual}). While it effectively identifies that discrepancies exist, root cause analysis may require further investigation, and the initial setup effort (\autoref{sec:discussion_rq1}) must be acknowledged.

In essence, the framework offers significant advancements in validation effectiveness, providing targeted, data-driven feedback to improve SBDT quality and reliability, particularly when integrated alongside traditional verification practices. AFS offers insights in which model component may be underrepresented in the SBDT and thus needs to be improved.

\section{Implications of the Findings}
\label{sec:discussion_implications}

The findings from this research carry several implications for both the theory and practice of developing and validating SBDTs, particularly those generated automatically.

\subsection{Methodological and Theoretical Insights}
\label{sec:implications_theoretical}

This work contributes to the evolving field of VVUQ for complex simulation models and DTs. The successful use of a simplified OCEL format (\autoref{tab:exemplary-ocel}) demonstrates its potential beyond PM as a structure for validation data, proving effective for feature engineering and model input although not fully complying to the OCED standard (\autoref{sec:object-centric-event-logs}). The core theoretical contribution lies in the novel validation paradigm proposed and tested: a supervised classification approach where simulated and real data are distinctly labelled. In contrast to unsupervised methods measuring discrepancy magnitude, this framework trains a classifier to distinguish the two data sources. The innovation is the reversed interpretation of its performance (\autoref{sec:discussion_rq2}): Low classifier accuracy (for example AUC $\le 0.5$) means high SBDT fidelity for the tested component relative to the feature subset, as the distributions are indistinguishable. Conversely, high accuracy indicates low fidelity\textemdash the twin failed to replicate the real complexity, making the distributions easily separable, leading to the rejection of the null hypothesis ($H_0$). This counter-intuitive interpretation, where high performance signals an inaccurately learned component, underpins the framework. It assumes that a valuable SBDT should mirror complex data patterns, more than simpler abstractions like DM or DS. While any virtual model is an abstraction, the primary abstraction considered here is the OCEL format, which is assumed to be complete for encoding reality in this context. Furthermore, the integration of permutation testing provides statistical rigor, yielding p-values and rejection rates $RR$ that quantify the confidence in the validation assessment and offer inherent UQ. Hyperparameters $N$ (permutation count) and $\alpha$ (significance) allow tuning the VVUQ process to specific domain needs, enabling stricter thresholds where required (e.g., healthcare). This component-based fidelity assessment using AFS provides more actionable feedback than simple static scores.

Overall, this framework refines V\&V theory for automated SBDTs by changing focus from code verification to validating behaviour against data distributions, demonstrating the feasibility of the layered conceptual model (\autoref{fig:framework}) developed in \autoref{chap:methodology} through its implementation (\autoref{chap:implementation}).

\subsection{Recommendations for Practical Application}
\label{sec:implications_practical}

The findings offer several recommendations for practitioners. Firstly, prioritizing data infrastructure and quality is important. Implementing automated ML-based validation requires robust data pipelines, standardized formats like OCEL, and rigorous quality checks (\autoref{sec:requirements-automatically-generated-models}), as significant setup effort may be involved (\autoref{sec:discussion_rq1}). Secondly, leveraging domain knowledge remains crucial for effective feature engineering (\autoref{sec:feature-engineering}) to capture relevant process aspects aligned with SBDT components (\autoref{sec:comparing-dmfs}). Thirdly, employing a hybrid model approach is advantageous: Use interpretable whitebox models like DTree for initial insights, debugging, and guiding feature selection (\autoref{sec:discussion_rq2}), supplemented by more complex blackbox models to potentially capture more intricate dynamics. Fourthly, utilize the objective fidelity metrics derived from the classification approach and permutation testing (ROC AUC, p-value, $RR$) (\autoref{chap:case-study}}) to enhance trust and credibility over subjective assessments (\autoref{sec:conclusion_remarks}). Fifthly, adopt component-wise validation using relevant feature subsets to gain targeted feedback for SBDT improvement. Sixthly, plan for integration with existing manufacturing systems (MES, PPC, SCADA), leveraging the framework's modular design and standard protocols (\autoref{sec:requirements-automatically-generated-models}) to enable continuous validation cycles. Finally, remember to complement this automated validation framework with traditional verification techniques (code review, unit testing) to ensure the SBDT is implemented correctly (\autoref{sec:comparison_manual}).

\section{Limitations of the Study}
\label{sec:discussion_limitations}

Despite promising results, this study has limitations. Generalizability is constrained by the single case study. The IoT Factory (\autoref{sec:factory}) using the OFacT platform (\autoref{sec:automated-digital-twin}) is a laboratory framework designed for students to assess their work. Results may differ in other domains or with other SBDT tools. Data dependency is significant as well. The framework requires quality data, and the substantial preprocessing effort noted (\autoref{sec:discussion_rq1}, \autoref{sec:data-pipeline}), including filling missing values with zeros (\autoref{sec:factory}), presents a practical challenge and potential point of influence on results. Methodological choices, such as the specific classifiers (DTree vs. BiLSTM), engineered features (\autoref{sec:feature-engineering}), and permutation test parameters (N, $\alpha$) (\autoref{tab:results-whitebox}, \autoref{tab:results-blackbox}), impact outcomes and represent specific points in a larger space.

A key aspect is that manual verification remains the main limitation of this framework. Furthermore, interpretability of discrepancies poses a challenge; while the framework identifies inaccurate components (\autoref{sec:discussion_rq2}), determining the root cause, especially with the BiLSTM, requires further investigation, potentially using XAI tools or expert analysis. Lastly, the reliance on a simplified OCEL format means the validation is performed on an abstraction of reality (\autoref{sec:conclusion_summary_findings}); detected differences could partially reflect format limitations, or the format might obscure other discrepancies.

\section{Discussion Summary}
\label{sec:discussion_summary}

In summary, this chapter critically discussed the thesis findings, connecting the empirical results from the IoT Factory case study to the theoretical foundations. The ML-based VVUQ framework demonstrated potential for improving the validation of SBDTs. The discussion contextualized the answers to the research questions, affirming the potential for efficient implementation despite initial setup costs \autoref{par:rq1}, validating supervised classification with a novel reversed interpretation as a suitable data-driven approach \autoref{par:rq2}, and confirming improvements in validation scope, depth, and objectivity over traditional methods \autoref{par:rq3}.