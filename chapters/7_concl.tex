\chapter{Conclusion and Future Work}
\label{chap:conclusion}

\section{Summary of Key Findings and Contributions}
\label{sec:conclusion_summary_findings}

This thesis addressed the critical challenge of efficiently validating and verifying automatically generated \gls{sbdt}s for \gls{dmfs}. The research was guided by three core questions \autoref{sec:problem}, which this work aimed to answer:

Regarding \gls{rq}1 \autoref{par:rq1}, the study demonstrated that while significant effort at the beginning is required for data pipeline setup and feature engineering, also concerning data quality, the operational execution of the proposed \gls{vvuq} framework can be highly efficient and scalable. The framework utilizes a two-model approach (whitebox \gls{dtree} and blackbox \gls{bilstm}) to balance interpretability and performance, automating the core validation and uncertainty quantification task once established. The efficiency stems from leveraging readily available simulation and real-world data, processed into an \gls{ocel} format.

In response to \gls{rq}2 \autoref{par:rq2}, the empirical validation confirmed that data-driven supervised classification is possible for identifying discrepancies between simulated \gls{sbdt} behaviour and real operational data. By labelling simulated data differently from real data, a binary classifier is trained to distinguish between them. The innovation lies in the reversed interpretation of the classifier's performance: \textit{Low} performance (\gls{auc} $\le 0.5$) implies that the \gls{sbdt} component has been learned accurately relative to the tested features, as the simulated and real distributions are statistically indistinguishable. Conversely, \textit{high} classifier performance indicates that the component was \textit{not} learned accurately, as the distributions are easily separable. This contrasts with unsupervised approaches like \textcite{dos2024digital} which measure discrepancy magnitude with p-charts, and also runs counter to the intuition that high performance always signals success. Here, high performance signals high distinguishability, meaning the twin failed to replicate the complex manifold of the real data for that component. Permutation testing (\autoref{sec:model-logic}) provided a robust method for assessing the statistical significance (p-value) and consistency (Rejection Rate, $RR$) of this distinguishability. This approach makes the assumption that a successful \gls{sbdt} must accurately reflect complex underlying data patterns, going further than simpler abstractions like \gls{dm} or \gls{ds}, and that the chosen \gls{ocel} format is suitable to encode the relevant aspects of reality.

Addressing \gls{rq}3 \autoref{par:rq3}, the developed \gls{ml} framework demonstrated improvements over traditional \gls{vv} approaches (\autoref{sec:comparison_manual}). The case study (\autoref{chap:case-study}) showed enhanced context and objectivity, systematically identifying inaccuracies across all tested \gls{sbdt} components with high statistical significance (\autoref{tab:results-whitebox}, \autoref{tab:results-blackbox}). This automated \gls{vvuq} capability offers advantages in scalability and objectivity. The significance level ($\alpha$) and number of permutations ($N$) can be tuned based on domain-specific requirements (e.g., lower $\alpha$, higher $N$ in safety-critical applications like healthcare).

The main empirical results from the \gls{iot} Factory case study (\autoref{chap:case-study}) showed that both classifiers achieved high performance for most components, leading to the rejection of the null hypothesis ($H_0$) and the conclusion that these components were inaccurate in the current \gls{sbdt} configuration. This highlights the frameworks sensitivity in detecting deviations and provides specific clues for targeted \gls{sbdt} model improvement or recalibration

The primary contributions of this thesis are:
\begin{enumerate}
  \item The development and conceptualization of a multi-layered, automated \gls{vvuq} framework (\autoref{fig:framework}) leveraging supervised classification for \gls{sbdt} fidelity assessment.
  \item A novel methodological interpretation where low classifier performance indicates high \gls{sbdt} fidelity for specific components/features, combined with permutation testing for statistical rigor (validation) and uncertainty quantification (p-values, $RR$).
  \item Empirical demonstration of the frameworks ability to target inaccuracies in an industrial \gls{sbdt} case study (\autoref{chap:case-study}).
\end{enumerate}

However, a \testit{main limitation} stays: The framework automates validation and uncertainty quantification but relies on \textit{manual verification} to ensure the \gls{sbdt} model is built correctly according to its conceptual description and specifications.

\section{Concluding Remarks}
\label{sec:conclusion_remarks}

The increasing adoption of \gls{sbdt}s necessitates robust and efficient \gls{vvuq} methods to ensure trust and reliability. Traditional approaches face significant hurdles in scalability and objectivity when applied to complex, automatically generated twins. This thesis proposed and validated a novel, data-driven framework using supervised classification with a reversed interpretation of performance metrics, complemented by permutation testing. This approach offers a scalable, objective, and statistically grounded method for assessing \gls{sbdt} fidelity component-wise. The empirical success in identifying specific areas of inaccuracy underscores the value of data-driven validation. While verification remains a manual task and data quality is paramount, this work contributes a significant step towards continuous, automated, and trustworthy validation of \gls{sbdt}s, essential for their successful deployment in demanding Industry 4.0 applications.

\section{Future Research Directions}
\label{sec:conclusion_future_work}

Based on the findings and limitations identified (\autoref{sec:discussion_limitations}), several avenues for future research emerge:

\begin{itemize}
  \item \textbf{Enhancing Interpretability (Root Cause Analysis):} Integrate \gls{xai} techniques (\gls{shap}, \gls{lime}) to move beyond identifying that a component is inaccurate (high classifier performance) to explaining why, thereby facilitating targeted model correction.
  \item \textbf{Refining Uncertainty Quantification:} Further explore methods like \gls{bnn}s or \gls{mcd} to provide richer UQ measures beyond the p-value/$RR$, potentially offering confidence intervals on the fidelity assessment itself.
  \item \textbf{Automated Recalibration and Verification Support:} Develop mechanisms for automated \gls{sbdt} recalibration triggered by validation results. Explore how the frameworks outputs could potentially support or partially automate aspects of manual verification, addressing the main limitation.
  \item \textbf{Improving Online Capabilities and Integration:} Enhance the framework for robust real-time operation, optimizing data stream processing and developing standardized interfaces for seamless integration into industrial \gls{iot} environments.
  \item \textbf{Addressing Data Quality and Pipeline Automation:} Research more automated techniques for ensuring input data quality and streamlining the data pipeline setup, potentially leveraging \gls{automl} or anomaly detection on the input data itself.
  \item \textbf{Investigating Foundational Assumptions:} Conduct studies on the impact of the \gls{ocel} format's completeness and the assumption regarding required \gls{sbdt} complexity across different application domains.
  \item \textbf{Generalizability and Broader Application:} Validate the framework's effectiveness and the reversed interpretation hypothesis across diverse industrial domains, \gls{sbdt} platforms, and levels of system complexity.
  \item \textbf{Exploring Alternative \gls{ml} Approaches:} Investigate other promising \gls{ml} techniques not yet applied in this specific concurrent \gls{vvuq} context. The Local Outlier Factor (\gls{lof}), a distance-based approach, could identify outliers potentially linked to rare events or concept drift \autocite{alghushairy2020review}. Density-based methods like \gls{dbscan} identify anomalies in low-density regions \autocite{ccelik2011anomaly}, distinguishing normal clusters from deviations. Isolation Forest (\gls{if}) offers efficiency for high-dimensional data by isolating anomalies quickly through random partitioning \autocite{xu2017improved}. One-Class \gls{svm}s \autocite{li2003improving} could be useful when only normal data is available, learning a boundary for normal operations. \gls{ae}s \autocite{zhou2017anomaly} trained on normal data can detect anomalies via high reconstruction errors, capturing complex non-linear patterns. Furthermore, semi-supervised techniques could leverage small amounts of labelled anomaly data alongside larger normal datasets to enhance detection.
\end{itemize}

Addressing these directions will further mature automated \gls{vvuq} methodologies, making \gls{sbdt}s more reliable, trustworthy, and impactful in practice. \blacksquare