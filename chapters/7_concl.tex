\chapter{Conclusion and Future Work}
\label{chap:conclusion}

\section{Summary of Key Findings and Contributions}
\label{sec:conclusion_summary_findings}

This thesis addressed the critical challenge of efficiently validating and verifying automatically generated SBDTs for DMFS. The research was guided by three core questions \autoref{sec:problem}, which this work aimed to answer:

Regarding RQ1 \autoref{par:rq1}, the study demonstrated that while significant effort at the beginning is required for data pipeline setup and feature engineering, particularly concerning data quality, the operational execution of the proposed VVUQ framework can be highly efficient and scalable. The framework utilizes a two-model approach (whitebox DTree and blackbox BiLSTM) to balance interpretability and performance, automating the core validation and uncertainty quantification task once established. The efficiency stems from leveraging readily available simulation and real-world data, processed into an OCEL format.

In response to RQ2 \autoref{par:rq2}, the empirical validation confirmed that data-driven supervised classification is possible for identifying discrepancies between simulated SBDT behaviour and real operational data. By labelling simulated data differently from real data, a binary classifier is trained to distinguish between them. The innovation lies in the reversed interpretation of the classifier's performance: \textit{Low} performance (AUC $\le 0.5$) implies that the SBDT component has been learned accurately relative to the tested features, as the simulated and real distributions are statistically indistinguishable. Conversely, \textit{high} classifier performance indicates that the component was \textit{not} learned accurately, as the distributions are easily separable. This contrasts with unsupervised approaches like \autocite{dos2024digital} which measure discrepancy magnitude with p-charts, and also runs counter to the intuition that high performance always signals success. Here, high performance signals high distinguishability, meaning the twin failed to replicate the complex manifold of the real data for that component. Permutation testing (\autoref{sec:model-logic}) provided a robust method for assessing the statistical significance (p-value) and consistency (Rejection Rate, $RR$) of this distinguishability. This approach makes the assumption that a successful SBDT must accurately reflect complex underlying data patterns, going further than simpler abstractions like DM or DS, and that the chosen OCEL format (\autoref{tab:ocel_format}) is suitable to encode the relevant aspects of reality.

Addressing RQ3 \autoref{par:rq3}, the developed ML framework demonstrated improvements over traditional V&V approaches (\autoref{sec:comparison_manual}). The case study (\autoref{chap:case-study}) showed enhanced context and objectivity, systematically identifying inaccuracies across all tested SBDT components with high statistical significance (\autoref{tab:results-whitebox}, \autoref{tab:results-blackbox}). This automated VVUQ capability offers advantages in scalability and objectivity. The significance level ($\alpha$) and number of permutations ($N$) can be tuned based on domain-specific requirements (e.g., lower $\alpha$, higher $N$ in safety-critical applications like healthcare).

The main empirical results from the IoT Factory case study (\autoref{chap:case-study}) showed that both classifiers achieved high performance for most components, leading to the rejection of the null hypothesis ($H_0$) and the conclusion that these components were inaccurate in the current SBDT configuration. This highlights the frameworks sensitivity in detecting deviations and provides specific clues for targeted SBDT model improvement or recalibration

The primary contributions of this thesis are:
\begin{enumerate}
  \item The development and conceptualization of a multi-layered, automated VVUQ framework (\autoref{fig:framework}) leveraging supervised classification for SBDT fidelity assessment.
  \item A novel methodological interpretation where low classifier performance indicates high SBDT fidelity for specific components/features, combined with permutation testing for statistical rigor (validation) and uncertainty quantification (p-values, $RR$).
  \item Empirical demonstration of the frameworks ability to target inaccuracies in an industrial SBDT case study (\autoref{chap:case-study}).
\end{enumerate}

However, a \textbf{main limitation} persists: the framework automates validation and uncertainty quantification but relies on \textbf{manual verification} to ensure the SBDT model is built correctly according to its conceptual description and specifications.

\section{Concluding Remarks}
\label{sec:conclusion_remarks}

The increasing adoption of SBDTs necessitates robust and efficient VVUQ methods to ensure trust and reliability. Traditional approaches face significant hurdles in scalability and objectivity when applied to complex, automatically generated twins. This thesis proposed and validated a novel, data-driven framework using supervised classification with a reversed interpretation of performance metrics, complemented by permutation testing. This approach offers a scalable, objective, and statistically grounded method for assessing SBDT fidelity component-wise. The empirical success in identifying specific areas of inaccuracy underscores the value of data-driven validation. While verification remains a manual task and data quality is paramount, this work contributes a significant step towards continuous, automated, and trustworthy validation of SBDTs, essential for their successful deployment in demanding Industry 4.0 applications.

\section{Future Research Directions}
\label{sec:conclusion_future_work}

Based on the findings and limitations identified (\autoref{sec:discussion_limitations}), several avenues for future research emerge:

\begin{itemize}
  \item \textbf{Enhancing Interpretability (Root Cause Analysis):} Integrate XAI techniques (SHAP, LIME) to move beyond identifying *that* a component is inaccurate (high classifier performance) to explaining *why*, thereby facilitating targeted model correction.
  \item \textbf{Refining Uncertainty Quantification:} Further explore methods like BNNs or MCD to provide richer UQ measures beyond the p-value/RR, potentially offering confidence intervals on the fidelity assessment itself.
  \item \textbf{Automated Recalibration and Verification Support:} Develop mechanisms for automated SBDT recalibration triggered by validation results. Explore how the framework's outputs could potentially support or partially automate aspects of manual verification, addressing the main limitation.
  \item \textbf{Improving Online Capabilities and Integration:} Enhance the framework for robust real-time operation, optimizing data stream processing and developing standardized interfaces for seamless integration into industrial IT/OT environments.
  \item \textbf{Addressing Data Quality and Pipeline Automation:} Research more automated techniques for ensuring input data quality and streamlining the data pipeline setup, potentially leveraging AutoML or anomaly detection on the input data itself.
  \item \textbf{Investigating Foundational Assumptions:} Conduct studies on the impact of the OCEL format's completeness and the assumption regarding required SBDT complexity across different application domains.
  \item \textbf{Generalizability and Broader Application:} Validate the framework's effectiveness and the reversed interpretation hypothesis across diverse industrial domains, SBDT platforms, and levels of system complexity.
  \item \textbf{Exploring Alternative ML Approaches:} Investigate other promising ML techniques not yet applied in this specific concurrent VVUQ context. The Local Outlier Factor (LOF), a distance-based approach, could identify outliers potentially linked to rare events or concept drift \autocite{alghushairy2020review}. Density-based methods like DBSCAN identify anomalies in low-density regions \autocite{ccelik2011anomaly}, distinguishing normal clusters from deviations. Isolation Forest offers efficiency for high-dimensional data by isolating anomalies quickly through random partitioning \autocite{xu2017improved}. One-Class SVMs \autocite{li2003improving} could be useful when only normal data is available, learning a boundary for normal operations. Autoencoders (AE) \autocite{zhou2017anomaly} trained on normal data can detect anomalies via high reconstruction errors, capturing complex non-linear patterns. Furthermore, semi-supervised techniques could leverage small amounts of labelled anomaly data alongside larger normal datasets to enhance detection.
\end{itemize}

Addressing these directions will further mature automated VVUQ methodologies, making SBDTs more reliable, trustworthy, and impactful in practice.