\chapter{Testing}
\label{chap:case-study}

The thesis now focusses on the application of the implemented features and methods in a real-world scenario. The goal is to demonstrate the practical applicability of the developed concepts and to validate the theoretical findings presented in the previous chapters.

\section{Application Scenario}
\label{sec:factory}
The framework has been applied on the Internet of Things-Factory (IOT) in Gütersloh, Germany \parencite{IoTFactory2024}. It is a cyber-physical system (CPS) \parencite{baheti2011cyber} mimicking industry-relevant processes in a smaller scale for research students. It consists of several stations that are partly interconnected via an assembly line or a delivery service conducted by automatic guided vehicles (AGVs). The factory is modular, so processes can be discovered module-wise in isolation. All modules are working on edge but are connected to a cluster to control it. The factory works without any personnel in theory. The main process also evaluated here is circular, meaning that the product is assembled and can be disassembled in a loop. The factory is shown in Figure \ref{fig:iotoverview}.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/iot1.png}
  \caption{Overview of the IOT factory. It consists of three production stations from left to right, which are followed by a sorting station and a packaging station. The stations are interconnected by an assembly line. Isolated from the assembly part, two AGVs are used to transport parts between the warehouse station (upper right) and another flexible workstation (right).}
  \label{fig:iotoverview}
\end{figure}

The robot cells are responsible for performing transformation operation like assembling additional parts or testing functions.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/robots.png}
  \caption{Two robot cells. The first cell is the main actor in this exemplary production process. Cell two is not part of the observed process here. The third image shows an AGV which transports boxes with assembled and disassembled parts to the stations.}
  \label{fig:robots}
\end{figure}

The factory produces an exemplary product, consisting of a back part, a breadboard for several parts and a front panel. The parts to put on the breadboard are a display, gyroscope, an analog board and a weather station.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.2\textwidth]{figures/parts.png}
  \caption{The product consists of a back part, a breadboard and a front panel. The breadboard is used to put on several parts, such as a display, gyroscope, analog board and a weather station.}
  \label{fig:iotproduct}
\end{figure}

The only colour produced at the moment is black. Not all parts can be put on the breadboard and there are several parts which conflict in size and location on the breadboard. The ground-truth assembly rules look as follows:
PRÜFEN UND ANPASSEN

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/assrules.png}
  \caption{Three variants are possible. No transformation is also a possibility.}
  \label{fig:assrules}
\end{figure}

Back and front cover are necessary parts and are assembled every time. The main part \texttt{main\_pcb} is also obligatory. It is placed at the beginning. Following that, the display or gyroscope can be assembled. If the display has been chosen, only the analog board or the weather station can be assembled. The weather station takes too much space on the breadboard for the gyroscope to be placed. If the analog board has been placed, the gyroscope is of course possible. The product is finished by placing the front cover and delivering the product to the sink.

During the production, the factory gathers data via sensors. The data is saved in a database and can be used for further analysis.

Finally, the sequential order of the production process is as follows:

\begin{figure}[H]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/processdirection.png}
  \caption{The blueprint with transitions between resources.}
  \label{fig:transitions}
\end{figure}

\subsection{Data Basis}

\subsubsection{SBDT Dataset}

The dataset gathered from the SBDT is referred to as \texttt{sim\_data}. The OpenFactoryTwin (see \autoref{sec:automated-digital-twin}) provides a method to deserialize the simulated orders and save them in a CSV file. The dataset then gets converted to the OCEL structure. For this endeveur, a separate connector logic has to be implemented in every use case. See \texttt{src/connector/ofact} for the connector here.

The connector's output follows a standardized structure defined by the \texttt{OutputStructure} class, which ensures consistency across different data sources. The connector first deserializes the dynamic state model from a pickle file, accessing the full simulation state. Only actual process executions (as opposed to planned ones) are retained for further processing, identified by the "ACTUAL" flag in their event type. The connector creates mapping dictionaries for various categorical attributes following the OCED standard:

\begin{itemize}
  \item \textbf{Part ID Mapping:} Uses domain expertise to identify part types from process names, normalizing text (lowercase, no whitespace) to match against a predefined list of possible parts. This list contains possible parts from \autoref{fig:iotproduct}.
  \item \textbf{Process Type Categorization:} Assigns process steps to expert-defined categories such as "machine," "feature," "endproduct," "test," and "transport" based on keywords in process names. These types have been assigned by the modeller and are of free choice. See \autoref{tab:output-structure-detailed}.
  \item \textbf{Process ID Mapping:} Creates unique integer identifiers for each distinct process description based on enumeration.
  \item \textbf{Resource ID Mapping:} Generates unique identifiers for each resource involved in the process executions based on enumeration.
  \item \textbf{Temporal Data Extraction:} The connector extracts start and end times for each process execution.
\end{itemize}

Process executions are associated with their respective order IDs, establishing the connection between individual process steps and the orders they belong to. All extracted information is then consolidated into a standardized DataFrame structure with properly typed columns as defined in the \texttt{OutputStructure} class. The connector also includes a validation step to ensure that the generated DataFrame adheres to the expected structure and data types, raising an error if any discrepancies are found.

A key aspect of the connector's functionality is the integration of domain knowledge into the data transformation process. Instead of relying solely on the raw data structure, the connector employs expert-defined categorizations and normalization procedures to ensure semantic consistency in the transformed data.

For example, the part identification logic uses a predefined list of potential parts (such as "GYROSCOPE," "MAIN PCB," "FRONT COVER," etc.) and searches for these terms within process descriptions. Similarly, the process type categorization uses domain-specific groupings like "machine," "feature," and "test" based on keywords found in process names.This domain knowledge integration ensures that the transformed data maintains the semantic richness required for meaningful validation and comparison between simulated and real-world processes.

The \texttt{sim\_data} rows will receive the label $0$ for \texttt{is\_valid}, because we want the black-box model to learn to distinguish between real and simulated data to perform VVUQ. This way we can ensure that the SBDT learned important characteristics from the factory data, which receives the integer $1$ for \texttt{is\_valid}.

\subsubsection{Factory Dataset}

The dataset has been gathered from the database of the IoT Factory. In the code it is referred to as \texttt{real\_data}.

The dataset contains 18057 rows and 4696 orders from a timespan of 22.04.2020 to 14.05.2024. There are no duplicate rows. Each row describes a unique operation step in an order. Roughly 40 percent of the rows contain missing values, which have been filled with zeros.
The data has been mined from a MariaDB SQL database of the IoT Factory. Several tables have been aggregated to form the dataset. Without additional tables, no information about workplans, resources and operation numbers would be available. The dataset is filtered for promising trajectories of meaningful and intact process executions. The filter considers data between the 20.04.2020 and 22.04.2022, 02.05.2022 and 19.07.2022, 02.11.2022, 11.11.2022, 23.01.2022 and March 2023, each data inclusively.

The dataset contains the following columns:

\begin{itemize}
  \item \textbf{WPNo}: The workplan number of the specific workplan.
  \item \textbf{StepNo}: The step number of the specific workplan. The workplan is divided in sequential steps enumerated by this number.
  \item \textbf{ONo}: The order number of the specific order. Each order has a unique number.
  \item \textbf{OPos}: The order position of the specific order. The order position is the position of the order in the workplan.
  \item \textbf{Description}: The description of the specific step in spoken language.
  \item \textbf{OpNo}: The operation number of the specific operation. Each operation has a unique number.
  \item \textbf{NextStepNo}: The next step number of the specific step. This is the step that follows the current step.
  \item \textbf{FirstStep}: The first step of the specific workplan.
  \item \textbf{ErrorStepNo}: The error step number of the specific step. This is the step that is executed in case of an error.
  \item \textbf{Start}: The start time of the specific step.
  \item \textbf{End}: The end time of the specific step.
  \item \textbf{OPNoType}: The operation number type of the specific operation.
  \item \textbf{ResourceID}: The resource ID of the specific resource.
  \item \textbf{ErrorStep}: Is this step an error step (yes/no)?
  \item \textbf{ErrorRetVal}: The error return value of the specific step.
  \item \textbf{Active}: Is this step active (yes/no)?
  \item \textbf{op\_desc}: The operation description of the specific operation. More specific than the description.
  \item \textbf{ResourceName}: The name of the specific resource.
  \item \textbf{resource\_desc}: The description of the specific resource.
  \item \textbf{workplan\_desc}: The description of the specific workplan.
  \item \textbf{workplantype\_desc}: The description of the specific workplan type.
  \item \textbf{case\_id}: The case ID of the specific case.
  \item \textbf{Description\_Encoded}: The description of the specific step ordinally encoded.
\end{itemize}

The operations performed can be seen in the following:

\begin{enumerate}
  \item \textbf{Release a part on stopper 1}: The AGV delivered the box with the parts to be manufactured to the first station. The box is travelling over the assembly line to the first station where the first part is released.
  \item \textbf{Place cover to assembly place}: The first part is placed on the assembly place. The cover is placed on top of the part as the first piece of the product.
  \item \textbf{Assemble part from box on RASS1 - MAIN PCB}: The main PCB is assembled on the cover. This is the first configured part of the product. The main PCB is the breadboard of the product.
  \item \textbf{Switch on PCB}: The main PCB is switched on. This is necessary to activate it.
  \item \textbf{Assemble part from box on RASS1 - DISPLAY}: The display is assembled on the main PCB. This is the second configured part of the product. The display is one optional part of the product. This factors out one product variant.
  \item \textbf{Move part to pallet on belt}: The product is moved to the pallet on the belt for further processing.
  \item \textbf{Measure a part (analog)}: The product is measured. This is necessary to ensure the quality of the product for later steps.
  \item \textbf{Assemble part from box on RASS2 - ANALOG}: The analog part is assembled on the product. This is the third configured part of the product. The analog part is also one optional part of the product.
  \item \textbf{Assemble part from box on RASS2 - GYROSCOPE}: The gyroscope is assembled on the product. This is the fourth configured part of the product. The gyroscope is also one optional part of the product.
  \item \textbf{Move part to pallet on belt}: The product is moved to the pallet on the belt for further processing.
  \item \textbf{Check analog}: The analog part is checked. This is necessary to ensure the quality of the product for later steps.
  \item \textbf{Check gyroscope}: The gyroscope is checked. This is necessary to ensure the quality of the product for later steps
  \item \textbf{Assemble part from box on RASS3 - FRONT COVER}: The front cover is assembled on the product. This is the last configured part of the product. The front cover is the last part of the product.
  \item \textbf{Move part to pallet on belt}: The product is moved to the pallet on the belt for further processing.
  \item \textbf{Test connection to IoT main PCB}: The connection to the IoT main PCB is tested. This is necessary to ensure the quality of the product for later steps.
  \item \textbf{Test the function of the touch display}: The touch display is tested. This is necessary to ensure the quality of the product for later steps.
  \item \textbf{Test the analog input/output shield}: Analog PCB is tested.
  \item \textbf{Test the historical gyroscope data}: The gyroscope is tested.
  \item \textbf{Print Label}: The label is printed. The label contains information about the product configuration, the time manufactured and the serial number.
  \item \textbf{Deliver Part}: The final product is delivered to the sink.
\end{enumerate}

Per step, several resources are involved. The following table shows the utilized resources per step:

\begin{table}[H]
  \centering
  \caption{Steps and Resources Used}
  \label{tab:description-resources}
  \begin{tabular}{@{}p{0.4\textwidth}p{0.4\textwidth}@{}}
    \toprule
    \textbf{Process Step Name}                    & \textbf{Resource Name}                \\
    \midrule
    Release a part on stopper 1                   & CP-F-ASRS32-P                         \\
    Place cover to assembly place                 & CP-F-RASS-1, CP-F-RASS-2, CP-F-RASS-3 \\
    Assemble part from box on RASS1 - MAIN PCB    & CP-F-RASS-1                           \\
    Switch on PCB                                 & CP-F-RASS-1                           \\
    Assemble part from box on RASS1 - DISPLAY     & CP-F-RASS-1                           \\
    Move part to pallet on belt                   & CP-F-RASS-1, CP-F-RASS-2, CP-F-RASS-3 \\
    Measure a part (analog)                       & CP-AM-MEASURE                         \\
    Assemble part from box on RASS2 - ANALOG      & CP-F-RASS-2                           \\
    Assemble part from box on RASS2 - GYROSCOPE   & CP-F-RASS-2                           \\
    Move part to pallet on belt                   & CP-F-RASS-1, CP-F-RASS-2, CP-F-RASS-3 \\
    Check analog                                  & CP-AM-CAM                             \\
    Check gyroscope                               &                                       \\
    Assemble part from box on RASS3 - FRONT COVER & CP-F-RASS-3                           \\
    Move part to pallet on belt                   & CP-F-RASS-1, CP-F-RASS-2, CP-F-RASS-3 \\
    Test connection to IoT main PCB               & CP-AM-FTEST                           \\
    Test the function of the touch display        & CP-AM-FTEST                           \\
    Test the analog input/output shield           & CP-AM-FTEST                           \\
    Test the historical gyroscope data            & CP-AM-FTEST                           \\
    Print Label                                   & CP-AM-LABEL                           \\
    Deliver Part                                  & CP-AM-OUT                             \\
    \bottomrule
  \end{tabular}
\end{table}


This process has been identified as the ground-truth process. The process is circular, meaning that the product is assembled and can be disassembled in a loop. The process is also modular, meaning that the product can be assembled in different configurations. The process is also flexible, meaning that the product can be assembled in different ways.

\section{Open Factory Twin}
\label{sec:automated-digital-twin}

The Simulation-Based Digital Twin (SBDT) for the IoT Factory use case was developed using the Open Factory Twin (OFacT) framework \autocite{ofactintern}. OFacT is an open-source digital twin framework specifically designed for modeling, simulating, and controlling production and logistics environments. Its goal is to support system design, planning, and operational control throughout the entire lifecycle of such systems.

A principle of OFacT is the separation between the static description of the system and its dynamic behavior. This is achieved by distinguishing between:

\begin{itemize}
  \item \textbf{State Model:} This component represents the static structure of the factory, its components (resources, parts, layout), their properties, relationships, and the potential processes or behaviors they can exhibit.
  \item \textbf{Agent Control:} This component implements the dynamic logic that governs the system's operation during simulation, making decisions about resource allocation, process execution sequences, and handling events based on the state model.
\end{itemize}

The construction of the State Model within OFacT uses structured input methods, such as Excel files, where different sheets correspond to specific classes within the OFacT metamodel. To model the IoT Factory scenario (\autoref{sec:factory}), the relevant components of the OFacT State Model were defined, including:

\begin{itemize}
  \item \textbf{Plant}: The overall entity of production. The plant name used here was \texttt{iot\_factory}.
  \item \textbf{EntityType}: All entities have to be defined here. This ranges from the parts, resources and AGVs to the factory itself. For a complete list see the excel document \texttt{small.xlsx} on the second sheet.
  \item \textbf{StationaryResource}: Stationary resources are static and can not move. In this case, these are the RASS stations, measurement-, cam-, function test- and labelling station. The AGVs are not stationary resources, because they can move.
  \item \textbf{Storage}: Storage units contain parts. They are used to traverse the parts through the factory. In this context, the warehouse and box storage, have been modelled.
  \item \textbf{Warehouse}: This is a static storage unit where the parts are stored in boxes until they are processed.
  \item \textbf{WorkStation}: Workstations are resources which perform processes on parts. In our use case, these are the RASS stations.
  \item \textbf{ConveyorBelt}: There is one belt where the storage units traverse through the factory.
  \item \textbf{NonStationaryResource}: There are no non-stationary resources.
  \item \textbf{PassiveMovingResource}: One artifical passive moving resource has been mod
  \item \textbf{Process}: Contains processes and value added processes (VAP). VAP are adding features to parts and modify it. For each activity in the data a VAP has been created.
  \item \textbf{ProcessController}: This controller summarizes all processes to come and connects them.
  \item \textbf{ResourceModel}: Resource groups are formulated for activities like montage, identifying to attach the relevant parts to these resources. This way, main resources and parts are getting matched.
  \item \textbf{ProcessTimeModel}: Each part receives a time simple time distribution to account for its production time.
  \item \textbf{QualityModel}: Each part receives a bernoulli distribution to account for its quality. In the dataset, no quality information existed.
  \item \textbf{TransitionModel}: This model connects possible origins to possible destinations, to that the traversal of the parts can be modelled correctly. The packaging has been modelled as transition model.
  \item \textbf{TransformationModel}: This model contains an artificial transformation model.
  \item \textbf{Time}: Process execution plans get a starting time here.
  \item \textbf{Part}: Parts are connected to their EntityType here. There is also information attached where the part is stored or situated in.
  \item \textbf{Sales}: Lists the features and feature cluster. This matches the building rules. Parts have to be defined as features here.
  \item \textbf{CustomerGeneration}: Customer generation logic.
  \item \textbf{Customer}: List of customers.
  \item \textbf{Orders}: The orders with their requested features.
  \item \textbf{Process}: Contains processes and value added processes (VAP). VAP are adding features to parts and modify it. For each activity in the data a VAP has been created.
  \item \textbf{TransitionModel}: This model connects possible origins to possible destinations, to that the traversal of the parts can be modelled correctly. The packaging has been modelled as transition model.
  \item \textbf{TransformationModel}: This model contains an artificial transformation model.
\end{itemize}


By defining these elements according to the IoT Factory's characteristics, a detailed static model was created within the OFacT framework. For simulation purposes, the state model then gets 'played out' with orders. The orders contain this model then served as the basis for running simulations to generate process execution data, forming the SBDT dataset used in this thesis for comparison against real-world data. The \autoref{fig:assrules} are inherently modelled by the chosen order variants.

\section{Simulation}

The dataset gathered from the SBDT is referred to as \texttt{sim\_data}. The OpenFactoryTwin (see \autoref{sec:automated_digital_twin}) provides a method to deserialize the simulated orders and save them in a CSV file. The dataset then gets converted to the OCEL structure. For this endeveaur, a separate connector logic has to be implemented in every use case. See \texttt{src/connector/ofact} for the connector here.

The connector's output follows a standardized structure defined by the \texttt{OutputStructure} class, which ensures consistency across different data sources. The connector first deserializes the dynamic state model from a pickle file, accessing the full simulation state. Only actual process executions (as opposed to planned ones) are retained for further processing, identified by the "ACTUAL" flag in their event type. The connector creates mapping dictionaries for various categorical attributes following the OCED standard:

\begin{itemize}
  \item \textbf{Part ID Mapping:} Uses domain expertise to identify part types from process names, normalizing text (lowercase, no whitespace) to match against a predefined list of possible parts. This list contains possible parts from \autoref{fig:iotproduct}.
  \item \textbf{Process Type Categorization:} Assigns process steps to expert-defined categories such as "machine," "feature," "endproduct," "test," and "transport" based on keywords in process names. These types have been assigned by the modeller and are of free choice. See \autoref{tab:output-structure-detailed}.
  \item \textbf{Process ID Mapping:} Creates unique integer identifiers for each distinct process description based on enumeration.
  \item \textbf{Resource ID Mapping:} Generates unique identifiers for each resource involved in the process executions based on enumeration.
  \item \textbf{Temporal Data Extraction:} The connector extracts start and end times for each process execution.
\end{itemize}

Process executions are associated with their respective order IDs, establishing the connection between individual process steps and the orders they belong to. All extracted information is then consolidated into a standardized DataFrame structure with properly typed columns as defined in the \texttt{OutputStructure} class. The connector also includes a validation step to ensure that the generated DataFrame adheres to the expected structure and data types, raising an error if any discrepancies are found.

A key aspect of the connector's functionality is the integration of domain knowledge into the data transformation process. Instead of relying solely on the raw data structure, the connector employs expert-defined categorizations and normalization procedures to ensure semantic consistency in the transformed data.

For example, the part identification logic uses a predefined list of potential parts (such as "GYROSCOPE," "MAIN PCB," "FRONT COVER," etc.) and searches for these terms within process descriptions. Similarly, the process type categorization uses domain-specific groupings like "machine" "feature" and "test" based on keywords found in process names.This domain knowledge integration ensures that the transformed data maintains the semantic richness required for meaningful validation and comparison between simulated and real-world processes.

The \texttt{sim\_data} rows will receive the label $0$ for \texttt{is\_valid}, because we want the black-box model to learn to distinguish between real and simulated data to perform VVUQ. This way we can ensure that the SBDT learned important characteristics from the factory data, which receives the integer $1$ for \texttt{is\_valid}.


\subsection{Concatenation of Datasets}

Several preprocessing steps had to be performed to account for the fact that the SBDT simulated only one variant of the product: 'analog', 'cover', 'display', 'gyroscope', 'pcb' and the involved machines. This yielded the necessity to make both datasets congruent to each other. The following steps were performed:

\begin{itemize}
  \item The \texttt{sim\_data} dataset was aligned for the same time period as the \texttt{real\_data} dataset. The SBDT chose the time of order as the production time. In the modelling phase, when adaptive feature selection had been performed to identify if the SBDT was able to learn the Time Model, only \textit{relative} time features were chosen which were developed in \autoref{sec:feature-engineering}.
  \item The \texttt{real\_data} dataset was filtered for the same process steps as the \texttt{sim\_data} dataset. This means that only the process steps which are present in the \texttt{sim\_data} dataset were kept, producing only one product variant. This has also been applied on the part\_id, the process\_type and resource\_id columns. The IDs based on enumeration had to be mapped to the original IDs in \texttt{real\_data} to ensure that the correct IDs are used in the simulation. The mapping was done through the JSON files generated by the connector.
  \item Both datasets have been cleaned and entries containing invalid IDs were removed. This means that all entries which are not present in the mapping dictionaries were removed. The mapping dictionaries are generated by the connector and contain only valid IDs per definition.
  \item Only a subset of all performed processes was included (in detail, all \texttt{process\_id} $\le 26$. These processes all have the \texttt{process\_type} 'machine', 'feature' or 'endproduct'. The processes with the \texttt{process\_type} 'test' and 'transport' were removed. This was done to ensure that only the relevant processes are included in the dataset.
\end{itemize}

The \texttt{real\_data} dataset was then concatenated with the \texttt{sim\_data} dataset. The concatenation was done by appending the \texttt{sim\_data} dataset to the \texttt{real\_data} dataset. The resulting dataset contains all process steps from both datasets. The resulting dataset is referred to as \texttt{final\_data}. The unification before concatenation was necessary to ensure that no logical flaws are present in the data.

The \texttt{final\_data} dataset finally after preprocessing as described contains 1978 rows and 56 orders with 24 features. The following section elaborates how these features were generated.

\subsubsection{Feature Engineering}
\label{sec:feature-engineering}

Several features have been engineered to assist both the whitebox and blackbox model on the validation task. The features were furher introduced to empower adaptive feature selection (AFS) regarding the model components of the SBDT \autocite{schwede2024learning}. This way, if the features sufficiently explain the data, the conclusion can be made that the SBDT was able to learn the respective model component.

The following features were generated with domain knowledge:

\begin{itemize}
  \item \textbf{KPIs}: Throughput, setup time, lead time and cycle time have been added to assist the VVUQ, see \autoref{sec:relevant-kpis}. They allow PPC VVUQ to be performed. They further can be integrated to check if the Time Model has been learned.
  \item \textbf{duration}: The duration of the process step. This feature is important to understand how long the process step took. It is calculated as the difference between the start and end time of the process step.
  \item \textbf{sequence\_number}: The sequence number of the process step. This feature is important to understand the order of the process steps. It is calculated by grouping the data by the order number and then enumerating the process steps within each order based on the end time. This way, the sequence number enumerates each process step per order. It helps the \texttt{DecisionTree Classifier} to learn the order of the process steps.
  \item \textbf{is\_not\_weekday}: A binary feature indicating whether the process step occurred on a weekend (1) or a weekday (0). No activities have been performed on weekends in the factory, so weekend activity is an anomaly.
  \item \textbf{is\_break}: A binary feature indicating whether the process step occurred during a break (1) or not (0). No activities have been performed during breaks in the factory, so break activity is an anomaly as well.
  \item \textbf{hour\_of\_day}: The hour of the day when the process step occurred. This feature is important to understand the time of day when the process step took place. It is calculated as the hour of the start time of the process step.
  \item \textbf{day\_of\_week}: The day of the week when the process step occurred. It is also calculated as the day of the week of the start time of the process step.
  \item \textbf{day\_of\_week\_sin and day\_of\_week\_cos}: A periodic time feature representing the sine and cosine of the day of the week, which helps capture weekly patterns in the data.
  \item \textbf{hour\_of\_day\_cos and hour\_of\_day\_sin}: A periodic time feature representing the cosine and sine of the hour of the day, which helps capture daily patterns in the data.
\end{itemize}

Days and hours are cyclical features that require special handling. Representing them as raw integers fails to capture their continuity (e.g., hour 23 is close to hour 0). To address this, we transform the feature $x$ with period $P$ using sine and cosine functions, effectively mapping it onto a unit circle:
\begin{equation}
  x_{\sin} = \sin\left(\frac{2 \pi x}{P}\right) \quad ; \quad
  x_{\cos} = \cos\left(\frac{2 \pi x}{P}\right)
  \label{eq:sincos_transform}
\end{equation}
This provides a continuous, two-dimensional representation $(x_{\cos}, x_{\sin})$ that preserves the cyclical nearness of values.

In this work, this transformation is applied to:

\begin{itemize}
  \item \textbf{Hour of Day (`hour\_of\_day`):} $P=24$. Features: $\text{hour\_of\_day}_{\sin}$, $\text{hour\_of\_day}_{\cos}$.
  \item \textbf{Day of Week (`day\_of\_week`):} $P=7$. Features: $\text{day\_of\_week}_{\sin}$, $\text{day\_of\_week}_{\cos}$.
\end{itemize}

Figure \ref{fig:time-encoding} illustrates this concept for the hour of the day. This encoding helps machine learning models better understand and utilize the cyclical nature of time.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[
      scale=2.5, % Adjust scale to change overall size
      hour dot/.style={circle, fill=blue!70, inner sep=1.5pt},
      hour label/.style={font=\small}
    ]

    % Draw the unit circle
    \draw [gray, dashed] (0,0) circle (1);

    % Draw axes
    \draw [->, gray!80] (-1.3,0) -- (1.3,0) node[right, black] {$x_{\cos}$};
    \draw [->, gray!80] (0,-1.3) -- (0,1.3) node[above, black] {$x_{\sin}$};
    \node[above=5pt] at (current bounding box.north) {Hour of Day Encoded on Unit Circle}; % Title


    % Plot hour points and labels
    \foreach \h in {0,...,23} {
        % Calculate angle in degrees (TikZ trigonometric functions use degrees)
        \pgfmathsetmacro{\angle}{\h * 360 / 24}

        % Draw the point for the hour
        \node[hour dot] at (\angle:1) {}; % Place dot using polar coordinates (angle:radius)

        % Add labels for key hours slightly outside the circle
        \ifnum \h = 0 \node[hour label, anchor=west] at (\angle:1.15) {0h (Midnight)}; \fi
        \ifnum \h = 3 \node[hour label, anchor=south west] at (\angle:1.15) {3h}; \fi
        \ifnum \h = 6 \node[hour label, anchor=south] at (\angle:1.15) {6h (Morning)}; \fi
        \ifnum \h = 9 \node[hour label, anchor=south east] at (\angle:1.15) {9h}; \fi
        \ifnum \h = 12 \node[hour label, anchor=east] at (\angle:1.15) {12h (Noon)}; \fi
        \ifnum \h = 15 \node[hour label, anchor=north east] at (\angle:1.15) {15h}; \fi
        \ifnum \h = 18 \node[hour label, anchor=north] at (\angle:1.15) {18h (Evening)}; \fi
        \ifnum \h = 21 \node[hour label, anchor=north west] at (\angle:1.15) {21h}; \fi
      }

  \end{tikzpicture}
  \caption{Sine and Cosine transformation of the hour of the day (0-23). Each hour is mapped to a point $(x_{\cos}, x_{\sin})$ on the unit circle (blue dots). Key hours are labeled, demonstrating how the transformation preserves cyclical continuity (hour 23 is near hour 0).}
  \label{fig:time-encoding}
\end{figure}

\section{Experiments}
With the the integration of these features, AFS can be performed. Most features are time based an can be used to check for the Time Model component. The learning of the resource model and the resource capacity model is faciliated by the categorical feature \texttt{resource\_id} with respect to the sequence number and time information. In general, time features have been found useful because they inherently encode order information as well. Now we restrict the whitebox and blackbox model on subsets of features and gather evidence if they were able to distinguish the real processes (with the label \texttt{is\_valid} equal to $1$) from the simulated twin data with the zeroed label.

\subsection{Statistical Testing Framework}
\label{sec:model-logic}
\noindent The null hypothesis ($H_0$) posits that the SBDT accurately represents the real system with respect to the features $\mathcal{F}_c$, meaning the data distributions are indistinguishable:
\begin{equation}
  H_0: \mathcal{D}_{real}(\mathbf{X} | \mathcal{F}_c) = \mathcal{D}_{sim}(\mathbf{X} | \mathcal{F}_c)
  \label{eq:h0}
\end{equation}

\noindent The alternative hypothesis ($H_1$) posits that the SBDT does \textit{not} accurately represent the real system, and the distributions are statistically distinguishable using the features $\mathcal{F}_c$:
\begin{equation}
  H_1: \mathcal{D}_{real}(\mathbf{X} | \mathcal{F}_c) \neq \mathcal{D}_{sim}(\mathbf{X} | \mathcal{F}_c)
  \label{eq:h1}
\end{equation}

\noindent Under this framework, if a classifier trained on the feature set $\mathcal{F}_c$ achieves performance significantly better than chance at distinguishing between real ($y=1$) and simulated ($y=0$) data, it provides evidence to reject $H_0$ in favour of $H_1$. This would imply that the SBDT component $c$ has \emph{not} been learned accurately, as detectable discrepancies exist. Conversely, if the classifier performs poorly (close to random chance), we fail to reject $H_0$, suggesting that, based on the features $\mathcal{F}_c$, the simulated data is consistent with the real data for that component.


While classification reports provide metrics like accuracy, F1-score, precision, and recall, these point estimates do not directly quantify the statistical significance of the separation between the two classes (real vs. simulated) against the null hypothesis $H_0$. To determine if the classifier's ability to distinguish the data sources is statistically significant (i.e., unlikely to be due to random chance), we employ \textbf{Permutation Testing}.

Permutation testing is a non-parametric statistical significance test well-suited for this classification task. It directly assesses the likelihood of observing the classifier's performance (or better) under the null hypothesis ($H_0$) that the labels (real/simulated) are independent of the features $\mathcal{F}_c$. The procedure is as follows:

\begin{enumerate}
  \item \textbf{Compute Observed Statistic:} Train and evaluate the classifier (e.g., Decision Tree or BiLSTM) on the original dataset using the feature subset $\mathcal{F}_c$. Record the chosen performance metric (e.g., Accuracy or AUC), denoted as $S_{obs}$. This is typically obtained from cross-validation or a hold-out test set.
  \item \textbf{Generate Null Distribution:} Repeat the following steps a large number of times ($N$, e.g., $N=1000$ or $N=5000$):
        \begin{itemize}
          \item Create a permuted dataset by randomly shuffling the true labels ($y=0$ and $y=1$) across all data instances $\mathbf{X}$ in the test set (or across folds in cross-validation). This simulates the scenario where the features $\mathcal{F}_c$ carry no information about the data's origin (real vs. simulated), consistent with $H_0$.
          \item Train and evaluate the classifier on this permuted dataset using the same procedure as in step 1. Record the performance metric, $S_{perm}$.
        \end{itemize}
        This generates an empirical distribution of the performance metric under the null hypothesis.
  \item \textbf{Calculate p-value:} The p-value is the proportion of permutation scores ($S_{perm}$) that are greater than or equal to the observed score ($S_{obs}$):
        \begin{equation}
          p = \frac{\sum_{i=1}^{N} \mathbb{I}(S_{perm, i} \ge S_{obs})}{N}
          \label{eq:pvalue_perm}
        \end{equation}
        where $\mathbb{I}(\cdot)$ is the indicator function (1 if the condition is true, 0 otherwise).
  \item \textbf{Interpret Result:} Compare the p-value to a pre-defined significance level $\alpha$ (e.g., $\alpha = 0.05$).
        \begin{itemize}
          \item If $p < \alpha$: We reject the null hypothesis $H_0$. There is statistically significant evidence that the classifier can distinguish between real and simulated data based on features $\mathcal{F}_c$. This suggests the SBDT component $c$ was \emph{not} learned accurately.
          \item If $p \ge \alpha$: We fail to reject the null hypothesis $H_0$. There is insufficient evidence to conclude that the distributions are different based on features $\mathcal{F}_c$. This suggests the SBDT component $c$ might be adequately learned (or at least, its inaccuracies are not detectable with this feature set and classifier).
        \end{itemize}
\end{enumerate}

This procedure will be applied for each feature subset $\mathcal{F}_c$ corresponding to the different SBDT model components being evaluated. The resulting p-values provide a rigorous basis for concluding whether observed differences between the real and simulated data are statistically meaningful.

\begin{comment}
Recommended Approach: Train Once, Permute Test Predictions
The key insight is that we can train the model once and then perform permutation testing on the test set predictions:

Train the BiLSTM model on the original training data
Generate predictions on the test set
For permutation testing, only shuffle the test set labels while keeping the predictions fixed
This maintains the statistical soundness while dramatically reducing computation time
Implementation Changes
Here's how to modify your permutation_test function for BiLSTM models:
% Experimental design
% Execution of automated validation
%  model adjustment
% Feature Engineering: Creation of features such as "duration," "sequence_number," and periodic time features (e.g., "day_of_week_sin," "hour_of_day_cos") (→ Reference to 4.3)
% Machine Learning Models: Implementation of a Decision Tree Classifier and an BiLSTM model for validation (→ Reference to 4.3)
% Empirical verification of theoretical V\&V concepts (→ reference to 2.2 and 4.3)

Reporting: In your results section, for each component/feature set, you would report the primary classification metric (e.g., Accuracy or AUC) and the p-value obtained from the permutation test. This allows you to state whether the observed separation capability of the classifier is statistically significant.
\end{comment}

\subsection{Testing the SBDT Components}
To rigorously evaluate the SBDTs fidelity concerning different process aspects, we implemented the hypothesis testing framework outlined previously (testing $H_0$: \autoref{eq:h0} against $H_1$: \autoref{eq:h1}). This involved using machine learning classifiers to determine if statistically significant differences exist between the real process data ($\mathcal{D}_{real}$, labeled $y=1$) and the simulated data ($\mathcal{D}_{sim}$, labeled $y=0$) based on specific feature subsets ($\mathcal{F}_c$) corresponding to key SBDT model components.

The analysis was performed separately for distinct feature subsets $\mathcal{F}_c$, each curated to reflect the behaviour governed by specific SBDT components. Based on the feature engineering described in \autoref{sec:feature-engineering}, subsets were defined for the \texttt{time\_model} (including duration, sequence, cyclical time features), \texttt{resource\_model} (categorical resource, part, process IDs), \texttt{process\_model} (process ID, duration, sequence), \texttt{kpi\_based} (calculated KPIs like throughput, cycle time), and an \texttt{all\_features} set encompassing all available engineered features.

\subsection{Permutation Testing Procedure}

Statistical significance was assessed using \textbf{Permutation Testing}, as described conceptually earlier (\autoref{eq:pvalue_perm}). The practical implementation followed these steps for each feature subset $\mathcal{F}_c$:

\begin{enumerate}
  \item \textbf{Data Splitting:} The combined dataset (\texttt{final\_data}) was split into stratified training ($\mathcal{D}_{train}$) and testing ($\mathcal{D}_{test}$) sets using \texttt{sklearn.model\_selection.train\_test\_split}, ensuring proportional representation of real ($y=1$) and simulated ($y=0$) data in both sets. A random seed was used for reproducibility within a single run.
  \item \textbf{Model Training:} The chosen classifier (Decision Tree or BiLSTM) was trained on $\mathcal{D}_{train}$ using only the features in $\mathcal{F}_c$.
  \item \textbf{Observed Statistic Calculation:} The trained model was evaluated on the original test set $\mathcal{D}_{test}$. The performance metric, specifically the Area Under the Receiver Operating Characteristic Curve (\textbf{ROC AUC}, \texttt{sklearn.metrics.roc\_auc\_score}), was calculated and recorded as the observed statistic, $S_{obs}$. ROC AUC was chosen for its robustness to class imbalance and its ability to measure overall discriminative power.
  \item \textbf{Null Distribution Generation (Permutation):} To generate the null distribution, $N$ permutations (e.g., $N=100$) were performed. Critically, an efficient approach was used, particularly for the computationally intensive BiLSTM model:
        \begin{itemize}
          \item The trained model generated predictions (binary $\hat{\mathbf{y}}_{test}$) and class probabilities (specifically for the positive class, $\hat{\mathbf{p}}_{test}$) on the original test set $\mathcal{D}_{test}$ *once*.
          \item For each permutation $i=1...N$: The *true labels* $\mathbf{y}_{test}$ of the test set were randomly shuffled, creating $\mathbf{y}_{test, perm}^{(i)}$.
          \item The permuted statistic $S_{perm, i}$ was calculated by comparing the shuffled labels $\mathbf{y}_{test, perm}^{(i)}$ against the *fixed* probabilities $\hat{\mathbf{p}}_{test}$ (i.e., calculating ROC AUC between $\mathbf{y}_{test, perm}^{(i)}$ and $\hat{\mathbf{p}}_{test}$). This avoids retraining the model for each permutation. (A similar principle was applied for the Decision Tree, comparing permuted labels against fixed predictions/probabilities).
        \end{itemize}
  \item \textbf{P-value Calculation:} The p-value was computed as the proportion of permutation statistics greater than or equal to the observed statistic, following Eq.~\ref{eq:pvalue_perm}.
        \begin{equation}
          p = \frac{\sum_{i=1}^{N} \mathbb{I}(S_{perm, i} \ge S_{obs})}{N}
          \nonumber % No number as it's conceptually the same as eq:pvalue_perm
        \end{equation}
\end{enumerate}

This can also be summarized in pseudo code:

\begin{algorithm}[H] % Floating environment
  \caption{Multi-Run Permutation Testing for SBDT Component Validation}
  \label{alg:permutation_testing_readable} % New label for readable version
  \begin{algorithmic}[1] % Optional line numbering
    \footnotesize % Font size (Consider trying \small if this spacing allows)
    % -----------

    \Require % Input:
    \Statex \hspace{\algorithmicindent} Preprocessed dataset $\mathcal{D}_{final}$ ($\mathbf{X}$, $y \in \{0=\text{sim}, 1=\text{real}\}$),
    \Statex \hspace{\algorithmicindent} Set of feature subsets $\{\mathcal{F}_c\}_{c=1}^C$ (for components $c$),
    \Statex \hspace{\algorithmicindent} Number of runs $n_{runs}$,
    \Statex \hspace{\algorithmicindent} Number of permutations $N$, \Comment{e.g., 1000 DT / 100 BiLSTM}
    \Statex \hspace{\algorithmicindent} Significance level $\alpha$, \Comment{e.g., 0.01 DT / 0.05 BiLSTM}
    \Statex \hspace{\algorithmicindent} Classifier type $M_{type}$ (Decision Tree or BiLSTM)
    \Ensure % Output:
    \Statex \hspace{\algorithmicindent} Aggregated results $R[c] = \{\bar{S}_{obs}, \sigma_{S_{obs}}, \bar{p}, RR\}$ for each component $c$.
    \Statex \hspace{\algorithmicindent} \Comment{$\bar{S}_{obs}$=mean ROC AUC, $\sigma_{S_{obs}}$=std dev, $\bar{p}$=mean p-val, $RR$=rejection rate}
    \medskip % Add space after inputs/outputs

    \State Initialize overall results dictionary $R \gets \emptyset$
    \medskip

    \ForAll{feature subset $\mathcal{F}_c$ corresponding to component $c$} \Comment{Test each SBDT component}
    \State Initialize run result lists: $S_{obs\_list} \gets [ ]$, $p_{list} \gets [ ]$
    \medskip % Space before inner loop

    \For{$run = 1$ to $n_{runs}$} \Comment{Repeat for stability}
    \State $seed \gets \text{GenerateNewRandomSeed}()$
    \State $(\mathcal{D}_{train}, \mathcal{D}_{test}) \gets \text{SplitDataStratified}(\mathcal{D}_{final}, \mathcal{F}_c, seed)$
    \State $\mathbf{X}_{train}, \mathbf{y}_{train} \gets \text{GetFeaturesAndLabels}(\mathcal{D}_{train})$
    \State $\mathbf{X}_{test}, \mathbf{y}_{test} \gets \text{GetFeaturesAndLabels}(\mathcal{D}_{test})$
    \medskip % Space after data prep

    \State $M \gets \text{TrainModel}(M_{type}, \mathbf{X}_{train}, \mathbf{y}_{train})$ \Comment{Train classifier}
    \medskip % Space after training

    \State $\hat{\mathbf{p}}_{test} \gets \text{PredictProbabilities}(M, \mathbf{X}_{test})$ \Comment{Get fixed probabilities $P(y=1|\mathbf{X}_{test})$}
    \State $S_{obs} \gets \text{CalculateMetric}(\mathbf{y}_{test}, \hat{\mathbf{p}}_{test}, \text{'ROC AUC'})$ \Comment{Observed score; robust calc.}
    \medskip % Space after observed score calc

    \State $S_{perm\_list} \gets [ ]$ \Comment{Store permutation scores}
    \For{$i = 1$ to $N$} \Comment{Generate null distribution}
    \State $\mathbf{y}_{test, perm}^{(i)} \gets \text{ShuffleLabels}(\mathbf{y}_{test})$ \Comment{Permute ground truth}
    \State $S_{perm, i} \gets \text{CalculateMetric}(\mathbf{y}_{test, perm}^{(i)}, \hat{\mathbf{p}}_{test}, \text{'ROC AUC'})$ \Comment{Score vs fixed probs (efficient)}
    \State Append $S_{perm, i}$ to $S_{perm\_list}$
    \EndFor
    \medskip % Space after permutation loop

    \State $count_{ge} \gets \sum_{i=1}^{N} \mathbb{I}(S_{perm\_list}[i] \ge S_{obs})$
    \State $p_{run} \gets count_{ge} / N$ \Comment{p-value for this run}
    \medskip % Space after p-value calc

    \State Append $S_{obs}$ to $S_{obs\_list}$; Append $p_{run}$ to $p_{list}$ \Comment{Store run results (handles NaNs)}
    \EndFor \Comment{End of runs loop}
    \medskip % Space after inner loop

    \State $( \bar{S}_{obs}, \sigma_{S_{obs}}, \bar{p}, RR ) \gets \text{AggregateValidResults}(S_{obs\_list}, p_{list}, \alpha)$ \Comment{Calc stats ignoring NaNs}
    \State $R[c] \gets \{\dots\}$ \Comment{Store aggregated results for component $c$}
    \medskip % Space after aggregation
    \EndFor \Comment{End of feature subset loop}

    \State \Return $R$

  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H] % Floating environment
  \caption{Multi-Run Permutation Testing (Symbolic Notation)}
  \label{alg:permutation_testing_symbolic} % New label for symbolic version
  \begin{algorithmic}[1] % Optional line numbering
    \footnotesize % Font size
    % -----------

    \Require % Input:
    \Statex \hspace{\algorithmicindent} Dataset $\mathcal{D}_{final}$ (Features $\mathbf{X}$, Labels $y \in \{0,1\}$),
    \Statex \hspace{\algorithmicindent} Feature Subsets $\{\mathcal{F}_c\}_{c=1}^C$,
    \Statex \hspace{\algorithmicindent} Runs $n_{runs}$, Permutations $N$, Significance $\alpha$, Model Type $M_{type}$
    \Ensure % Output:
    \Statex \hspace{\algorithmicindent} Results $R : \{c \mapsto (\bar{S}_{obs}, \sigma_{S_{obs}}, \bar{p}, RR)\}$ \Comment{Map component to aggregated stats}
    \medskip

    \State $R := \emptyset$ \Comment{Initialize results map}
    \medskip

    \ForAll{feature subset $\mathcal{F}_c$ for component $c$}
    \State $S_{obs\_list} := [\,]$; $p_{list} := [\,]$ \Comment{Initialize lists for run results}
    \medskip
    \For{$run = 1$ to $n_{runs}$}
    \State $seed \leftarrow \text{RandomSeed}()$
    \State $(\mathcal{D}_{train}, \mathcal{D}_{test}) \leftarrow \text{Split}(\mathcal{D}_{final}, \mathcal{F}_c, seed)$ \Comment{Stratified split}
    \State $M \leftarrow \text{Train}(M_{type}, \mathcal{D}_{train})$ \Comment{Train on $(\mathbf{X}_{train}, \mathbf{y}_{train})$ using features $\mathcal{F}_c$}
    \medskip

    \State $\hat{\mathbf{p}}_{test} \leftarrow M(\mathbf{X}_{test})$ \Comment{Predict $P(y=1|\mathbf{X}_{test})$ using trained model $M$}
    \State $S_{obs} \leftarrow \text{ROC\_AUC}(\mathbf{y}_{test}, \hat{\mathbf{p}}_{test})$ \Comment{Observed score (robust calc.)}
    \medskip

    \State $S_{perm\_list} := [\,]$
    \For{$i = 1$ to $N$}
    \State $\mathbf{y}_{test, perm}^{(i)} \leftarrow \text{Permute}(\mathbf{y}_{test})$ \Comment{Randomly shuffle test labels}
    \State $S_{perm, i} \leftarrow \text{ROC\_AUC}(\mathbf{y}_{test, perm}^{(i)}, \hat{\mathbf{p}}_{test})$ \Comment{Score vs fixed $\hat{\mathbf{p}}_{test}$ (efficient)}
    \State Append $S_{perm, i}$ to $S_{perm\_list}$
    \EndFor
    \medskip

    \State $count_{ge} \leftarrow \sum_{i=1}^{N} \mathbb{I}(S_{perm\_list}[i] \ge S_{obs})$
    \State $p_{run} \leftarrow count_{ge} / N$ \Comment{p-value for this run}
    \medskip

    \State Append $S_{obs}$ to $S_{obs\_list}$; Append $p_{run}$ to $p_{list}$ \Comment{Store run results}
    \EndFor \Comment{End runs}
    \medskip

    \State $\bar{S}_{obs} \leftarrow \text{Mean}(S_{obs\_list} | \text{not NaN})$ \Comment{Calculate mean of valid observed scores}
    \State $\sigma_{S_{obs}} \leftarrow \text{StdDev}(S_{obs\_list} | \text{not NaN})$ \Comment{Calculate std dev of valid observed scores}
    \State $\bar{p} \leftarrow \text{Mean}(p_{list} | \text{not NaN})$ \Comment{Calculate mean of valid p-values}
    \State $RR \leftarrow \text{Mean}(\mathbb{I}(p < \alpha) \mid p \in p_{list}, p \neq \text{NaN})$ \Comment{Rejection Rate on valid p-values}
    \State $R[c] \leftarrow (\bar{S}_{obs}, \sigma_{S_{obs}}, \bar{p}, RR)$ \Comment{Store aggregated results for component $c$}
    \medskip
    \EndFor \Comment{End feature subsets}

    \State \Return $R$

  \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H] % Floating environment
  \caption{Multi-Run Permutation Testing (Balanced Notation)}
  \label{alg:permutation_testing_balanced} % New label for balanced version
  \begin{algorithmic}[1] % Optional line numbering
    \footnotesize % Font size
    % -----------

    \Require % Input:
    \Statex \hspace{\algorithmicindent} Dataset $\mathcal{D}_{final}$ (Features $\mathbf{X}$, Labels $y \in \{0,1\}$),
    \Statex \hspace{\algorithmicindent} Feature Subsets $\{\mathcal{F}_c\}_{c=1}^C$, Runs $n_{runs}$, Permutations $N$, Significance $\alpha$, Model Type $M_{type}$
    \Ensure % Output:
    \Statex \hspace{\algorithmicindent} Results $R[c] = (\bar{S}_{obs}, \sigma_{S_{obs}}, \bar{p}, RR)$ \Comment{Mean/Std ROC AUC, Mean p-val, Rejection Rate}
    \medskip

    \State $R \leftarrow \emptyset$ \Comment{Initialize results dictionary}
    \medskip

    \ForAll{feature subset $\mathcal{F}_c$ for component $c$} \Comment{Test each component}
    \State $S_{obs\_list} \leftarrow []$; $p_{list} \leftarrow []$ \Comment{Initialize lists for run results}
    \medskip
    \For{$run = 1$ to $n_{runs}$} \Comment{Repeat for stability}
    \State $seed \leftarrow \text{RandomSeed}()$
    \State $(\mathcal{D}_{train}, \mathcal{D}_{test}) \leftarrow \text{SplitData}(\mathcal{D}_{final}, \mathcal{F}_c, seed)$ \Comment{Stratified split}
    \State $M \leftarrow \text{TrainModel}(M_{type}, \mathcal{D}_{train})$ \Comment{Train on $(\mathbf{X}_{train}, \mathbf{y}_{train})$}
    \medskip % Space

    \State $\hat{\mathbf{p}}_{test} \leftarrow \text{PredictProbabilities}(M, \mathbf{X}_{test})$ \Comment{Get fixed $P(y=1|\mathbf{X}_{test})$}
    \State $S_{obs} \leftarrow \text{CalculateMetric}(\mathbf{y}_{test}, \hat{\mathbf{p}}_{test}, \text{'ROC AUC'})$ \Comment{Observed score (robust calc.)}
    \medskip % Space

    \State $S_{perm\_list} \leftarrow []$ \Comment{Store permutation scores}
    \For{$i = 1$ to $N$} \Comment{Generate null distribution}
    \State $\mathbf{y}_{test, perm}^{(i)} \leftarrow \text{ShuffleLabels}(\mathbf{y}_{test})$ \Comment{Permute true labels}
    \State $S_{perm, i} \leftarrow \text{CalculateMetric}(\mathbf{y}_{test, perm}^{(i)}, \hat{\mathbf{p}}_{test}, \text{'ROC AUC'})$ \Comment{Score vs fixed probs (efficient)}
    \State Append $S_{perm, i}$ to $S_{perm\_list}$
    \EndFor
    \medskip % Space

    \State $count_{ge} \leftarrow \sum_{i=1}^{N} \mathbb{I}(S_{perm\_list}[i] \ge S_{obs})$
    \State $p_{run} \leftarrow count_{ge} / N$ \Comment{p-value for this run}
    \medskip % Space

    \State Append $S_{obs}$ to $S_{obs\_list}$; Append $p_{run}$ to $p_{list}$ \Comment{Store run results (handles NaNs)}
    \EndFor \Comment{End runs}
    \medskip % Space

    \State $( \bar{S}_{obs}, \sigma_{S_{obs}}, \bar{p}, RR ) \leftarrow \text{AggregateValidResults}(S_{obs\_list}, p_{list}, \alpha)$ \Comment{Calculate stats over valid runs}
    \State $R[c] \leftarrow (\bar{S}_{obs}, \sigma_{S_{obs}}, \bar{p}, RR)$ \Comment{Store aggregated results}
    \medskip % Space
    \EndFor \Comment{End feature subsets}

    \State \Return $R$

  \end{algorithmic}
\end{algorithm}



\section{Results and Interpretation}

The feature subsets are equal to the model components, as noted. The following UpSet plot summarizes the feature subsets used for the SBDT components. The UpSet plot is a visualization technique that provides an alternative to Venn diagrams for representing the intersections of multiple sets. It is particularly useful when dealing with a large number of sets or when the intersections are complex:

INSERT
Based on the aggregated results, the fidelity of the SBDT component corresponding to the feature set $\mathcal{F}_c$ was assessed. A high \textit{rejection rate} (e.g., consistently above 0.9 across the 10 runs) was interpreted as strong evidence against the null hypothesis $H_0$. This indicates that the classifier could reliably distinguish between real and simulated data based on the features $\mathcal{F}_c$, suggesting that the corresponding SBDT component was \textit{not learned accurately} (labeled 'INACCURATE'). Conversely, a low rejection rate suggests insufficient evidence to reject $H_0$, implying the SBDT component might be \texttt{adequately learned} ('ACCURATE'), or at least its potential inaccuracies were not detectable by the classifier using the given features.


% xAI
% Analysis of validation metrics (→ Reference to 4.4)
% Discrepancies and Anomalies: Identification of discrepancies between real and simulated data (→ Reference to 7.3)
% Evaluation of results in the context of V\&V theory (→ reference to 2.2)

\section{Comparison with Manual Validation Methods}
% Effort analysis
% Quality comparison
% Cost-benefit analysis
% Empirical evidence for the advantages of automated V\&V (→ reference to 3.2 and 7.3)