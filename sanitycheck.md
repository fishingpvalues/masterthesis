# **5.X Validation of Classifier Discrimination via Identical Data Control Experiments**

## **5.X.1 Motivation and Justification: Addressing Potential Confounding Factors in High AUC Scores**

The empirical validation presented earlier in this chapter demonstrated notably high Area Under the Receiver Operating Characteristic Curve (ROC AUC) scores when employing both Decision Tree and BiLSTM classifiers to distinguish between real-world process event data (labeled y=1) and simulated data generated by the Simulation-Based Digital Twin (SBDT) (labeled y=0), as detailed in Tables 5.2 and 5.3 \[source: 1368, 1388\]. Within the proposed validation framework, these high AUC scores are interpreted as indicative of significant, detectable discrepancies between the real and simulated datasets. This suggests that the SBDT, for the specific components under evaluation via Adaptive Feature Selection (AFS), exhibits lower fidelity in replicating the complexities and nuances present in the real process data \[source: 1493, 1530\]. The framework leverages the classifier's ability to discriminate between the two data sources as a proxy measure for simulation fidelity, positioning the classification task as a form of Classifier Two-Sample Test (C2ST).1  
However, achieving ROC AUC scores approaching 1.0 warrants careful scrutiny. While desirable in many classification tasks, near-perfect scores in a real-versus-simulation context can be ambiguous.3 Such high performance might not exclusively arise from the classifier learning genuine differences in process dynamics captured (or missed) by the SBDT. Instead, it could potentially stem from the classifier exploiting unintended artifacts within the data or spurious correlations introduced during data preprocessing or feature engineering.6 These artifacts might be subtle characteristics inadvertently linked to the data source (real or simulated) but unrelated to the actual process behavior the SBDT aims to model. Relying on such spurious correlations represents a significant pitfall in machine learning validation, potentially leading to overly optimistic performance estimates and models that fail to generalize.8 For instance, subtle differences in timestamp formatting, missing value patterns resulting from distinct processing pipelines \[source: 1152, 1264-1313\], or even characteristics introduced by feature engineering techniques like cyclical encoding \[source: 1284-1313, Figure 5.5\] could serve as unintended cues for the classifier.  
To address this critical concern and bolster the validity of the framework's interpretation, it is necessary to conduct specific control experiments, often referred to as "sanity checks".6 Sanity checks are designed to verify whether a model achieves its performance for the intended reasons or due to confounding factors. In this context, an "Identical Data Control Experiment" is proposed. This experiment serves as a targeted sanity check and artifact detection test, specifically designed to isolate the effect of the assigned label (real vs. simulated) from any inherent patterns or artifacts within the feature data itself.  
This control experiment differs from standard label permutation tests.18 Label permutation tests evaluate classifier significance against the null hypothesis that features and labels are independent by randomly shuffling labels. In contrast, the Identical Data Control Experiment utilizes *identical* feature sets for both classes but assigns *opposite* labels (y=1 to one copy, y=0 to the other). This directly probes whether artifacts correlated with the *original data source* exist, allowing the classifier to distinguish between the copies even though the feature data is the same. If the classifier can achieve high performance in this artificial scenario, it strongly suggests that the high performance in the original real-vs-sim experiment was driven by these artifacts rather than genuine differences in process representation.  
Therefore, this control experiment acts as a crucial validation step for the C2ST approach employed in this thesis. It directly assesses whether the test statistic (ROC AUC) derived from the primary classification task \[source: 1368, 1388\] is a reliable indicator of true distributional differences between the real (P) and simulated (Q) data, or if it is inflated by artifacts.2 Given the potential for subtle artifacts in complex event log data and the near-perfect scores observed, this validation is not merely supplementary but necessary to ensure a robust and trustworthy interpretation of the framework's results. Failure to perform such checks could lead to erroneous conclusions about SBDT component fidelity.6

## **5.X.2 Methodology: Identical Data Control Experiments**

The primary objective of these control experiments is to empirically determine whether the Decision Tree and BiLSTM classifiers can distinguish between datasets based *solely* on the assigned binary label (is\_valid \= 1 vs. is\_valid \= 0\) when the underlying feature data for both labels is identical. This methodology allows for the isolation and assessment of potential confounding artifacts within the data that might be correlated with the original data source (real or simulated). Two distinct control experiments are conducted: one using the real dataset and one using the simulated dataset.  
**Procedure 1: Identical Real Data Test (IRDT)**  
This test directly addresses the concern that artifacts within the real data, or introduced during its specific preprocessing, might be responsible for the high AUC scores observed in the original real-vs-sim classification.

1. **Data Selection:** The preprocessed real\_data dataset, containing features X and originally assigned label y=1, as utilized in the main experiments (Chapter 5, Section 5.3 \[source: 1264-1313\]), is isolated.  
2. **Duplication:** An exact, byte-for-byte duplicate of this dataset (X\_copy) is created. This ensures that the feature vectors in both sets are identical.  
3. **Label Assignment:** The label y=0 is assigned to all instances within the duplicated dataset, X\_copy.  
4. **Concatenation:** The original real\_data (X, y=1) is concatenated with the duplicated and relabeled data (X\_copy, y=0) to form a new dataset, designated final\_data\_IRDT. Crucially, no data from the sim\_data dataset is included in final\_data\_IRDT.  
5. **Classifier Training & Evaluation:** Both the Decision Tree and BiLSTM classifiers are trained and evaluated on the final\_data\_IRDT dataset. This process must meticulously replicate the procedures used in the original experiments described in Chapter 5:  
   * The same feature sets, including those defined by the Adaptive Feature Selection (AFS) mappings for different SBDT components (Sections 5.4/5.5 \[source: 1314, 1327, 1345\]), are used.  
   * The *exact same* train/test splitting strategy employed previously is applied to final\_data\_IRDT. **Caution:** Standard random splitting could lead to data leakage if identical copies of an original instance fall into both training and testing sets. To mitigate this, strategies like splitting the original real\_data *before* duplication and label assignment, or employing GroupKFold cross-validation 25 using a unique identifier for each original real data instance (e.g., potentially derived from order\_id if appropriate, as suggested in user notes) should be considered to ensure strict separation and prevent leakage.10  
   * Performance is evaluated using the ROC AUC metric.  
   * While permutation testing was used for significance in the main experiments, the primary outcome here is the magnitude of the ROC AUC score relative to the random chance baseline.

**Procedure 2: Identical Simulated Data Test (ISDT)**  
This test serves as a complementary diagnostic, investigating whether artifacts or learnable patterns exist within the simulated data itself.

1. **Data Selection:** The preprocessed sim\_data dataset, containing features X\_sim and originally assigned label y=0 (Section 5.3 \[source: 1262\]), is isolated.  
2. **Duplication:** An exact duplicate (X\_sim\_copy) is created.  
3. **Label Assignment:** The label y=1 is assigned to all instances within X\_sim\_copy.  
4. **Concatenation:** The original sim\_data (X\_sim, y=0) is concatenated with the duplicated and relabeled data (X\_sim\_copy, y=1) to form final\_data\_ISDT. No real\_data is included.  
5. **Classifier Training & Evaluation:** Step 5 from the IRDT procedure is repeated, using the final\_data\_ISDT dataset.

**Justification for Two Tests:**  
Conducting both the IRDT and ISDT provides a more comprehensive diagnostic picture \[source: User Query\]. The IRDT directly tests the primary hypothesis regarding artifacts in the real data potentially confounding the original real-vs-sim comparison. The ISDT, conversely, probes the internal characteristics of the simulation output. For example, a high AUC in the ISDT might suggest the simulation produces overly deterministic or repetitive patterns, or that preprocessing steps affected the simulated data in a uniquely identifiable way \[source: User Query\]. Achieving the expected outcome (AUC ≈ 0.5) in *both* tests significantly strengthens the conclusion that the original high AUC scores reflect genuine real-vs-simulated differences.  
**Expected Outcome (Ideal):**  
For both the IRDT and ISDT, the theoretically expected ROC AUC score is 0.5. This signifies random guessing performance. Since the feature distributions for the y=1 and y=0 classes are identical by construction in these control experiments, a classifier properly relying on feature information should be unable to distinguish between them better than chance.3  
The "ideal" outcome of AUC ≈ 0.5 in these control experiments directly relates to the null hypothesis (H0) framework presented earlier (Equation 5.2 \[source: 1314\]). In the original C2ST context, H0 stated that the distributions of real and simulated data were indistinguishable (D\_real \= D\_sim). In the IRDT control experiment, the null hypothesis H0\_IRDT: D\_real\_copy(X|y=0) \= D\_real\_original(X|y=1) is *true by design* because the feature data X is identical. A resulting AUC ≈ 0.5 signifies that the classifier fails to reject this true null hypothesis.1 If the classifier behaves correctly in the control setting (i.e., fails to reject H0 when H0 is true), it lends credibility to its behavior in the original experiment. Specifically, it increases confidence that rejecting H0 in the original real-vs-sim comparison (indicated by a high AUC) was due to genuine differences between D\_real and D\_sim (i.e., H1 being true), rather than being an artifact of the classifier incorrectly finding distinctions where none should exist based on features alone.

## **5.X.3 Results of Control Experiments**

The results of the Identical Real Data Test (IRDT) and Identical Simulated Data Test (ISDT) are presented below. The primary metric reported is the ROC AUC, calculated using the same cross-validation and evaluation procedures as employed in the main experiments detailed earlier in Chapter 5\. These results are compared against the theoretical random chance baseline (AUC \= 0.5) and the original ROC AUC scores obtained when classifying real versus simulated data (from Tables 5.2 and 5.3).  
Table 5.X provides a summary of these comparative results for both the Decision Tree and BiLSTM classifiers, considering the 'All Features' set as well as representative feature subsets corresponding to specific SBDT components (e.g., time\_model, resource\_model) as defined by the AFS methodology \[source: 1314, 1327, 1345\].  
**Table 5.X: Results of Identical Data Control Experiments (ROC AUC)**

| Classifier | Data Source | Feature Set (AFS) | Original AUC (Real vs. Sim) \[Ch 5\] | IRDT AUC (Real vs. Real-Copy) | ISDT AUC (Sim vs. Sim-Copy) | Expected AUC (Random Chance) |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| Decision Tree | Real/Sim | All Features |  | \*\* | \*\* | \~0.5 |
| Decision Tree | Real/Sim | time\_model |  | \*\* | \*\* | \~0.5 |
| Decision Tree | Real/Sim | resource\_model |  | \*\* | \*\* | \~0.5 |
| ... (other DT AFS) | ... | ... | ... | \*\* | \*\* | \~0.5 |
| BiLSTM | Real/Sim | All Features |  | \*\* | \*\* | \~0.5 |
| BiLSTM | Real/Sim | time\_model |  | \*\* | \*\* | \~0.5 |
| BiLSTM | Real/Sim | resource\_model |  | \*\* | \*\* | \~0.5 |
| ... (other BiLSTM AFS) | ... | ... | ... | \*\* | \*\* | \~0.5 |

*(Note: Replace "" and "" placeholders with the specific numerical values obtained from the original experiments and the control experiments conducted as part of this validation.)*  
The presentation of results in this tabular format facilitates a direct comparison between the performance achieved in the original classification task and the performance observed under the controlled conditions where feature data was identical. This comparison is essential for interpreting the validity of the original findings. While formal statistical tests could determine if the IRDT and ISDT AUC scores are significantly different from 0.5, the primary interpretation hinges on the practical deviation of these scores from the random chance baseline.

## **5.X.4 Interpretation and Discussion**

The interpretation of the results from the Identical Data Control Experiments (IRDT and ISDT) is crucial for validating the findings presented in Chapter 5 and the overall framework. Two primary scenarios arise based on the observed ROC AUC scores in Table 5.X.  
**Scenario 1: Control Tests Pass (AUC ≈ 0.5 for both IRDT and ISDT)**

* **Interpretation:** If the ROC AUC scores obtained in both the IRDT and ISDT experiments are close to the random chance baseline of 0.5, this constitutes the desired outcome. It strongly suggests that the classifiers (Decision Tree and BiLSTM) were unable to effectively distinguish between the two classes when the underlying feature data was identical. The minor deviations from 0.5, if any, would not indicate a meaningful ability to discriminate based on artifacts.  
* **Implication:** This result provides substantial evidence against the hypothesis that the original high AUC scores (Tables 5.2 & 5.3 \[source: 1368, 1388\]) were primarily driven by data-internal artifacts, preprocessing biases, or spurious correlations unrelated to the simulation's fidelity. It validates the framework's fundamental premise: the high discriminative power observed in the original real-vs-sim comparison likely reflects genuine, detectable differences between the real process behavior and the SBDT's simulated output for the tested components \[source: 1493, 1530\]. The classification framework, in this case, functions reliably as a C2ST 1, sensitive to differences indicative of SBDT fidelity issues.  
* **Consequent Actions:** With this validation, the focus shifts towards utilizing the framework's original results (component-wise AUC scores and Relative Reduction (RR) values) for their intended purpose: identifying specific SBDT components that require refinement ("Reparieren und Neu machen" \[source: User Query\]). The high AUC scores for certain components can now be interpreted with greater confidence as signals of low fidelity. Subsequent diagnostic steps, such as detailed feature importance analysis 30 and error analysis 34 applied to the *original* real-vs-sim classification results, become more meaningful. These analyses can help pinpoint *why* the SBDT failed to accurately simulate the behavior governed by specific components (e.g., which features related to the time\_model or resource\_model were most discriminative). Analyzing results per order\_id \[source: User Query\] could further reveal specific process instances where simulation deviates most significantly.

**Scenario 2: Control Tests Fail (AUC significantly \> 0.5 for IRDT and/or ISDT)**

* **Interpretation:** If the ROC AUC scores in either the IRDT or ISDT (or both) are significantly greater than 0.5, this indicates a critical issue. It means the classifier successfully learned to distinguish between the two classes even when the feature data was identical. This implies the presence of an artifact—either inherent in the source data or introduced during preprocessing, feature engineering, or train/test splitting—that is correlated with the original source label (real=1, sim=0) and allows the classifier to "cheat."  
  * **If IRDT Fails (AUC \> 0.5):** This points towards artifacts within the real\_data itself or, more likely, artifacts introduced during the preprocessing steps applied specifically to it (Section 5.3 \[source: 1264-1313\]). Potential culprits include systematic differences arising from missing value imputation (e.g., filling with zeros \[source: 1152\]), normalization/scaling methods, or the creation of engineered features (e.g., cyclical time features \[source: 1284-1313\], KPIs) that inadvertently encode information related to the original data structure even within the real dataset copies.10 Another significant possibility is data leakage during the train/test split applied to the identical data, where information inadvertently crosses the boundary between training and testing sets.8  
  * **If ISDT Fails (AUC \> 0.5):** This suggests learnable patterns or artifacts exist *within the simulated data itself*. This could arise from insufficient stochasticity in the simulation model (OFacT \[source: 1206\]), systematic patterns introduced by the simulation logic or control flow \[source: 1211\], or potentially differential effects of the preprocessing pipeline on the simulated data compared to the real data \[source: User Query\].  
* **Implication:** A failed control test invalidates the straightforward interpretation of the original high AUC scores \[source: 1368, 1388\] as purely reflecting SBDT inaccuracy. The framework's ability to detect differences is confirmed, but these differences are confounded by artifacts. The interpretation must be revised to state that the framework detects significant distributional differences, which *may* be due to SBDT inaccuracies *or* data/processing artifacts, necessitating further investigation \[source: User Query\].  
* **Consequent Actions:** A failed test necessitates a debugging phase ("Reparieren und Neu machen" \[source: User Query\]).  
  1. **Identify Source:** The failed control experiment itself becomes a diagnostic tool. Apply feature importance techniques (e.g., analyzing Decision Tree splits 30, permutation importance 33, or SHAP/LIME for the BiLSTM 43) and error analysis 34 to the *model trained on the failed control data* (IRDT or ISDT). This analysis aims to identify *which specific features* or data characteristics allowed the classifier to spuriously distinguish between the identical copies ("Part ID an welchen Datensätzen differenziert er?" \[source: User Query\]). Investigation should focus on potentially problematic features like IDs (part\_id, resource\_id, process\_id) \[source: 1461\] or engineered features (time features, KPIs) \[source: 1284-1313\]. Analyzing misclassified instances can reveal patterns related to specific data segments or processing steps.35  
  2. **Review Pipeline:** Conduct a meticulous review of the entire data preprocessing pipeline (Section 5.3 \[source: 1264-1313\]). Examine steps like filtering \[source: 1274\], ID mapping \[source: 1271\], cleaning \[source: 1273\], imputation \[source: 1152\], normalization, and feature creation \[source: 1284-1313\] for potential sources of introduced artifacts.8 Critically assess the train/test splitting methodology for potential data leakage, considering alternatives like GroupKFold based on order\_id to ensure temporal or group integrity.13  
  3. **Remediate:** Based on the findings from the source identification and pipeline review, implement necessary corrections. This might involve modifying preprocessing steps, removing or adjusting problematic engineered features 53, or adopting a more robust train/test splitting strategy.  
  4. **Re-validate:** After implementing fixes, the control experiments (IRDT and ISDT) must be re-run. Only when these tests pass (AUC ≈ 0.5) can the original real-vs-sim experiment be conducted again and its results interpreted with confidence regarding SBDT fidelity.

This iterative process highlights how the Identical Data Control Experiment functions not just as a validation check, but also as a powerful debugging mechanism. A failure directly guides the investigation towards the specific elements in the data or pipeline that compromise the integrity of the classification results, enabling targeted remediation.  
**Limitations:**  
It is important to acknowledge the limitations of this control experiment. While passing the IRDT and ISDT significantly increases confidence, it does not offer an absolute guarantee against all possible confounding factors or artifacts. Furthermore, the process of duplicating large datasets and running multiple classifier evaluations can be computationally intensive. The core assumption is that exact duplication adequately captures any potential artifacts present in the original data segments; complex temporal dependencies might not be fully represented in this static duplication, although careful splitting (like GroupKFold) aims to mitigate related leakage issues.

## **5.X.5 Conclusion on Framework Validation**

The Identical Data Control Experiments (IRDT and ISDT) were conducted to rigorously assess the validity of the high ROC AUC scores initially observed when classifying real versus simulated process data. The outcome of these control experiments determines the confidence with which the proposed validation framework can be interpreted as a measure of SBDT fidelity.

* which was addressed by \[specific fix\]. Re-running the IRDT after correction yielded an AUC of \[value near 0.5\], confirming the removal of the artifact."\]\*

Based on these findings, the following conclusion is drawn regarding the validity of the framework and the results presented in Chapter 5:

* **If Control Tests Passed (Initially or After Correction):** The successful outcome of the identical data control experiments strongly supports the conclusion that the high ROC AUC scores reported in Tables 5.2 and 5.3 \[source: 1368, 1388\] are not primarily attributable to data-internal artifacts or preprocessing biases. This validates the core interpretation of the framework: the classifiers are detecting genuine, statistically significant differences between the real and simulated event log data. Consequently, the framework serves as a reliable C2ST 1, and the observed high AUC scores can be confidently interpreted as indicators of lower fidelity in the corresponding SBDT components \[source: 1493, 1530\]. The component-wise analysis presented earlier remains a valid basis for identifying areas for SBDT model improvement.  
* **If Control Tests Failed (and Issue Unresolved):** Should the control experiments reveal persistent artifacts that cannot be readily identified or eliminated, the interpretation of the framework's results must be qualified. While the framework demonstrably detects differences between the real and simulated datasets, these differences cannot be definitively attributed solely to SBDT fidelity issues. The potential influence of confounding artifacts must be acknowledged, and the component-wise results should be viewed as indicative of potential problems requiring further, more nuanced investigation beyond the scope of the current classification approach.

The implementation and analysis of these identical data control experiments underscore the critical importance of incorporating rigorous sanity checks within machine learning validation pipelines, particularly when dealing with complex data sources like simulation outputs and real-world event logs.6 Such checks are indispensable for moving beyond aggregate performance metrics and ensuring that model evaluations are robust, interpretable, and trustworthy, thereby enhancing the credibility of simulation model validation efforts.55

#### **Works cited**

1. REVISITING CLASSIFIER TWO-SAMPLE TESTS \- OpenReview, accessed on April 16, 2025, [https://openreview.net/pdf/f810ace79b2282d0ac7c553a182c01b0670ce8dc.pdf](https://openreview.net/pdf/f810ace79b2282d0ac7c553a182c01b0670ce8dc.pdf)  
2. Classification Accuracy as a Proxy for Two Sample Testing | Request PDF \- ResearchGate, accessed on April 16, 2025, [https://www.researchgate.net/publication/301847701\_Classification\_Accuracy\_as\_a\_Proxy\_for\_Two\_Sample\_Testing](https://www.researchgate.net/publication/301847701_Classification_Accuracy_as_a_Proxy_for_Two_Sample_Testing)  
3. Understanding Area Under the Curve (AUC) in Model Performance, accessed on April 16, 2025, [https://www.lyzr.ai/glossaries/area-under-the-curve/](https://www.lyzr.ai/glossaries/area-under-the-curve/)  
4. AUC in Machine Learning: Evaluating ROC Performance Metrics Effectively, accessed on April 16, 2025, [https://www.numberanalytics.com/blog/auc-machine-learning-roc-performance](https://www.numberanalytics.com/blog/auc-machine-learning-roc-performance)  
5. Caveats and pitfalls of ROC analysis in clinical microarray research (and how to avoid them), accessed on April 16, 2025, [https://academic.oup.com/bib/article/13/1/83/218392](https://academic.oup.com/bib/article/13/1/83/218392)  
6. Detecting Spurious Correlations With Sanity Tests for Artificial ..., accessed on April 16, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC8521929/](https://pmc.ncbi.nlm.nih.gov/articles/PMC8521929/)  
7. Detecting Spurious Correlations With Sanity Tests for Artificial Intelligence Guided Radiology Systems \- Frontiers, accessed on April 16, 2025, [https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2021.671015/full](https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2021.671015/full)  
8. 5 Common Mistakes in Machine Learning and How to Avoid Them \- MachineLearningMastery.com, accessed on April 16, 2025, [https://machinelearningmastery.com/5-common-mistakes-in-machine-learning-and-how-to-avoid-them/](https://machinelearningmastery.com/5-common-mistakes-in-machine-learning-and-how-to-avoid-them/)  
9. Key concepts, common pitfalls, and best practices in artificial intelligence and machine learning: focus on radiomics \- PMC, accessed on April 16, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC9682557/](https://pmc.ncbi.nlm.nih.gov/articles/PMC9682557/)  
10. On the Cross-Validation Bias due to Unsupervised Preprocessing \- Oxford Academic, accessed on April 16, 2025, [https://academic.oup.com/jrsssb/article/84/4/1474/7073256](https://academic.oup.com/jrsssb/article/84/4/1474/7073256)  
11. AUC: A Fatally Flawed Model Metric \- Elder Research, accessed on April 16, 2025, [https://www.elderresearch.com/blog/auc-a-fatally-flawed-model-metric/](https://www.elderresearch.com/blog/auc-a-fatally-flawed-model-metric/)  
12. Why AUC is not a good performance metric for a classification model? \- Cross Validated, accessed on April 16, 2025, [https://stats.stackexchange.com/questions/375351/why-auc-is-not-a-good-performance-metric-for-a-classification-model](https://stats.stackexchange.com/questions/375351/why-auc-is-not-a-good-performance-metric-for-a-classification-model)  
13. Data Leakage \- Kaggle, accessed on April 16, 2025, [https://www.kaggle.com/code/alexisbcook/data-leakage](https://www.kaggle.com/code/alexisbcook/data-leakage)  
14. Data Leakage in Machine Learning \- MachineLearningMastery.com, accessed on April 16, 2025, [https://machinelearningmastery.com/data-leakage-machine-learning/](https://machinelearningmastery.com/data-leakage-machine-learning/)  
15. papers.neurips.cc, accessed on April 16, 2025, [http://papers.neurips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf](http://papers.neurips.cc/paper/8160-sanity-checks-for-saliency-maps.pdf)  
16. Sanity Checks for Saliency Methods Explaining Object Detectors \- ResearchGate, accessed on April 16, 2025, [https://www.researchgate.net/publication/371311722\_Sanity\_Checks\_for\_Saliency\_Methods\_Explaining\_Object\_Detectors](https://www.researchgate.net/publication/371311722_Sanity_Checks_for_Saliency_Methods_Explaining_Object_Detectors)  
17. \[2306.02424\] Sanity Checks for Saliency Methods Explaining Object Detectors \- arXiv, accessed on April 16, 2025, [https://arxiv.org/abs/2306.02424](https://arxiv.org/abs/2306.02424)  
18. www.jmlr.org, accessed on April 16, 2025, [https://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf](https://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf)  
19. Test with permutations the significance of a classification score ..., accessed on April 16, 2025, [https://scikit-learn.org/stable/auto\_examples/model\_selection/plot\_permutation\_tests\_for\_classification.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_permutation_tests_for_classification.html)  
20. permutation\_test\_score — scikit-learn 1.6.1 documentation, accessed on April 16, 2025, [https://scikit-learn.org/stable/modules/generated/sklearn.model\_selection.permutation\_test\_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html)  
21. Lightweight Quality Evaluation of Generated Samples and Generative Models \- Semantic Scholar, accessed on April 16, 2025, [https://pdfs.semanticscholar.org/0dd6/39ee9247a0a7f5f2bcea0ded17dffd0e22e1.pdf](https://pdfs.semanticscholar.org/0dd6/39ee9247a0a7f5f2bcea0ded17dffd0e22e1.pdf)  
22. (PDF) Benchmarking Simulation-Based Inference \- ResearchGate, accessed on April 16, 2025, [https://www.researchgate.net/publication/348426865\_Benchmarking\_Simulation-Based\_Inference](https://www.researchgate.net/publication/348426865_Benchmarking_Simulation-Based_Inference)  
23. Appendices \- Proceedings of Machine Learning Research, accessed on April 16, 2025, [http://proceedings.mlr.press/v130/lueckmann21a/lueckmann21a-supp.pdf](http://proceedings.mlr.press/v130/lueckmann21a/lueckmann21a-supp.pdf)  
24. Intrinsic Alignments of Galaxies: analytic and deep machine learning studies in cosmological simulations \- Carnegie Mellon University, accessed on April 16, 2025, [https://kilthub.cmu.edu/ndownloader/files/52815056](https://kilthub.cmu.edu/ndownloader/files/52815056)  
25. GroupKFold — scikit-learn 1.6.1 documentation, accessed on April 16, 2025, [https://scikit-learn.org/stable/modules/generated/sklearn.model\_selection.GroupKFold.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html)  
26. Time-series grouped cross-validation \- Data Science Stack Exchange, accessed on April 16, 2025, [https://datascience.stackexchange.com/questions/77684/time-series-grouped-cross-validation](https://datascience.stackexchange.com/questions/77684/time-series-grouped-cross-validation)  
27. The effects of data leakage on connectome-based machine learning models \- PMC, accessed on April 16, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC10793416/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10793416/)  
28. 3.1. Cross-validation: evaluating estimator performance \- Scikit-learn, accessed on April 16, 2025, [https://scikit-learn.org/stable/modules/cross\_validation.html](https://scikit-learn.org/stable/modules/cross_validation.html)  
29. On the cross-validation bias due to unsupervised preprocessing, accessed on April 16, 2025, [https://academic.oup.com/jrsssb/article-pdf/84/4/1474/49462949/jrsssb\_84\_4\_1474.pdf](https://academic.oup.com/jrsssb/article-pdf/84/4/1474/49462949/jrsssb_84_4_1474.pdf)  
30. Understanding Feature Importance in Machine Learning | Built In, accessed on April 16, 2025, [https://builtin.com/data-science/feature-importance](https://builtin.com/data-science/feature-importance)  
31. Feature importances with a forest of trees \- Scikit-learn, accessed on April 16, 2025, [https://scikit-learn.org/stable/auto\_examples/ensemble/plot\_forest\_importances.html](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)  
32. Feature importance — Scikit-learn course, accessed on April 16, 2025, [https://inria.github.io/scikit-learn-mooc/python\_scripts/dev\_features\_importance.html](https://inria.github.io/scikit-learn-mooc/python_scripts/dev_features_importance.html)  
33. 4.2. Permutation feature importance — scikit-learn 1.6.1 ..., accessed on April 16, 2025, [https://scikit-learn.org/stable/modules/permutation\_importance.html](https://scikit-learn.org/stable/modules/permutation_importance.html)  
34. Improving Machine Learning Models: A Guide to Error Classification Analysis \- DataHeroes, accessed on April 16, 2025, [https://dataheroes.ai/blog/improving-machine-learning-models-a-guide-to-error-classification-analysis/](https://dataheroes.ai/blog/improving-machine-learning-models-a-guide-to-error-classification-analysis/)  
35. Error Analysis for Machine Learning Classification Models \- Analytics Vidhya, accessed on April 16, 2025, [https://www.analyticsvidhya.com/blog/2021/08/a-quick-guide-to-error-analysis-for-machine-learning-classification-models/](https://www.analyticsvidhya.com/blog/2021/08/a-quick-guide-to-error-analysis-for-machine-learning-classification-models/)  
36. 5 Must-Do Error Analysis Before You Put Your Model in Production \- Neptune.ai, accessed on April 16, 2025, [https://neptune.ai/blog/must-do-error-analysis](https://neptune.ai/blog/must-do-error-analysis)  
37. Data Preprocessing Techniques for AI and Machine Learning Readiness: Scoping Review of Wearable Sensor Data in Cancer Care, accessed on April 16, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11470224/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11470224/)  
38. Data preprocessing \- Machine Learning Lens \- AWS Documentation, accessed on April 16, 2025, [https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/data-preprocessing.html](https://docs.aws.amazon.com/wellarchitected/latest/machine-learning-lens/data-preprocessing.html)  
39. Out-of-time cross-validation strategies for classification in the presence of dataset shift, accessed on April 16, 2025, [https://www.researchgate.net/publication/354054218\_Out-of-time\_cross-validation\_strategies\_for\_classification\_in\_the\_presence\_of\_dataset\_shift](https://www.researchgate.net/publication/354054218_Out-of-time_cross-validation_strategies_for_classification_in_the_presence_of_dataset_shift)  
40. Machine-Learning/Explaining Feature Importance in Decision Trees.md at main \- GitHub, accessed on April 16, 2025, [https://github.com/xbeat/Machine-Learning/blob/main/Explaining%20Feature%20Importance%20in%20Decision%20Trees.md](https://github.com/xbeat/Machine-Learning/blob/main/Explaining%20Feature%20Importance%20in%20Decision%20Trees.md)  
41. Understanding the decision tree structure \- Scikit-learn, accessed on April 16, 2025, [https://scikit-learn.org/stable/auto\_examples/tree/plot\_unveil\_tree\_structure.html](https://scikit-learn.org/stable/auto_examples/tree/plot_unveil_tree_structure.html)  
42. 1.10. Decision Trees — scikit-learn 1.6.1 documentation, accessed on April 16, 2025, [https://scikit-learn.org/stable/modules/tree.html](https://scikit-learn.org/stable/modules/tree.html)  
43. Shap For Lstm Model Interpretation | Restackio, accessed on April 16, 2025, [https://www.restack.io/p/ai-interpretability-answer-shap-lstm-models-cat-ai](https://www.restack.io/p/ai-interpretability-answer-shap-lstm-models-cat-ai)  
44. Interpreting an LSTM through LIME \- Towards Data Science, accessed on April 16, 2025, [https://towardsdatascience.com/interpreting-an-lstm-through-lime-e294e6ed3a03/](https://towardsdatascience.com/interpreting-an-lstm-through-lime-e294e6ed3a03/)  
45. Practical guide to SHAP analysis: Explaining supervised machine learning model predictions in drug development \- PMC, accessed on April 16, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC11513550/](https://pmc.ncbi.nlm.nih.gov/articles/PMC11513550/)  
46. Explainable AI, LIME & SHAP for Model Interpretability | Unlocking AI's Decision-Making, accessed on April 16, 2025, [https://www.datacamp.com/tutorial/explainable-ai-understanding-and-trusting-machine-learning-models](https://www.datacamp.com/tutorial/explainable-ai-understanding-and-trusting-machine-learning-models)  
47. Interpreting an LSTM through LIME | Towards Data Science, accessed on April 16, 2025, [https://towardsdatascience.com/interpreting-an-lstm-through-lime-e294e6ed3a03](https://towardsdatascience.com/interpreting-an-lstm-through-lime-e294e6ed3a03)  
48. Error analysis in machine learning: going beyond predictive performance \- Openlayer, accessed on April 16, 2025, [https://www.openlayer.com/blog/post/systematic-error-analysis](https://www.openlayer.com/blog/post/systematic-error-analysis)  
49. Automating Data Quality Validation for Dynamic Data Ingestion \- OpenProceedings.org, accessed on April 16, 2025, [https://openproceedings.org/2021/conf/edbt/p79.pdf](https://openproceedings.org/2021/conf/edbt/p79.pdf)  
50. JENGA \- A Framework to Study the Impact of Data Errors on the Predictions of Machine Learning Models \- OpenProceedings.org, accessed on April 16, 2025, [https://openproceedings.org/2021/conf/edbt/p134.pdf](https://openproceedings.org/2021/conf/edbt/p134.pdf)  
51. \[D\] \- Data Leakage in Time Series Classification : r/MachineLearning \- Reddit, accessed on April 16, 2025, [https://www.reddit.com/r/MachineLearning/comments/1ietuk8/d\_data\_leakage\_in\_time\_series\_classification/](https://www.reddit.com/r/MachineLearning/comments/1ietuk8/d_data_leakage_in_time_series_classification/)  
52. Cross-Validation for Time Series Forecasting | Python Tutorial \- YouTube, accessed on April 16, 2025, [https://www.youtube.com/watch?v=1rZpbvSI26c\&pp=0gcJCdgAo7VqN5tD](https://www.youtube.com/watch?v=1rZpbvSI26c&pp=0gcJCdgAo7VqN5tD)  
53. Common pitfalls in feature engineering | Statsig, accessed on April 16, 2025, [https://www.statsig.com/perspectives/feature-engineering-pitfalls](https://www.statsig.com/perspectives/feature-engineering-pitfalls)  
54. Best Practices for Feature Engineering \- Elite Data Science, accessed on April 16, 2025, [https://elitedatascience.com/feature-engineering-best-practices](https://elitedatascience.com/feature-engineering-best-practices)  
55. Simulation models verification and validation: Recent development and challenges: A review, accessed on April 16, 2025, [https://www.worldscientific.com/doi/pdf/10.1142/S1793962325300018?download=true](https://www.worldscientific.com/doi/pdf/10.1142/S1793962325300018?download=true)  
56. Modeling and Simulation Verification and Validation Challenges \- Johns Hopkins University Applied Physics Laboratory, accessed on April 16, 2025, [https://secwww.jhuapl.edu/techdigest/content/techdigest/pdf/V25-N02/25-02-Pace.pdf](https://secwww.jhuapl.edu/techdigest/content/techdigest/pdf/V25-N02/25-02-Pace.pdf)  
57. Methods That Support the Validation of Agent-Based Models: An Overview and Discussion, accessed on April 16, 2025, [https://www.jasss.org/27/1/11.html](https://www.jasss.org/27/1/11.html)  
58. DETERMINING M\&S CREDIBILITY: What The Accreditor Needs To Know \- National Defense Industrial Association, accessed on April 16, 2025, [https://www.ndia.org/-/media/sites/ndia/meetings-and-events/divisions/systems-engineering/modeling-and-simulation/past-events/2016-april/elele-james-se-ms-april-2016.pdf](https://www.ndia.org/-/media/sites/ndia/meetings-and-events/divisions/systems-engineering/modeling-and-simulation/past-events/2016-april/elele-james-se-ms-april-2016.pdf)  
59. Model validation and testing in simulation: a literature review \- ResearchGate, accessed on April 16, 2025, [https://www.researchgate.net/publication/327542741\_Model\_validation\_and\_testing\_in\_simulation\_a\_literature\_review](https://www.researchgate.net/publication/327542741_Model_validation_and_testing_in_simulation_a_literature_review)