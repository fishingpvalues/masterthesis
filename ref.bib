@inproceedings{10283684,
  author    = {Rezazadeh, Farhad and Chergui, Hatim and Mangues-Bafalluy, Josep},
  booktitle = {2023 IEEE International Conference on Communications Workshops (ICC Workshops)},
  title     = {Explanation-Guided Deep Reinforcement Learning for Trustworthy 6G RAN Slicing},
  year      = {2023},
  volume    = {},
  number    = {},
  pages     = {1026-1031},
  keywords  = {6G mobile communication;Deep learning;Network slicing;Conferences;Wireless networks;Reinforcement learning;Resource management;6G;network slicing;AI;XAI;XRL;resource allocation},
  doi       = {10.1109/ICCWorkshops57953.2023.10283684}
}
@inproceedings{10566218,
  author    = {Eikså, Kristoffer and Vatne, Jan Erlend and Lekkas, Anastasios M.},
  booktitle = {2024 32nd Mediterranean Conference on Control and Automation (MED)},
  title     = {Explaining Deep Reinforcement Learning Policies with SHAP, Decision Trees, and Prototypes},
  year      = {2024},
  volume    = {},
  number    = {},
  pages     = {700-705},
  keywords  = {Explainable AI;Road vehicles;Decision making;Software algorithms;Closed box;Prototypes;Deep reinforcement learning},
  doi       = {10.1109/MED61351.2024.10566218}
}

@article{aas2021explaining,
  title     = {Explaining individual predictions when features are dependent: More accurate approximations to Shapley values},
  author    = {Aas, Kjersti and Jullum, Martin and L{\o}land, Anders},
  journal   = {Artificial Intelligence},
  volume    = {298},
  pages     = {103502},
  year      = {2021},
  publisher = {Elsevier}
}

@inproceedings{adebayo2018sanity,
  author    = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Sanity Checks for Saliency Maps},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf},
  volume    = {31},
  year      = {2018}
}


@article{alexander2024interrogative,
  title     = {An Interrogative Survey of Explainable AI in Manufacturing},
  author    = {Zoe Alexander and Duen Horng Chau and Christopher Saldana},
  year      = 2024,
  journal   = {IEEE Transactions on Industrial Informatics},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/tii.2024.3361489},
  issn      = 19410050,
  abstract  = {
               Artificial intelligence (AI) is a driving force behind Industry
               4.0\&\#x00A0;in manufacturing. Specifically, machine learning has been
               applied to all parts of the manufacturing process: from product design
               optimization to anomaly detection for quality control. Explainable AI (XAI)
               and interpretable AI (IAI) methods have been developed to provide
               transparency into how models make decisions. This survey presents a thorough
               review of who, what, when, where, why, and how both IAI and XAI methods have
               been used in manufacturing. Due to the multidisciplinary nature of
               manufacturing, this work provides the results from a systematic literature
               review that surveyed papers from highly rated venues in multiple
               manufacturing and AI-related fields to give the reader a holistic view of the
               space. This survey is intended to help both individuals from academia and
               industry quickly understand the applications, areas of research, and future
               work involved with creating explainable industrial solutions.
               },
  keywords  = {
               Artificial intelligence,Artificial intelligence (AI),Biological system
               modeling,Data models,Industries,Manufacturing,Predictive models,Surveys,deep
               learning (DL),explainable artificial intelligence (XAI),human–computer
               interaction (HCI),industry 4.0,interpretable artificial intelligence
               (IAI),machine learning (ML),manufacturing
               }
}



@article{armstrongbonferroni,
  author   = {Armstrong, Richard A.},
  title    = {When to use the Bonferroni correction},
  journal  = {Ophthalmic and Physiological Optics},
  volume   = {34},
  number   = {5},
  pages    = {502-508},
  keywords = {Bonferroni correction, Clinical & Experimental Optometry, Ophthalmic & Physiological Optics, Optometry & Vision Science, statistical guidelines},
  doi      = {https://doi.org/10.1111/opo.12131},
  url      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/opo.12131},
  eprint   = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/opo.12131},
  abstract = {Abstract Purpose The Bonferroni correction adjusts probability (p) values because of the increased risk of a type I error when making multiple statistical tests. The routine use of this test has been criticised as deleterious to sound statistical judgment, testing the wrong hypothesis, and reducing the chance of a type I error but at the expense of a type II error; yet it remains popular in ophthalmic research. The purpose of this article was to survey the use of the Bonferroni correction in research articles published in three optometric journals, viz. Ophthalmic \& Physiological Optics, Optometry \& Vision Science, and Clinical \& Experimental Optometry, and to provide advice to authors contemplating multiple testing. Recent findings Some authors ignored the problem of multiple testing while others used the method uncritically with no rationale or discussion. A variety of methods of correcting p values were employed, the Bonferroni method being the single most popular. Bonferroni was used in a variety of circumstances, most commonly to correct the experiment-wise error rate when using multiple ‘t’ tests or as a post-hoc procedure to correct the family-wise error rate following analysis of variance (anova). Some studies quoted adjusted p values incorrectly or gave an erroneous rationale. Summary Whether or not to use the Bonferroni correction depends on the circumstances of the study. It should not be used routinely and should be considered if: (1) a single test of the ‘universal null hypothesis’ (Ho) that all tests are not significant is required, (2) it is imperative to avoid a type I error, and (3) a large number of tests are carried out without preplanned hypotheses.},
  year     = {2014}
}
@misc{arras2016explainingpredictionsnonlinearclassifiers,
  title         = {Explaining Predictions of Non-Linear Classifiers in NLP},
  author        = {Leila Arras and Franziska Horn and Grégoire Montavon and Klaus-Robert Müller and Wojciech Samek},
  year          = {2016},
  eprint        = {1606.07298},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  url           = {https://arxiv.org/abs/1606.07298}
}
@article{arrieta2020explainable,
  title     = {
               Explainable Artificial Intelligence (XAI): Concepts, taxonomies,
               opportunities and challenges toward responsible AI
               },
  author    = {
               Alejandro Barredo Arrieta and Natalia D\'{\i}az-Rodr\'{\i}guez and Javier Del
               Ser and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador
               Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja
               Chatila and Francisco Herrera
               },
  year      = 2020,
  month     = 6,
  journal   = {Information Fusion},
  publisher = {Elsevier},
  volume    = 58,
  pages     = {82--115},
  doi       = {10.1016/j.inffus.2019.12.012},
  issn      = {1566-2535},
  abstract  = {
               In the last few years, Artificial Intelligence (AI) has achieved a notable
               momentum that, if harnessed appropriately, may deliver the best of
               expectations over many application sectors across the field. For this to
               occur shortly in Machine Learning, the entire community stands in front of
               the barrier of explainability, an inherent problem of the latest techniques
               brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were
               not present in the last hype of AI (namely, expert systems and rule based
               models). Paradigms underlying this problem fall within the so-called
               eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature
               for the practical deployment of AI models. The overview presented in this
               article examines the existing literature and contributions already done in
               the field of XAI, including a prospect toward what is yet to be reached. For
               this purpose we summarize previous eff\shortcite{muller2024reinforcement,müller2024smaller}orts made to define explainability in
               Machine Learning, establishing a novel definition of explainable Machine
               Learning that covers such prior conceptual propositions with a major focus on
               the audience for which the explainability is sought. Departing from this
               definition, we propose and discuss about a taxonomy of recent contributions
               related to the explainability of different Machine Learning models, including
               those aimed at explaining Deep Learning methods for which a second dedicated
               taxonomy is built and examined in detail. This critical literature analysis
               serves as the motivating background for a series of challenges faced by XAI,
               such as the interesting crossroads of data fusion and explainability. Our
               prospects lead toward the concept of Responsible Artificial Intelligence,
               namely, a methodology for the large-scale implementation of AI methods in
               real organizations with fairness, model explainability and accountability at
               its core. Our ultimate goal is to provide newcomers to the field of XAI with
               a thorough taxonomy that can serve as reference material in order to
               stimulate future research advances, but also to encourage experts and
               professionals from other disciplines to embrace the benefits of AI in their
               activity sectors, without any prior bias for its lack of interpretability.
               },
  keywords  = {
               Accountability,Comprehensibility,Data Fusion,Deep Learning,Explainable
               Artificial Intelligence,Fairness,Interpretability,Machine
               Learning,Privacy,Responsible Artificial Intelligence,Transparency
               }
}


@article{aslam2023conceptual,
  title     = {
               A Conceptual Model Framework for XAI Requirement Elicitation of Application
               Domain System
               },
  author    = {Maria Aslam and Diana Segura-Velandia and Yee Mey Goh},
  year      = 2023,
  journal   = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  volume    = 11,
  pages     = {108080--108091},
  doi       = {10.1109/access.2023.3315605},
  issn      = 21693536,
  abstract  = {
               The use of data analytics and Machine Learning (ML) branches of AI for
               predictive and analytic knowledge retrieval has surged significantly in
               various industries (e.g., health, finance, business, and manufacturing).
               However, the acceptance of AI has been hindered by opaque models that lack
               transparency. Explainability in AI (XAI) has gained significant prominence
               owing to its focus on introducing avenues of accountability in AI. XAI
               acknowledges the importance of human factors and strives to incorporate them
               into the design process, recognising that the cognitive effort involved in
               understanding explanations is a key aspect. Mental Models play a crucial role
               in the XAI evaluative premise, but their current utility is limited. By
               intentionally designing explanations that align with users' mental models,
               their experiences can be significantly enhanced, leading to improved
               understanding, satisfaction, trust, and performance. This study proposes
               using Mental Models to elicit explainability requirements and to develop an
               Ontology-Driven Conceptual Model to facilitate the learning process for a
               better understanding of explanations.
               },
  keywords  = {
               Conceptual model,explainability in AI,mental models,requirements elicitation
               }
}

@article{bach2015pixel,
  title     = {On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
  author    = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal   = {PloS one},
  volume    = {10},
  number    = {7},
  pages     = {e0130140},
  year      = {2015},
  publisher = {Public Library of Science San Francisco, CA USA}
}

@article{bao2022whose,
  title     = {Whose AI? How different publics think about AI and its social impacts},
  author    = {Bao, Luye and Krause, Nicole M and Calice, Mikhaila N and Scheufele, Dietram A and Wirz, Christopher D and Brossard, Dominique and Newman, Todd P and Xenos, Michael A},
  journal   = {Computers in Human Behavior},
  volume    = {130},
  pages     = {107182},
  year      = {2022},
  publisher = {Elsevier}
}
@article{bekkemoen2023explainable,
  title     = {
               Explainable reinforcement learning (XRL): a systematic literature review and
               taxonomy
               },
  author    = {Yanzhe Bekkemoen},
  year      = 2023,
  month     = 11,
  journal   = {Machine Learning 2023 113:1},
  publisher = {Springer},
  volume    = 113,
  pages     = {355--441},
  doi       = {10.1007/s10994-023-06479-7},
  isbn      = {0123456789},
  issn      = {1573-0565},
  url       = {https://link.springer.com/article/10.1007/s10994-023-06479-7},
  abstract  = {
               In recent years, reinforcement learning (RL) systems have shown impressive
               performance and remarkable achievements. Many achievements can be attributed
               to combining RL with deep learning. However, those systems lack
               explainability, which refers to our understanding of the system's
               decision-making process. In response to this challenge, the new explainable
               RL (XRL) field has emerged and grown rapidly to help us understand RL
               systems. This systematic literature review aims to give a unified view of the
               field by reviewing ten existing XRL literature reviews and 189 XRL studies
               from the past five years. Furthermore, we seek to organize these studies into
               a new taxonomy, discuss each area in detail, and draw connections between
               methods and stakeholder questions (e.g., ``how can I get the agent to do
               \_?''). Finally, we look at the research trends in XRL, recommend XRL
               methods, and present some exciting research directions for future research.
               We hope stakeholders, such as RL researchers and practitioners, will utilize
               this literature review as a comprehensive resource to overview existing
               state-of-the-art XRL methods. Additionally, we strive to help find research
               gaps and quickly identify methods that answer stakeholder questions.
               },
  issue     = 1,
  keywords  = {
               Artificial Intelligence,Control,Machine Learning,Mechatronics,Natural
               Language Processing (NLP),Robotics,Simulation and Modeling
               }
}
@article{bengiononlinear,
  author   = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title    = {Representation Learning: A Review and New Perspectives},
  year     = {2013},
  volume   = {35},
  number   = {8},
  pages    = {1798-1828},
  keywords = {Learning systems;Machine learning;Abstracts;Feature extraction;Manifolds;Neural networks;Speech recognition;Deep learning;representation learning;feature learning;unsupervised learning;Boltzmann machine;autoencoder;neural nets},
  doi      = {10.1109/TPAMI.2013.50}
}

@article{bernardo2023affective,
  title     = {
               Affective Design Analysis of Explainable Artificial Intelligence (XAI): A
               User-Centric Perspective
               },
  author    = {Ezekiel Bernardo and Rosemary Seva},
  year      = 2023,
  month     = 3,
  journal   = {Informatics 2023, Vol. 10, Page 32},
  publisher = {Multidisciplinary Digital Publishing Institute},
  volume    = 10,
  pages     = 32,
  doi       = {10.3390/informatics10010032},
  issn      = {2227-9709},
  url       = {
               https://www.mdpi.com/2227-9709/10/1/32/htm
               https://www.mdpi.com/2227-9709/10/1/32
               },
  abstract  = {
               Explainable Artificial Intelligence (XAI) has successfully solved the black
               box paradox of Artificial Intelligence (AI). By providing human-level
               insights on AI, it allowed users to understand its inner workings even with
               limited knowledge of the machine learning algorithms it uses. As a result,
               the field grew, and development flourished. However, concerns have been
               expressed that the techniques are limited in terms of to whom they are
               applicable and how their effect can be leveraged. Currently, most XAI
               techniques have been designed by developers. Though needed and valuable, XAI
               is more critical for an end-user, considering transparency cleaves on trust
               and adoption. This study aims to understand and conceptualize an
               end-user-centric XAI to fill in the lack of end-user understanding.
               Considering recent findings of related studies, this study focuses on design
               conceptualization and affective analysis. Data from 202 participants were
               collected from an online survey to identify the vital XAI design components
               and testbed experimentation to explore the affective and trust change per
               design configuration. The results show that affective is a viable trust
               calibration route for XAI. In terms of design, explanation form,
               communication style, and presence of supplementary information are the
               components users look for in an effective XAI. Lastly, anxiety about AI,
               incidental emotion, perceived AI reliability, and experience using the system
               are significant moderators of the trust calibration process for an end-user.
               },
  issue     = 1,
  keywords  = {
               AI,XAI,affective design,artificial intelligence,computer
               vision,emotions,end,explainable AI,interpretable deep learning,machine
               learning,user design
               }
}

@inproceedings{binder2016layer,
  title        = {Layer-wise relevance propagation for deep neural network architectures},
  author       = {Binder, Alexander and Bach, Sebastian and Montavon, Gregoire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  booktitle    = {Information science and applications (ICISA) 2016},
  pages        = {913--922},
  year         = {2016},
  organization = {Springer}
}

@article{bohm2015people,
  title     = {How people explain their own and others’ behavior: a theory of lay causal explanations},
  author    = {B{\"o}hm, Gisela and Pfister, Hans-R{\"u}diger},
  journal   = {Frontiers in psychology},
  volume    = {6},
  pages     = {109763},
  year      = {2015},
  publisher = {Frontiers}
}

@article{breiman1984cart,
  title     = {Cart},
  author    = {Breiman, Leo and Friedman, Jerome and Olshen, Richard and Stone, Charles},
  journal   = {Classification and regression trees},
  year      = {1984},
  publisher = {Wadsworth and Brooks/Cole Monterey, CA, USA}
}


@article{brennen2020what,
  title     = {
               What do people really want when they say they want "explainable AI?" we asked
               60 stakeholders
               },
  author    = {Andrea Brennen},
  year      = 2020,
  month     = 4,
  journal   = {Conference on Human Factors in Computing Systems - Proceedings},
  publisher = {Association for Computing Machinery},
  doi       = {10.1145/3334480.3383047},
  isbn      = 9781450368193,
  url       = {https://dl.acm.org/doi/10.1145/3334480.3383047},
  abstract  = {
               This paper summarizes findings from a qualitative research effort aimed at
               understanding how various stakeholders characterize the problem of
               Explainable Artificial Intelligence (Explainable AI or XAI). During a
               nine-month period, the author conducted 40 interviews and 2 focus groups. An
               analysis of data gathered led to two significant initial findings: (1)
               current discourse on Explainable AI is hindered by a lack of consistent
               terminology; and (2) there are multiple distinct use cases for Explainable
               AI, including: debugging models, understanding bias, and building trust.
               These uses cases assume different user personas, will likely require
               different explanation strategies, and are not evenly addressed by current XAI
               tools. This stakeholder research supports a broad characterization of the
               problem of Explainable AI and can provide important context to inform the
               design of future capabilities.
               },
  keywords  = {
               Data science,Explainable AI,Interface design,Machine learning,UI/UX
               design,User research
               }
}


@article{brownecrossval,
  title    = {Cross-Validation Methods},
  journal  = {Journal of Mathematical Psychology},
  volume   = {44},
  number   = {1},
  pages    = {108-132},
  year     = {2000},
  issn     = {0022-2496},
  doi      = {https://doi.org/10.1006/jmps.1999.1279},
  url      = {https://www.sciencedirect.com/science/article/pii/S0022249699912798},
  author   = {Michael W Browne},
  abstract = {This paper gives a review of cross-validation methods. The original applications in multiple linear regression are considered first. It is shown how predictive accuracy depends on sample size and the number of predictor variables. Both two-sample and single-sample cross-validation indices are investigated. The application of cross-validation methods to the analysis of moment structures is then justified. An equivalence of a single-sample cross-validation index and the Akaike information criterion is pointed out. It is seen that the optimal number of parameters suggested by both single-sample and two-sample cross-validation indices will depend on sample size.}
}

@article{cabour2021towards,
  title    = {Towards an Explanation Space to Align Humans and Explainable-AI Teamwork},
  author   = {
              Garrick Cabour and Andr\'{e}s Morales-Forero and Elise Ledoux and Samuel
              Bassetto
              },
  year     = 2021,
  month    = 6,
  url      = {https://arxiv.org/abs/2106.01503v1},
  abstract = {
              Providing meaningful and actionable explanations to end-users is a
              fundamental prerequisite for implementing explainable intelligent systems in
              the real world. Explainability is a situated interaction between a user and
              the AI system rather than being static design principles. The content of
              explanations is context-dependent and must be defined by evidence about the
              user and its context. This paper seeks to operationalize this concept by
              proposing a formative architecture that defines the explanation space from a
              user-inspired perspective. The architecture comprises five intertwined
              components to outline explanation requirements for a task: (1) the end-users
              mental models, (2) the end-users cognitive process, (3) the user interface,
              (4) the human-explainer agent, and the (5) agent process. We first define
              each component of the architecture. Then we present the Abstracted
              Explanation Space, a modeling tool that aggregates the architecture's
              components to support designers in systematically aligning explanations with
              the end-users work practices, needs, and goals. It guides the specifications
              of what needs to be explained (content - end-users mental model), why this
              explanation is necessary (context - end-users cognitive process), to delimit
              how to explain it (format - human-explainer agent and user interface), and
              when should the explanations be given. We then exemplify the tool's use in an
              ongoing case study in the aircraft maintenance domain. Finally, we discuss
              possible contributions of the tool, known limitations/areas for improvement,
              and future work to be done.
              },
  keywords = {
              Explainable AI,Human-AI Teaming,Interdisciplinary study,User-Centered Design
              }
}

@inproceedings{chakraborty2017interpretability,
  title     = {Interpretability of deep learning models: A survey of results},
  author    = {
               Chakraborty, Supriyo and Tomsett, Richard and Raghavendra, Ramya and
               Harborne, Daniel and Alzantot, Moustafa and Cerutti, Federico and Srivastava,
               Mani and Preece, Alun and Julier, Simon and Rao, Raghuveer M. and Kelley,
               Troy D. and Braines, Dave and Sensoy, Murat and Willis, Christopher J. and
               Gurram, Prudhvi
               },
  year      = 2017,
  booktitle = {
               2017 IEEE SmartWorld, Ubiquitous Intelligence \& Computing, Advanced \&
               Trusted Computed, Scalable Computing \& Communications, Cloud \& Big Data
               Computing, Internet of People and Smart City Innovation
               (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)
               },
  volume    = {},
  number    = {},
  pages     = {1--6},
  doi       = {10.1109/uic-atc.2017.8397411},
  keywords  = {
               Machine learning;Training;Training data;Data models;Brain modeling;Predictive
               models;Neurons
               }
}

@article{chatterjee2024exploration,
  title     = {Exploration of interpretability techniques for deep covid-19 classification using chest x-ray images},
  author    = {Chatterjee, Soumick and Saad, Fatima and Sarasaen, Chompunuch and Ghosh, Suhita and Krug, Valerie and Khatun, Rupali and Mishra, Rahul and Desai, Nirja and Radeva, Petia and Rose, Georg et al.},
  journal   = {Journal of imaging},
  volume    = {10},
  number    = {2},
  year      = {2024},
  publisher = {Multidisciplinary Digital Publishing Institute (MDPI)}
}

@incollection{chen2023applications,
  title     = {Applications of XAI to job sequencing and scheduling in manufacturing},
  author    = {Chen, Tin-Chih Toly},
  booktitle = {Explainable Artificial Intelligence (XAI) in Manufacturing: Methodology, Tools, and Applications},
  pages     = {83--105},
  year      = {2023},
  publisher = {Springer}
}

@incollection{chen2023explainable,
  title     = {Explainable artificial intelligence (XAI) in manufacturing},
  author    = {Chen, Tin-Chih Toly},
  booktitle = {Explainable Artificial Intelligence (XAI) in Manufacturing: Methodology, Tools, and Applications},
  pages     = {1--11},
  year      = {2023},
  publisher = {Springer}
}


@article{chi2022quantitative,
  title     = {
               A quantitative argumentation-based Automated eXplainable Decision System for
               fake news detection on social media
               },
  author    = {Haixiao Chi and Beishui Liao},
  year      = 2022,
  month     = 4,
  journal   = {Knowledge-Based Systems},
  publisher = {Elsevier},
  volume    = 242,
  pages     = 108378,
  doi       = {10.1016/j.knosys.2022.108378},
  issn      = {0950-7051},
  abstract  = {
               Social media is flooded with rumors, which make fake news detection a
               pressing problem. Many black-box approaches have been proposed to
               automatically predict the veracity of claims. These methods are lack of
               interpretability. Thus, we propose a Quantitative Argumentation-based
               Automated eXplainable Decision-making System (QA-AXDS) to tackle this problem
               and provide users with explanations about the results. The system is fully
               data-driven in its processes, which allows our models to make greater use of
               data and be more automatic and scalable than other quantitative framework
               models. In terms of interpretability, the system can automatically acquire
               human-level knowledge, and interact with users in the form of dialog trees
               through explanatory models, thus helping them understand the internal
               reasoning process of the system. The experimental results show that our
               system has better transparency and interpretability than other approaches
               based on the pure machine learning methods, and performs competitively in
               accuracy among others. In addition, the explanation model provides a way to
               improve the algorithms when some problems are identified by checking the
               explanations.
               },
  keywords  = {
               Fake news detection,Quantitative argumentation,eXplainable artificial
               intelligence
               }
}

@article{chromik2021human,
  title     = {
               Human-XAI Interaction: A Review and Design Principles for Explanation User
               Interfaces
               },
  author    = {Michael Chromik and Andreas Butz},
  year      = 2021,
  journal   = {
               Lecture Notes in Computer Science (including subseries Lecture Notes in
               Artificial Intelligence and Lecture Notes in Bioinformatics)
               },
  publisher = {Springer Science and Business Media Deutschland GmbH},
  volume    = {12933 Lncs},
  pages     = {619--640},
  doi       = {10.1007/978-3-030-85616-8\_36/tables/1},
  isbn      = 9783030856151,
  issn      = 16113349,
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-85616-8\_36},
  abstract  = {
               The interdisciplinary field of explainable artificial intelligence (XAI) aims
               to foster human understanding of black-box machine learning models through
               explanation-generating methods. Although the social sciences suggest that
               explanation is a social and iterative process between an explainer and an
               explainee, explanation user interfaces and their user interactions have not
               been systematically explored in XAI research yet. Therefore, we review prior
               XAI research containing explanation user interfaces for ML-based intelligent
               systems and describe different concepts of interaction. Further, we present
               observed design principles for interactive explanation user interfaces. With
               our work, we inform designers of XAI systems about human-centric ways to
               tailor their explanation user interfaces to different target audiences and
               use cases.
               },
  keywords  = {
               Explainable AI,Explanation user interfaces,Interaction design,Literature
               review
               }
}

@article{cialdini2001science,
  title     = {The science of persuasion},
  author    = {Cialdini, Robert B},
  journal   = {Scientific American},
  volume    = {284},
  number    = {2},
  pages     = {76--81},
  year      = {2001},
  publisher = {JSTOR}
}



@article{Clement2023,
  abstract  = {Currently, explainability represents a major barrier that Artificial Intelligence (AI) is facing in regard to its practical implementation in various application domains. To combat the lack of understanding of AI-based systems, Explainable AI (XAI) aims to make black-box AI models more transparent and comprehensible for humans. Fortunately, plenty of XAI methods have been introduced to tackle the explainability problem from different perspectives. However, due to the vast search space, it is challenging for ML practitioners and data scientists to start with the development of XAI software and to optimally select the most suitable XAI methods. To tackle this challenge, we introduce XAIR, a novel systematic metareview of the most promising XAI methods and tools. XAIR differentiates itself from existing reviews by aligning its results to the five steps of the software development process, including requirement analysis, design, implementation, evaluation, and deployment. Through this mapping, we aim to create a better understanding of the individual steps of developing XAI software and to foster the creation of real-world AI applications that incorporate explainability. Finally, we conclude with highlighting new directions for future research.},
  author    = {Tobias Clement and Nils Kemmerzell and Mohamed Abdelaal and Michael Amberg},
  doi       = {10.3390/MAKE5010006},
  issn      = {2504-4990},
  issue     = {1},
  journal   = {Machine Learning and Knowledge Extraction 2023, Vol. 5, Pages 78-108},
  keywords  = {explainable AI,feature importance,software development process,systematic review},
  month     = {1},
  pages     = {78-108},
  publisher = {Multidisciplinary Digital Publishing Institute},
  title     = {XAIR: A Systematic Metareview of Explainable AI (XAI) Aligned to the Software Development Process},
  volume    = {5},
  url       = {https://www.mdpi.com/2504-4990/5/1/6/htm https://www.mdpi.com/2504-4990/5/1/6},
  year      = {2023}
}

@article{clement2023xair,
  title     = {
               XAIR: A Systematic Metareview of Explainable AI (XAI) Aligned to the Software
               Development Process
               },
  author    = {Tobias Clement and Nils Kemmerzell and Mohamed Abdelaal and Michael Amberg},
  year      = 2023,
  month     = 1,
  journal   = {Machine Learning and Knowledge Extraction 2023, Vol. 5, Pages 78-108},
  publisher = {Multidisciplinary Digital Publishing Institute},
  volume    = 5,
  pages     = {78--108},
  doi       = {10.3390/make5010006},
  issn      = {2504-4990},
  url       = {
               https://www.mdpi.com/2504-4990/5/1/6/htm https://www.mdpi.com/2504-4990/5/1/6
               },
  abstract  = {
               Currently, explainability represents a major barrier that Artificial
               Intelligence (AI) is facing in regard to its practical implementation in
               various application domains. To combat the lack of understanding of AI-based
               systems, Explainable AI (XAI) aims to make black-box AI models more
               transparent and comprehensible for humans. Fortunately, plenty of XAI methods
               have been introduced to tackle the explainability problem from different
               perspectives. However, due to the vast search space, it is challenging for ML
               practitioners and data scientists to start with the development of XAI
               software and to optimally select the most suitable XAI methods. To tackle
               this challenge, we introduce XAIR, a novel systematic metareview of the most
               promising XAI methods and tools. XAIR differentiates itself from existing
               reviews by aligning its results to the five steps of the software development
               process, including requirement analysis, design, implementation, evaluation,
               and deployment. Through this mapping, we aim to create a better understanding
               of the individual steps of developing XAI software and to foster the creation
               of real-world AI applications that incorporate explainability. Finally, we
               conclude with highlighting new directions for future research.
               },
  issue     = 1,
  keywords  = {
               explainable AI,feature importance,software development process,systematic
               review
               }
}

@misc{das2020opportunities,
  title         = {Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey},
  author        = {Arun Das and Paul Rad},
  year          = {2020},
  eprint        = {2006.11371},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CV},
  url           = {https://arxiv.org/abs/2006.11371}
}

@article{dazeley2021levels,
  title     = {
               Levels of explainable artificial intelligence for human-aligned
               conversational explanations
               },
  author    = {
               Richard Dazeley and Peter Vamplew and Cameron Foale and Charlotte Young and
               Sunil Aryal and Francisco Cruz
               },
  year      = 2021,
  month     = 10,
  journal   = {Artificial Intelligence},
  publisher = {Elsevier},
  volume    = 299,
  pages     = 103525,
  doi       = {10.1016/j.artint.2021.103525},
  issn      = {0004-3702},
  note      = {MEta review for XAI},
  abstract  = {
               Over the last few years there has been rapid research growth into eXplainable
               Artificial Intelligence (XAI) and the closely aligned Interpretable Machine
               Learning (IML). Drivers for this growth include recent legislative changes
               and increased investments by industry and governments, along with increased
               concern from the general public. People are affected by autonomous decisions
               every day and the public need to understand the decision-making process to
               accept the outcomes. However, the vast majority of the applications of
               XAI/IML are focused on providing low-level `narrow' explanations of how an
               individual decision was reached based on a particular datum. While important,
               these explanations rarely provide insights into an agent's: beliefs and
               motivations; hypotheses of other (human, animal or AI) agents' intentions;
               interpretation of external cultural expectations; or, processes used to
               generate its own explanation. Yet all of these factors, we propose, are
               essential to providing the explanatory depth that people require to accept
               and trust the AI's decision-making. This paper aims to define levels of
               explanation and describe how they can be integrated to create a human-aligned
               conversational explanation system. In so doing, this paper will survey
               current approaches and discuss the integration of different technologies to
               achieve these levels with Broad eXplainable Artificial Intelligence
               (Broad-XAI), and thereby move towards high-level `strong' explanations.
               },
  keywords  = {
               Artificial General Intelligence (AGI),Broad-XAI,Explainable Artificial
               Intelligence (XAI),Human-Computer Interaction (HCI),Interpretable Machine
               Learning (IML)
               }
}

@article{dazeley2023explainable,
  title     = {
               Explainable reinforcement learning for broad-XAI: a conceptual framework and
               survey
               },
  author    = {Richard Dazeley and Peter Vamplew and Francisco Cruz},
  year      = 2023,
  month     = 8,
  journal   = {Neural Computing and Applications},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  volume    = 35,
  pages     = {16893--16916},
  doi       = {10.1007/s00521-023-08423-1},
  issn      = 14333058,
  note      = {Zeigt nur auf die XAI f\"{u}r RL aussehen soll},
  abstract  = {
               Broad-XAI moves away from interpreting individual decisions based on a single
               datum and aims to provide integrated explanations from multiple machine
               learning algorithms into a coherent explanation of an agent's behaviour that
               is aligned to the communication needs of the explainee. Reinforcement
               Learning (RL) methods, we propose, provide a potential backbone for the
               cognitive model required for the development of Broad-XAI. RL represents a
               suite of approaches that have had increasing success in solving a range of
               sequential decision-making problems. However, these algorithms operate as
               black-box problem solvers, where they obfuscate their decision-making policy
               through a complex array of values and functions. EXplainable RL (XRL) aims to
               develop techniques to extract concepts from the agent's: perception of the
               environment; intrinsic/extrinsic motivations/beliefs; Q-values, goals and
               objectives. This paper aims to introduce the Causal XRL Framework (CXF), that
               unifies the current XRL research and uses RL as a backbone to the development
               of Broad-XAI. CXF is designed to incorporate many standard RL extensions and
               integrated with external ontologies and communication facilities so that the
               agent can answer questions that explain outcomes its decisions. This paper
               aims to: establish XRL as a distinct branch of XAI; introduce a conceptual
               framework for XRL; review existing approaches explaining agent behaviour; and
               identify opportunities for future research. Finally, this paper discusses how
               additional information can be extracted and ultimately integrated into models
               of communication, facilitating the development of Broad-XAI.
               },
  issue     = 23,
  keywords  = {
               Broad XAI,Explainable Reinforcement learning (XRL),Explainable artificial
               intelligence (XAI),Reinforcement learning (RL)
               }
}

@article{DBLP:journals/corr/abs-2104-10505,
  author     = {Shikun Chen},
  title      = {Interpretation of multi-label classification models using shapley
                values},
  journal    = {CoRR},
  volume     = {abs/2104.10505},
  year       = {2021},
  url        = {https://arxiv.org/abs/2104.10505},
  eprinttype = {arXiv},
  eprint     = {2104.10505},
  timestamp  = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2104-10505.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{degenhardt2003exploring,
  title     = {Exploring the association},
  author    = {Degenhardt, Louisa and Hall, Wayne and Lynskey, Michael},
  year      = 2003,
  journal   = {Addiction},
  publisher = {Wiley Online Library},
  volume    = 98,
  number    = 11,
  pages     = {1493--1504}
}

@article{dhanorkar2021who,
  title     = {
               Who needs to know what, when?: Broadening the Explainable AI (XAI) Design
               Space by Looking at Explanations across the AI Lifecycle
               },
  author    = {
               Shipi Dhanorkar and Christine T. Wolf and Kun Qian and Anbang Xu and Lucian
               Popa and Yunyao Li
               },
  year      = 2021,
  month     = 6,
  journal   = {
               DIS 2021 - Proceedings of the 2021 ACM Designing Interactive Systems
               Conference: Nowhere and Everywhere
               },
  publisher = {Association for Computing Machinery, Inc},
  volume    = 12,
  pages     = {1591--1602},
  doi       = {10.1145/3461778.3462131},
  isbn      = 9781450384766,
  url       = {https://dl.acm.org/doi/10.1145/3461778.3462131},
  abstract  = {
               The interpretability or explainability of AI systems (XAI) has been a topic
               gaining renewed attention in recent years across AI and HCI communities.
               Recent work has drawn attention to the emergent explainability requirements
               of in situ, applied projects, yet further exploratory work is needed to more
               fully understand this space. This paper investigates applied AI projects and
               reports on a qualitative interview study of individuals working on AI
               projects at a large technology and consulting company. Presenting an
               empirical understanding of the range of stakeholders in industrial AI
               projects, this paper also draws out the emergent explainability practices
               that arise as these projects unfold, highlighting the range of explanation
               audiences (who), as well as how their explainability needs evolve across the
               AI project lifecycle (when). We discuss the importance of adopting a
               sociotechnical lens in designing AI systems, noting how the "AI lifecycle"can
               serve as a design metaphor to further the XAI design field.
               },
  keywords  = {Explainable AI,Interviews,Work Practices}
}

@misc{doshi_velez2017towards,
  title    = {Towards A Rigorous Science of Interpretable Machine Learning},
  author   = {Finale Doshi-Velez and Been Kim},
  year     = 2017,
  month    = 2,
  url      = {https://arxiv.org/abs/1702.08608v2},
  abstract = {
              As machine learning systems become ubiquitous, there has been a surge of
              interest in interpretable machine learning: systems that provide explanation
              for their outputs. These explanations are often used to qualitatively assess
              other criteria such as safety or non-discrimination. However, despite the
              interest in interpretability, there is very little consensus on what
              interpretable machine learning is and how it should be measured. In this
              position paper, we first define interpretability and describe when
              interpretability is needed (and when it is not). Next, we suggest a taxonomy
              for rigorous evaluation and expose open questions towards a more rigorous
              science of interpretable machine learning.
              }
}
@article{dwivedi2021artificial,
  title     = {Artificial Intelligence (AI): Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy},
  author    = {Dwivedi, Yogesh K and Hughes, Laurie and Ismagilova, Elvira and Aarts, Gert and Coombs, Crispin and Crick, Tom and Duan, Yanqing and Dwivedi, Rohita and Edwards, John and Eirug, Aled et al.},
  journal   = {International Journal of Information Management},
  volume    = {57},
  pages     = {101994},
  year      = {2021},
  publisher = {Elsevier}
}

@article{dwivedi2023explainable,
  title     = {Explainable AI (XAI): Core ideas, techniques, and solutions},
  author    = {Dwivedi, Rudresh and Dave, Devam and Naik, Het and Singhal, Smiti and Omer, Rana and Patel, Pankesh and Qian, Bin and Wen, Zhenyu and Shah, Tejal and Morgan, Graham et al.},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {9},
  pages     = {1--33},
  year      = {2023},
  publisher = {ACM New York, NY}
}

@article{ehsan2023charting,
  title     = {
               Charting the Sociotechnical Gap in Explainable AI: A Framework to Address the
               Gap in XAI
               },
  author    = {Upol Ehsan and Koustuv Saha and Munmun De Choudhury and Mark O. Riedl},
  year      = 2023,
  month     = 4,
  journal   = {Proceedings of the ACM on Human-Computer Interaction},
  publisher = {ACMPUB27New York, NY, USA},
  volume    = 7,
  doi       = {10.1145/3579467 },
  issn      = 25730142,
  url       = {https://dl.acm.org/doi/10.1145/3579467},
  abstract  = {
               Explainable AI (XAI) systems are sociotechnical in nature; thus, they are
               subject to the sociotechnical gap-divide between the technical affordances
               and the social needs. However, charting this gap...
               },
  issue     = {1 Cscw},
  keywords  = {
               AI ethics,AI governance,explainable ai,fate,framework,human-AI
               interaction,human-centered explainable ai,organizational
               dynamics,participatory design,responsible ai,sociotechnical gap,user study
               }
}

@misc{EUaiact,
  title        = {AI for Public Good: EU-U.S. Research Alliance in AI for the Public Good},
  author       = {European Commission},
  year         = {2024},
  url          = {https://digital-strategy.ec.europa.eu/en/library/ai-public-good-eu-us-research-alliance-ai-public-good},
  howpublished = {Directorate-General for Communications Networks, Content and Technology}
}

@inproceedings{fast2017long,
  title     = {Long-term trends in the public perception of artificial intelligence},
  author    = {Fast, Ethan and Horvitz, Eric},
  booktitle = {Proceedings of the AAAI conference on artificial intelligence},
  volume    = {31},
  number    = {1},
  year      = {2017}
}

@inproceedings{fernando2019study,
  title     = {A study on the Interpretability of Neural Retrieval Models using DeepSHAP},
  author    = {Fernando, Zeon Trevor and Singh, Jaspreet and Anand, Avishek},
  booktitle = {Proceedings of the 42nd international ACM SIGIR conference on research and development in information retrieval},
  pages     = {1005--1008},
  year      = {2019}
}

@article{franke2022ai,
  title     = {
               AI-based Improvement of Decision-makers' Knowledge in Production Planning and
               Control
               },
  author    = {Felix Franke and Susanne Franke and Ralph Riedel},
  year      = 2022,
  month     = 1,
  journal   = {IFAC-PapersOnLine},
  publisher = {Elsevier},
  volume    = 55,
  pages     = {2240--2245},
  doi       = {10.1016/j.ifacol.2022.10.041},
  issn      = {2405-8963},
  abstract  = {
               Production companies are required to implement efficient, flexible
               manufacturing processes to face the competitive global market. The ongoing
               digital transformation leads many companies to store production process data,
               which offers optimization potential using Artificial Intelligence (AI).
               However, production planning and control is often still performed manually
               with a significant amount of human expert knowledge. To enhance the
               decision-makers' knowledge and hereby the decisions' quality, the available
               database can be analyzed with AI methods. This paper aims at improving the
               human decision-making process by providing a methodology to evaluate and
               apply AI-generated knowledge w.r.t. its quality and long-term usefulness for
               decision-makers.
               },
  issue     = 10,
  keywords  = {
               Artificial Intelligence,Production planning and control,human
               decision-making,knowledge gain,optimization
               }
}


@article{fussenegger2022explainable,
  title    = {
              Explainable Machine Learning for Lead Time Prediction : A Case Study on
              Explainability Methods and Benefits in the Pharmaceutical Industry
              },
  author   = {Paul Fussenegger and Niklas Lange},
  year     = 2022,
  url      = {https://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-314396},
  abstract = {
              Artificial Intelligence (AI) has proven to be highly suitable for a wide
              range of problems in manufacturing environments, including the prediction of
              lead times. Most of these solutions are based o ...
              },
  keywords = {
              Engineering and Technology,Ledtid,Teknik och
              teknologier,explainability,f\"{o}rklarbarhet,maskininl\"{a}rning,production
              planning and control,produktionsplanering och produktionsstyrning,regression
              analysis,regressionsanalys
              }
}


@book{gentner2014mental,
  title     = {Mental models},
  author    = {Gentner, Dedre and Stevens, Albert L},
  year      = {2014},
  publisher = {Psychology Press}
}
@techreport{gillespie2021trust,
  title       = {Trust in Artificial Intelligence: A Five Country Study},
  author      = {Gillespie, Nicole and Lockey, Steve and Curtis, Caitlin},
  year        = {2021},
  institution = {The University of Queensland and KPMG Australia},
  url         = {https://doi.org/10.14264/e34bfa3},
  doi         = {10.14264/e34bfa3}
}
@article{Grumbach2022Robust,
  title     = {Robust-stable scheduling in dynamic flow shops based on deep reinforcement learning},
  volume    = {35},
  issn      = {1572-8145},
  url       = {http://dx.doi.org/10.1007/s10845-022-02069-x},
  doi       = {10.1007/s10845-022-02069-x},
  number    = {2},
  journal   = {Journal of Intelligent Manufacturing},
  publisher = {Springer Science and Business Media LLC},
  author    = {Grumbach,  Felix and M\"{u}ller,  Anna and Reusch,  Pascal and Trojahn,  Sebastian},
  year      = {2022},
  month     = dec,
  pages     = {667–686}
}
@article{Grumbach2023Memetic,
  title     = {A Memetic Algorithm With Reinforcement Learning for Sociotechnical Production Scheduling},
  volume    = {11},
  issn      = {2169-3536},
  url       = {http://dx.doi.org/10.1109/ACCESS.2023.3292548},
  doi       = {10.1109/access.2023.3292548},
  journal   = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
  author    = {Grumbach,  Felix and Badr,  Nour Eldin Alaa and Reusch,  Pascal and Trojahn,  Sebastian},
  year      = {2023},
  pages     = {68760–68775}
}
@article{haque2023explainable,
  title     = {
               Explainable Artificial Intelligence (XAI) from a user perspective: A
               synthesis of prior literature and problematizing avenues for future research
               },
  author    = {AKM Bahalul Haque and A. K.M.Najmul Islam and Patrick Mikalef},
  year      = 2023,
  month     = 1,
  journal   = {Technological Forecasting and Social Change},
  publisher = {North-Holland},
  volume    = 186,
  pages     = 122120,
  doi       = {10.1016/j.techfore.2022.122120},
  issn      = {0040-1625},
  abstract  = {
               The rapid growth and use of artificial intelligence (AI)-based systems have
               raised concerns regarding explainability. Recent studies have discussed the
               emerging demand for explainable AI (XAI); however, a systematic review of
               explainable artificial intelligence from an end user's perspective can
               provide a comprehensive understanding of the current situation and help close
               the research gap. The purpose of this study was to perform a systematic
               literature review of explainable AI from the end user's perspective and to
               synthesize the findings. To be precise, the objectives were to 1) identify
               the dimensions of end users' explanation needs; 2) investigate the effect of
               explanation on end user's perceptions, and 3) identify the research gaps and
               propose future research agendas for XAI, particularly from end users'
               perspectives based on current knowledge. The final search query for the
               Systematic Literature Review (SLR) was conducted on July 2022. Initially, we
               extracted 1707 journal and conference articles from the Scopus and Web of
               Science databases. Inclusion and exclusion criteria were then applied, and 58
               articles were selected for the SLR. The findings show four dimensions that
               shape the AI explanation, which are format (explanation representation
               format), completeness (explanation should contain all required information,
               including the supplementary information), accuracy (information regarding the
               accuracy of the explanation), and currency (explanation should contain recent
               information). Moreover, along with the automatic representation of the
               explanation, the users can request additional information if needed. We have
               also described five dimensions of XAI effects: trust, transparency,
               understandability, usability, and fairness. We investigated current knowledge
               from selected articles to problematize future research agendas as research
               questions along with possible research paths. Consequently, a comprehensive
               framework of XAI and its possible effects on user behavior has been
               developed.
               },
  keywords  = {
               AI Adoption,AI Use,Explainable AI
               (XAI),Transparency,Trust,Understandability,XAI effects
               }
}

@article{heuillet2021explainability,
  title     = {Explainability in deep reinforcement learning},
  author    = {Heuillet, Alexandre and Couthouis, Fabien and D{\'\i}az-Rodr{\'\i}guez, Natalia},
  journal   = {Knowledge-Based Systems},
  volume    = {214},
  pages     = {106685},
  year      = {2021},
  publisher = {Elsevier}
}

@article{hevner2004design,
  title     = {Design science in information systems research},
  author    = {Hevner, Alan R and March, Salvatore T and Park, Jinsoo and Ram, Sudha},
  journal   = {MIS quarterly},
  pages     = {75--105},
  year      = {2004},
  publisher = {JSTOR}
}

@article{hoffman2023evaluating,
  title     = {
               Evaluating machine-generated explanations: a ``Scorecard'' method for XAI
               measurement science
               },
  author    = {
               Robert R. Hoffman and Mohammadreza Jalaeian and Connor Tate and Gary Klein
               and Shane T. Mueller
               },
  year      = 2023,
  month     = 5,
  journal   = {Frontiers in Computer Science},
  publisher = {Frontiers Media S.A.},
  volume    = 5,
  pages     = 1114806,
  doi       = {10.3389/fcomp.2023.1114806/bibtex},
  issn      = 26249898,
  abstract  = {
               Introduction: Many Explainable AI (XAI) systems provide explanations that are
               just clues or hints about the computational models-Such things as feature
               lists, decision trees, or saliency images. However, a user might want answers
               to deeper questions such as How does it work?, Why did it do that instead of
               something else? What things can it get wrong? How might XAI system developers
               evaluate existing XAI systems with regard to the depth of support they
               provide for the user's sensemaking? How might XAI system developers shape new
               XAI systems so as to support the user's sensemaking? What might be a useful
               conceptual terminology to assist developers in approaching this challenge?
               Method: Based on cognitive theory, a scale was developed reflecting depth of
               explanation, that is, the degree to which explanations support the user's
               sensemaking. The seven levels of this scale form the Explanation Scorecard.
               Results and discussion: The Scorecard was utilized in an analysis of recent
               literature, showing that many systems still present low-level explanations.
               The Scorecard can be used by developers to conceptualize how they might
               extend their machine-generated explanations to support the user in developing
               a mental model that instills appropriate trust and reliance. The article
               concludes with recommendations for how XAI systems can be improved with
               regard to the cognitive considerations, and recommendations regarding the
               manner in which results on the evaluation of XAI systems are reported.
               },
  keywords  = {explainable AI,explanation scale,mental model,self-explanation,sensemaking}
}

@article{höhn2021what,
  title     = {What Does It Cost to Deploy an XAI System: A Case Study in Legacy Systems},
  author    = {Sviatlana H\"{o}hn and Niko Faradouris},
  year      = 2021,
  journal   = {
               Lecture Notes in Computer Science (including subseries Lecture Notes in
               Artificial Intelligence and Lecture Notes in Bioinformatics)
               },
  publisher = {Springer Science and Business Media Deutschland GmbH},
  volume    = {12688 Lnai},
  pages     = {173--186},
  doi       = {10.1007/978-3-030-82017-6\_11/tables/3 },
  isbn      = 9783030820169,
  issn      = 16113349,
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-82017-6\_11 },
  abstract  = {
               Enterprise Resource Planning (ERP) software is used by businesses and
               extended via customisation. Automated custom code analysis and migration is a
               critical issue at ERP release upgrade times. Despite research advances,
               automated code analysis and transformation require a huge amount of manual
               work related to parser adaptation, rule extension and post-processing. These
               operations become unmanageable if the frequency of updates increases from
               yearly to monthly intervals. This article describes how the process of custom
               code analysis to custom code transformation can be automated in an
               explainable way. We develop an aggregate taxonomy for explainability and
               analyse the requirements based on roles. We explain in which steps on the new
               code migration process machine learning is used. Further, we analyse
               additional effort needed to make the new way of code migration explainable to
               different stakeholders.
               },
  keywords  = {
               Explainability taxonomy,Explainable automated source-code
               transformation,Multi-modal conversational interfaces
               }
}

@article{jia2021prediction,
  title     = {Prediction of weaning from mechanical ventilation using convolutional neural networks},
  author    = {Jia, Yan and Kaul, Chaitanya and Lawton, Tom and Murray-Smith, Roderick and Habli, Ibrahim},
  journal   = {Artificial intelligence in medicine},
  volume    = {117},
  pages     = {102087},
  year      = {2021},
  publisher = {Elsevier}
}

@article{jin2021euca,
  title    = {EUCA: the End-User-Centered Explainable AI Framework},
  author   = {
              Weina Jin and Jianyu Fan and Diane Gromala and Philippe Pasquier and Ghassan
              Hamarneh
              },
  year     = 2021,
  month    = 2,
  url      = {https://arxiv.org/abs/2102.02437v2},
  abstract = {
              The ability to explain decisions to end-users is a necessity to deploy AI as
              critical decision support. Yet making AI explainable to non-technical
              end-users is a relatively ignored and challenging problem. To bridge the gap,
              we first identify twelve end-user-friendly explanatory forms that do not
              require technical knowledge to comprehend, including feature-, example-, and
              rule-based explanations. We then instantiate the explanatory forms as
              prototyping cards in four AI-assisted critical decision-making tasks, and
              conduct a user study to co-design low-fidelity prototypes with 32 layperson
              participants. The results confirm the relevance of using explanatory forms as
              building blocks of explanations, and identify their proprieties - pros, cons,
              applicable explanation goals, and design implications. The explanatory forms,
              their proprieties, and prototyping supports (including a suggested
              prototyping process, design templates and exemplars, and associated
              algorithms to actualize explanatory forms) constitute the End-User-Centered
              explainable AI framework EUCA, and is available at
              http://weinajin.github.io/end-user-xai . It serves as a practical prototyping
              toolkit for HCI/AI practitioners and researchers to understand user
              requirements and build end-user-centered explainable AI.
              },
  keywords = {
              Explainable Artificial Intelligence,Human-AI Collaboration,Machine Learning
              Interpretability,Usability Study,User-Centered Design
              }
}

@book{kamath2021explainable,
  title     = {Explainable artificial intelligence: an introduction to interpretable machine learning},
  author    = {Kamath, Uday and Liu, John},
  volume    = {2},
  year      = {2021},
  publisher = {Springer}
}

@article{Kayhan2021,
  title     = {Reinforcement learning applications to machine scheduling problems: a comprehensive literature review},
  volume    = {34},
  issn      = {1572-8145},
  url       = {http://dx.doi.org/10.1007/s10845-021-01847-3},
  doi       = {10.1007/s10845-021-01847-3},
  number    = {3},
  journal   = {Journal of Intelligent Manufacturing},
  publisher = {Springer Science and Business Media LLC},
  author    = {Kayhan,  Behice Meltem and Yildiz,  Gokalp},
  year      = {2021},
  month     = oct,
  pages     = {905–929}
}

@article{keleko2023health,
  title     = {Health condition monitoring of a complex hydraulic system using Deep Neural Network and DeepSHAP explainable XAI},
  author    = {Keleko, Aurelien Teguede and Kamsu-Foguem, Bernard and Ngouna, Raymond Houe and Tongne, Am{\`e}vi},
  journal   = {Advances in Engineering Software},
  volume    = {175},
  pages     = {103339},
  year      = {2023},
  publisher = {Elsevier}
}

@misc{khadivi2023deeprlmanufacturing,
  title         = {Deep reinforcement learning for machine scheduling: Methodology, the state-of-the-art, and future directions},
  author        = {Maziyar Khadivi and Todd Charter and Marjan Yaghoubi and Masoud Jalayer and Maryam Ahang and Ardeshir Shojaeinasab and Homayoun Najjaran},
  year          = {2023},
  eprint        = {2310.03195},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{kim2001application,
  title     = {Application of decision-tree induction techniques to personalized advertisements on internet storefronts},
  author    = {Kim, Jong Woo and Lee, Byung Hun and Shaw, Michael J and Chang, Hsin-Lu and Nelson, Matthew},
  journal   = {International Journal of Electronic Commerce},
  volume    = {5},
  number    = {3},
  pages     = {45--62},
  year      = {2001},
  publisher = {Taylor \& Francis}
}

@article{kim2021multi,
  title     = {
               A Multi-Component Framework for the Analysis and Design of Explainable
               Artificial Intelligence
               },
  author    = {
               Mi Young Kim and Shahin Atakishiyev and Housam Khalifa Bashier Babiker and
               Nawshad Farruque and Randy Goebel and Osmar R. Za\"{\i}ane and Mohammad
               Hossein Motallebi and Juliano Rabelo and Talat Syed and Hengshuai Yao and
               Peter Chun
               },
  year      = 2021,
  month     = 11,
  journal   = {Machine Learning and Knowledge Extraction 2021, Vol. 3, Pages 900-921},
  publisher = {Multidisciplinary Digital Publishing Institute},
  volume    = 3,
  pages     = {900--921},
  doi       = {10.3390/make3040045},
  issn      = {2504-4990},
  url       = {
               https://www.mdpi.com/2504-4990/3/4/45/htm
               https://www.mdpi.com/2504-4990/3/4/45
               },
  abstract  = {
               The rapid growth of research in explainable artificial intelligence (XAI)
               follows on two substantial developments. First, the enormous application
               success of modern machine learning methods, especially deep and reinforcement
               learning, have created high expectations for industrial, commercial, and
               social value. Second, the emerging and growing concern for creating ethical
               and trusted AI systems, including compliance with regulatory principles to
               ensure transparency and trust. These two threads have created a kind of
               ``perfect storm'' of research activity, all motivated to create and deliver
               any set of tools and techniques to address the XAI demand. As some surveys of
               current XAI suggest, there is yet to appear a principled framework that
               respects the literature of explainability in the history of science and which
               provides a basis for the development of a framework for transparent XAI. We
               identify four foundational components, including the requirements for (1)
               explicit explanation knowledge representation, (2) delivery of alternative
               explanations, (3) adjusting explanations based on knowledge of the explainee,
               and (4) exploiting the advantage of interactive explanation. With those four
               components in mind, we intend to provide a strategic inventory of XAI
               requirements, demonstrate their connection to a basic history of XAI ideas,
               and then synthesize those ideas into a simple framework that can guide the
               design of AI systems that require XAI.
               },
  issue     = 4,
  keywords  = {
               causal explanation,evaluation of explainable AI,explainable artificial
               intelligence,explainee,explanation,interpretation,specific explanation
               }
}

@article{kim2024do,
  title     = {
               Do stakeholder needs differ? - Designing stakeholder-tailored Explainable
               Artificial Intelligence (XAI) interfaces
               },
  author    = {
               Minjung Kim and Saebyeol Kim and Jinwoo Kim and Tae Jin Song and Yuyoung Kim
               },
  year      = 2024,
  month     = 1,
  journal   = {International Journal of Human-Computer Studies},
  publisher = {Academic Press},
  volume    = 181,
  pages     = 103160,
  doi       = {10.1016/j.ijhcs.2023.103160},
  issn      = {1071-5819},
  abstract  = {
               Explainable AI (XAI) is increasingly being used in the healthcare domain. In
               health management, clinicians and patients are critical stakeholders,
               requiring tailored XAI explanations based on their unique needs. Our study
               investigates the differences in explanation needs between clinicians and
               patients and designs corresponding explanation interfaces for each group.
               Using a scenario-based approach, we assessed stakeholder-tailored needs,
               analyzed differences, and designed interfaces using theoretical frameworks.
               The results demonstrate diverse stakeholder motivations for seeking
               explanations, leading to varied requirements. The designed interfaces
               effectively address these requirements, as validated by the preference
               selection and qualitative feedback from clinicians and patients. Their
               suggestions provide design insights and highlight the divergent needs of
               these stakeholder groups. This study contributes practical and theoretical
               implications to XAI research, emphasizing the importance of understanding
               diverse stakeholder needs and incorporating relevant theoretical concepts
               into user-centered interface design.
               },
  keywords  = {
               Digital health,Explanation interfaces,Explanation needs,Health
               management,Human-centered XAI,Medical XAI
               }
}

@article{kindermans2016investigating,
  title   = {Investigating the influence of noise and distractors on the interpretation of neural networks},
  author  = {Kindermans, Pieter-Jan and Sch{\"u}tt, Kristof and M{\"u}ller, Klaus-Robert and D{\"a}hne, Sven},
  journal = {arXiv preprint arXiv:1611.07270},
  year    = {2016}
}

@article{klar2024explainable,
  title     = {
               Explainable generative design in manufacturing for reinforcement learning
               based factory layout planning
               },
  author    = {
               Matthias Klar and Patrick Ruediger and Maik Schuermann and Goren Tobias
               G\"{o}ren and Moritz Glatt and Bahram Ravani and Jan C. Aurich
               },
  year      = 2024,
  month     = 2,
  journal   = {Journal of Manufacturing Systems},
  publisher = {Elsevier},
  volume    = 72,
  pages     = {74--92},
  doi       = {10.1016/j.jmsy.2023.11.012},
  issn      = {0278-6125},
  abstract  = {
               Generative design can be an effective approach to generate optimized factory
               layouts. One evolving topic in this field is the use of reinforcement
               learning (RL)-based approaches. Existing research has focused on the
               utilization of the approach without providing additional insights into the
               learned metrics and the derived policy. This information, however, is
               valuable from a layout planning perspective since the planner needs to ensure
               the trustworthiness and comprehensibility of the results. Furthermore, a
               deeper understanding of the learned policy and its influencing factors can
               help improve the manual planning process that follows as well as the
               acceptance of the results. These gaps in the existing approaches can be
               addressed by methods categorized as explainable artificial intelligence
               methods which have to be aligned with the properties of the problem and its
               audience. Consequently, this paper presents a method that will increase the
               trust in layouts generated by the RL approach. The method uses policy
               summarization and perturbation together with the state value evaluation. The
               method also uses explainable generative design for analyzing
               interrelationships between state values and actions at a feature level. The
               result is that the method identifies whether the RL approach learns the
               problem characteristics or if the solution is a result of random behavior.
               Furthermore, the method can be used to ensure that the reward function is
               aligned with the overall optimization goal and supports the planner in
               further detailed planning tasks by providing insights about the
               problem-defining interdependencies. The applicability of the proposed method
               is validated based on an industrial application scenario considering a layout
               planning case of 43 functional units. The results show that the method allows
               evaluation of the trustworthiness of the generated results by preventing
               randomly generated solutions from being considered in a detailed manual
               planning step. The paper concludes with a discussion of the results and a
               presentation of future research directions which also includes the transfer
               potentials of the proposed method to other application fields in RL-based
               generative design.
               },
  keywords  = {
               Explainable artificial intelligence,Explainable reinforcement
               learning,Facility layout problem,Factory layout planning,Policy summarization
               }
}

@inproceedings{kohlbrennerlrp,
  author    = {Kohlbrenner, Maximilian and Bauer, Alexander and Nakajima, Shinichi and Binder, Alexander and Samek, Wojciech and Lapuschkin, Sebastian},
  booktitle = {2020 International Joint Conference on Neural Networks (IJCNN)},
  title     = {Towards Best Practice in Explaining Neural Network Decisions with LRP},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {1-7},
  keywords  = {Neurons;Biological neural networks;Best practices;Feedforward neural networks;Visualization;Predictive models;layer-wise relevance propagation;explainable artificial intelligence;neural networks;visual object recognition;quantitative evaluation},
  doi       = {10.1109/IJCNN48605.2020.9206975}
}

@misc{kokhlikyan2020captum,
  title         = {Captum: A unified and generic model interpretability library for PyTorch},
  author        = {Narine Kokhlikyan and Vivek Miglani and Miguel Martin and Edward Wang and Bilal Alsallakh and Jonathan Reynolds and Alexander Melnikov and Natalia Kliushkina and Carlos Araya and Siqi Yan and Orion Reblitz-Richardson},
  year          = {2020},
  eprint        = {2009.07896},
  archiveprefix = {arXiv},
  primaryclass  = {id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}
@article{kotsiantis2013decision,
  title     = {Decision trees: a recent overview},
  author    = {Kotsiantis, Sotiris B},
  journal   = {Artificial Intelligence Review},
  volume    = {39},
  pages     = {261--283},
  year      = {2013},
  publisher = {Springer}
}

@article{kuhnle2022explainable,
  title     = {
               Explainable reinforcement learning in production control of job shop
               manufacturing system
               },
  author    = {Andreas Kuhnle and Marvin Carl May and Louis Sch\"{a}fer and Gisela Lanza},
  year      = 2022,
  month     = 10,
  journal   = {International Journal of Production Research},
  publisher = {Taylor \& Francis},
  volume    = 60,
  pages     = {5812--5834},
  doi       = {10.1080/00207543.2021.1972179},
  issn      = {1366588x},
  url       = {https://www.tandfonline.com/doi/abs/10.1080/00207543.2021.1972179},
  abstract  = {
               1. Reinforcement Learning (RL) and its applications have seen a rapid growth
               within the past years. Many domains, in particular in challenging
               optimisation and control tasks, fall into the ever-inc...
               },
  issue     = 19,
  keywords  = {
               explainability,production control,reinforcement learning,semiconductor
               manufacturing,simulation
               }
}
@article{langer2021what,
  title     = {
               What do we want from Explainable Artificial Intelligence (XAI)? – A
               stakeholder perspective on XAI and a conceptual model guiding
               interdisciplinary XAI research
               },
  author    = {
               Markus Langer and Daniel Oster and Timo Speith and Holger Hermanns and Lena
               K\"{a}stner and Eva Schmidt and Andreas Sesing and Kevin Baum
               },
  year      = 2021,
  month     = 7,
  journal   = {Artificial Intelligence},
  publisher = {Elsevier},
  volume    = 296,
  pages     = 103473,
  doi       = {10.1016/j.artint.2021.103473},
  issn      = {0004-3702},
  abstract  = {
               Previous research in Explainable Artificial Intelligence (XAI) suggests that
               a main aim of explainability approaches is to satisfy specific interests,
               goals, expectations, needs, and demands regarding artificial systems (we call
               these ``stakeholders' desiderata'') in a variety of contexts. However, the
               literature on XAI is vast, spreads out across multiple largely disconnected
               disciplines, and it often remains unclear how explainability approaches are
               supposed to achieve the goal of satisfying stakeholders' desiderata. This
               paper discusses the main classes of stakeholders calling for explainability
               of artificial systems and reviews their desiderata. We provide a model that
               explicitly spells out the main concepts and relations necessary to consider
               and investigate when evaluating, adjusting, choosing, and developing
               explainability approaches that aim to satisfy stakeholders' desiderata. This
               model can serve researchers from the variety of different disciplines
               involved in XAI as a common ground. It emphasizes where there is
               interdisciplinary potential in the evaluation and the development of
               explainability approaches.
               },
  keywords  = {
               Explainability,Explainable Artificial
               Intelligence,Explanations,Human-Computer Interaction,Interdisciplinary
               Research,Interpretability,Understanding
               }
}

@article{leavitt2020towards,
  title   = {Towards falsifiable interpretability research},
  author  = {Leavitt, Matthew L and Morcos, Ari},
  journal = {arXiv preprint arXiv:2010.12016},
  year    = {2020}
}

@book{leskovec2014mining,
  title     = {Mining of Massive Datasets},
  author    = {Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
  year      = 2014,
  publisher = {Cambridge University Press},
  address   = {Usa},
  isbn      = 1107077230,
  edition   = {2nd},
  abstract  = {
               Written by leading authorities in database and Web technologies, this book is
               essential reading for students and practitioners alike. The popularity of the
               Web and Internet commerce provides many extremely large datasets from which
               information can be gleaned by data mining. This book focuses on practical
               algorithms that have been used to solve key problems in data mining and can
               be applied successfully to even the largest datasets. It begins with a
               discussion of the map-reduce framework, an important tool for parallelizing
               algorithms automatically. The authors explain the tricks of
               locality-sensitive hashing and stream processing algorithms for mining data
               that arrives too fast for exhaustive processing. Other chapters cover the
               PageRank idea and related tricks for organizing the Web, the problems of
               finding frequent itemsets and clustering. This second edition includes new
               and extended coverage on social networks, machine learning and dimensionality
               reduction.
               }
}

@article{liehner2023perceptions,
  title     = {Perceptions, attitudes and trust towards artificial intelligence—an assessment of the public opinion},
  author    = {Liehner, Gian Luca and Biermann, Hannah and Hick, Alexander and Brauner, Philipp and Ziefle, Martina},
  journal   = {Artificial Intelligence and Social Computing},
  volume    = {72},
  number    = {72},
  year      = {2023},
  publisher = {AHFE Open Acces}
}

@article{longo2024explainable,
  title     = {
               Explainable Artificial Intelligence (XAI) 2.0: A manifesto of open challenges
               and interdisciplinary research directions
               },
  author    = {
               Luca Longo and Mario Brcic and Federico Cabitza and Jaesik Choi and Roberto
               Confalonieri and Javier Del Ser and Riccardo Guidotti and Yoichi Hayashi and
               Francisco Herrera and Andreas Holzinger and Richard Jiang and Hassan Khosravi
               and Freddy Lecue and Gianclaudio Malgieri and Andr\'{e}s P\'{a}ez and
               Wojciech Samek and Johannes Schneider and Timo Speith and Simone Stumpf
               },
  year      = 2024,
  month     = 6,
  journal   = {Information Fusion},
  publisher = {Elsevier},
  volume    = 106,
  pages     = 102301,
  doi       = {10.1016/j.inffus.2024.102301},
  issn      = {1566-2535},
  abstract  = {
               Understanding black box models has become paramount as systems based on
               opaque Artificial Intelligence (AI) continue to flourish in diverse
               real-world applications. In response, Explainable AI (XAI) has emerged as a
               field of research with practical and ethical benefits across various domains.
               This paper highlights the advancements in XAI and its application in
               real-world scenarios and addresses the ongoing challenges within XAI,
               emphasizing the need for broader perspectives and collaborative efforts. We
               bring together experts from diverse fields to identify open problems,
               striving to synchronize research agendas and accelerate XAI in practical
               applications. By fostering collaborative discussion and interdisciplinary
               cooperation, we aim to propel XAI forward, contributing to its continued
               success. We aim to develop a comprehensive proposal for advancing XAI. To
               achieve this goal, we present a manifesto of 28 open problems categorized
               into nine categories. These challenges encapsulate the complexities and
               nuances of XAI and offer a road map for future research. For each problem, we
               provide promising research directions in the hope of harnessing the
               collective intelligence of interested stakeholders.
               },
  keywords  = {
               Actionable XAI,Causality,Concept-based explanations,Ethical AI,Explainable
               artificial intelligence,Falsifiability,Generative
               AI,Interdisciplinarity,Interpretability,Large language
               models,Manifesto,Multi-faceted explanations,Open challenges,Responsible
               AI,Trustworthy AI,XAI
               }
}
@article{lorente2022xai,
  title     = {XAI Systems Evaluation: A Review of Human and Computer-Centred Methods},
  author    = {
               Sesmero Lorente and Plamen Angelov and Jose Antonio and Iglesias Martinez and
               Pedro Lopes and Eduardo Silva and Cristiana Braga and Tiago Oliveira and
               Lu\'{\i}s Rosado
               },
  year      = 2022,
  month     = 9,
  journal   = {Applied Sciences 2022, Vol. 12, Page 9423},
  publisher = {Multidisciplinary Digital Publishing Institute},
  volume    = 12,
  pages     = 9423,
  doi       = {10.3390/app12199423},
  issn      = {2076-3417},
  url       = {
               https://www.mdpi.com/2076-3417/12/19/9423/htm
               https://www.mdpi.com/2076-3417/12/19/9423
               },
  abstract  = {
               The lack of transparency of powerful Machine Learning systems paired with
               their growth in popularity over the last decade led to the emergence of the
               eXplainable Artificial Intelligence (XAI) field. Instead of focusing solely
               on obtaining highly performing models, researchers also develop explanation
               techniques that help better understand the system's reasoning for a
               particular output. An explainable system can be designed, developed, and
               evaluated from different perspectives, which enables researchers from
               different disciplines to work together on this topic. However, the
               multidisciplinary nature of XAI systems creates new challenges for condensing
               and structuring adequate methodologies to design and evaluate such systems.
               This paper presents a survey of Human-centred and Computer-centred methods to
               evaluate XAI systems. We propose a new taxonomy to categorize XAI evaluation
               methods more clearly and intuitively. This categorization gathers knowledge
               from different disciplines and organizes the evaluation methods according to
               a set of categories that represent key properties of XAI systems. Possible
               ways to use the proposed taxonomy in the design and evaluation of XAI systems
               are also discussed, alongside with some concluding remarks and future
               directions of research.
               },
  issue     = 19,
  keywords  = {
               centred,computer,evaluation methods,explainable artificial
               intelligence,human,literature review
               }
}
@article{lundberg2017unified,
  title   = {A unified approach to interpreting model predictions},
  author  = {Lundberg, Scott M and Lee, Su-In},
  journal = {Advances in neural information processing systems},
  volume  = {30},
  year    = {2017}
}

@article{martinez_plumed2021crisp,
  title     = {
               CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science
               Trajectories
               },
  author    = {
               Fernando Martinez-Plumed and Lidia Contreras-Ochando and Cesar Ferri and Jose
               Hernandez-Orallo and Meelis Kull and Nicolas Lachiche and Maria Jose
               Ramirez-Quintana and Peter Flach
               },
  year      = 2021,
  month     = 8,
  journal   = {IEEE Transactions on Knowledge and Data Engineering},
  publisher = {IEEE Computer Society},
  volume    = 33,
  pages     = {3048--3061},
  doi       = {10.1109/tkde.2019.2962680  },
  issn      = 15582191,
  abstract  = {
               CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in
               the second half of the nineties and is thus about two decades old. According
               to many surveys and user polls it is still the de facto standard for
               developing data mining and knowledge discovery projects. However, undoubtedly
               the field has moved on considerably in twenty years, with data science now
               the leading term being favoured over data mining. In this paper we
               investigate whether, and in what contexts, CRISP-DM is still fit for purpose
               for data science projects. We argue that if the project is goal-directed and
               process-driven the process model view still largely holds. On the other hand,
               when data science projects become more exploratory the paths that the project
               can take become more varied, and a more flexible model is called for. We
               suggest what the outlines of such a trajectory-based model might look like
               and how it can be used to categorise data science projects (goal-directed,
               exploratory or data management). We examine seven real-life exemplars where
               exploratory activities play an important role and compare them against 51 use
               cases extracted from the NIST Big Data Public Working Group. We anticipate
               this categorisation can help project planning in terms of time and cost
               characteristics.
               },
  issue     = 8,
  keywords  = {
               Data science trajectories,data mining,data-driven methodologies,knowledge
               discovery process
               }
}

@article{milani2024explainable,
  title     = {Explainable reinforcement learning: A survey and comparative review},
  author    = {Milani, Stephanie and Topin, Nicholay and Veloso, Manuela and Fang, Fei},
  journal   = {ACM Computing Surveys},
  volume    = {56},
  number    = {7},
  pages     = {1--36},
  year      = {2024},
  publisher = {ACM New York, NY}
}
@article{mnih2015drl,
  title     = {Human-level control through deep reinforcement learning},
  author    = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg et al.},
  journal   = {nature},
  volume    = {518},
  number    = {7540},
  pages     = {529--533},
  year      = {2015},
  publisher = {Nature Publishing Group UK London}
}

@article{mohseni202124,
  title    = {
              24 A Multidisciplinary Survey and Framework for Design and Evaluation of
              Explainable AI Systems
              },
  author   = {Sina Mohseni and Niloofar Zarei and Eric D Ragan and ; E D Ragan},
  year     = 2021,
  journal  = {ACM Transactions on Interactive Intelligent Systems},
  volume   = 11,
  doi      = {10.1145/3387166},
  isbn     = {10.1145/3387166},
  url      = {https://doi.org/10.1145/3387166},
  abstract = {
              The need for interpretable and accountable intelligent systems grows along
              with the prevalence of artificial intelligence (AI) applications used in
              everyday life. Explainable AI (XAI) systems are intended to self-explain the
              reasoning behind system decisions and predictions. Researchers from different
              disciplines work together to define, design, and evaluate explainable
              systems. However, scholars from different disciplines focus on different
              objectives and fairly independent topics of XAI research, which poses
              challenges for identifying appropriate design and evaluation methodology and
              consolidating knowledge across efforts. To this end, this article presents a
              survey and framework intended to share knowledge and experiences of XAI
              design and evaluation methods across multiple disciplines. Aiming to support
              diverse design goals and evaluation methods in XAI research, after a thorough
              review of XAI related papers in the fields of machine learning,
              vi-sualization, and human-computer interaction, we present a categorization
              of XAI design goals and evaluation methods. Our categorization presents the
              mapping between design goals for different XAI user groups and their
              evaluation methods. From our findings, we develop a framework with
              step-by-step design guidelines paired with evaluation methods to close the
              iterative design and evaluation cycles in multidisciplinary XAI teams.
              Further, we provide summarized ready-to-use tables of evaluation methods and
              recommendations for different goals in XAI research.
              },
  issue    = 4,
  keywords = {
              Explainable artificial intelligence (XAI),explanation,human-computer
              interaction (HCI),machine learning,transparency
              }
}
@article{MONTAVON20181,
  title    = {Methods for interpreting and understanding deep neural networks},
  journal  = {Digital Signal Processing},
  volume   = {73},
  pages    = {1-15},
  year     = {2018},
  issn     = {1051-2004},
  doi      = {https://doi.org/10.1016/j.dsp.2017.10.011},
  url      = {https://www.sciencedirect.com/science/article/pii/S1051200417302385},
  author   = {Grégoire Montavon and Wojciech Samek and Klaus-Robert Müller},
  keywords = {Deep neural networks, Activation maximization, Sensitivity analysis, Taylor decomposition, Layer-wise relevance propagation},
  abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. As a tutorial paper, the set of methods covered here is not exhaustive, but sufficiently representative to discuss a number of questions in interpretability, technical challenges, and possible applications. The second part of the tutorial focuses on the recently proposed layer-wise relevance propagation (LRP) technique, for which we provide theory, recommendations, and tricks, to make most efficient use of it on real data.}
}
@misc{moritz2018ray,
  title         = {Ray: A Distributed Framework for Emerging AI Applications},
  author        = {Philipp Moritz and Robert Nishihara and Stephanie Wang and Alexey Tumanov and Richard Liaw and Eric Liang and Melih Elibol and Zongheng Yang and William Paul and Michael I. Jordan and Ion Stoica},
  year          = {2018},
  eprint        = {1712.05889},
  archiveprefix = {arXiv},
  primaryclass  = {cs.DC}
}
@misc{müller2023bridging,
  title         = {Bridging the Reality Gap of Reinforcement Learning based Traffic Signal Control using Domain Randomization and Meta Learning},
  author        = {Arthur Müller and Matthia Sabatelli},
  year          = {2023},
  eprint        = {2307.11357},
  archiveprefix = {arXiv},
  primaryclass  = {id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@article{muller2024reinforcement,
  title     = {
               Reinforcement Learning for Two-Stage Permutation Flow Shop Scheduling - A
               Real-World Application in Household Appliance Production
               },
  author    = {Arthur Muller and Felix Grumbach and Fiona Kattenstroth},
  year      = 2024,
  journal   = {IEEE Access},
  publisher = {Institute of Electrical and Electronics Engineers Inc.},
  volume    = 12,
  pages     = {11388--11399},
  doi       = {10.1109/access.2024.3355269},
  issn      = 21693536,
  abstract  = {
               Solving production scheduling problems is a difficult and indispensable task
               for manufacturers with a push-oriented planning approach. In this study, we
               tackle a novel production scheduling problem from a household appliance
               production at the company Miele \& Cie. KG, namely a two-stage permutation
               flow shop scheduling problem (PFSSP) with a finite buffer and
               sequence-dependent setup efforts. The objective is to minimize idle times and
               setup efforts in lexicographic order. In extensive and realistic data, the
               identification of exact solutions is not possible due to the combinatorial
               complexity. Therefore, we developed a reinforcement learning (RL) approach
               based on the Proximal Policy Optimization (PPO) algorithm that integrates
               domain knowledge through reward shaping, action masking, and curriculum
               learning to solve this PFSSP. Benchmarking of our approach with a
               state-of-the-art genetic algorithm (GA) showed significant superiority. Our
               work thus provides a successful example of the applicability of RL in
               real-world production planning, demonstrating not only its practical utility
               but also showing the technical and methodological integration of the agent
               with a discrete event simulation (DES). We also conducted experiments to
               investigate the impact of individual algorithmic elements and a
               hyperparameter of the reward function on the overall solution.
               },
  keywords  = {
               Reinforcement learning,permutation flow shop scheduling problem,production
               scheduling
               }
}

@article{müller2024reinforcement,
  title    = {
              Reinforcement Learning for Two-Stage Permutation Flow Shop Scheduling--A
              Real-World Application in Household Appliance Production
              },
  author   = {Müller, Arthur and Grumbach, Felix and Kattenstroth, Fiona},
  year     = 2024,
  journal  = {IEEE Access},
  volume   = 12,
  number   = {},
  pages    = {11388--11399},
  doi      = {10.1109/access.2024.3355269},
  keywords = {
              Production;Job shop scheduling;Home appliances;Metaheuristics;Benchmark
              testing;Finite element analysis;Synthetic data;Reinforcement
              learning;production scheduling;permutation flow shop scheduling problem
              }
}

@misc{müller2024smaller,
  title         = {Smaller Batches, Bigger Gains? Investigating the Impact of Batch Sizes on Reinforcement Learning Based Real-World Production Scheduling},
  author        = {Arthur Müller and Felix Grumbach and Matthia Sabatelli},
  year          = {2024},
  eprint        = {2406.02294},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  note          = {Test}
}

@article{nahavandi2019industry,
  title          = {Industry 5.0--A Human-Centric Solution},
  author         = {Nahavandi, Saeid},
  year           = 2019,
  journal        = {Sustainability},
  volume         = 11,
  number         = 16,
  doi            = {10.3390/su11164371},
  issn           = {2071-1050},
  url            = {https://www.mdpi.com/2071-1050/11/16/4371},
  article-number = 4371,
  abstract       = {
                    Staying at the top is getting tougher and more challenging due to the
                    fast-growing and changing digital technologies and AI-based solutions. The
                    world of technology, mass customization, and advanced manufacturing is
                    experiencing a rapid transformation. Robots are becoming even more important
                    as they can now be coupled with the human mind by means of brain–machine
                    interface and advances in artificial intelligence. A strong necessity to
                    increase productivity while not removing human workers from the manufacturing
                    industry is imposing punishing challenges on the global economy. To counter
                    these challenges, this article introduces the concept of Industry 5.0, where
                    robots are intertwined with the human brain and work as collaborator instead
                    of competitor. This article also outlines a number of key features and
                    concerns that every manufacturer may have about Industry 5.0. In addition, it
                    presents several developments achieved by researchers for use in Industry 5.0
                    applications and environments. Finally, the impact of Industry 5.0 on the
                    manufacturing industry and overall economy is discussed from an economic and
                    productivity point of view, where it is argued that Industry 5.0 will create
                    more jobs than it will take away.
                    }
}

@article{naiseh2024c,
  title     = {
               C-XAI: A conceptual framework for designing XAI tools that support trust
               calibration
               },
  author    = {
               Mohammad Naiseh and Auste Simkute and Baraa Zieni and Nan Jiang and Raian Ali
               },
  year      = 2024,
  month     = 3,
  journal   = {Journal of Responsible Technology},
  publisher = {Elsevier},
  volume    = 17,
  pages     = 100076,
  doi       = {10.1016/j.jrt.2024.100076},
  issn      = {2666-6596},
  abstract  = {
               Recent advancements in machine learning have spurred an increased integration
               of AI in critical sectors such as healthcare and criminal justice. The
               ethical and legal concerns surrounding fully autonomous AI highlight the
               importance of combining human oversight with AI to elevate decision-making
               quality. However, trust calibration errors in human-AI collaboration,
               encompassing instances of over-trust or under-trust in AI recommendations,
               pose challenges to overall performance. Addressing trust calibration in the
               design process is essential, and eXplainable AI (XAI) emerges as a valuable
               tool by providing transparent AI explanations. This paper introduces
               Calibrated-XAI (C-XAI), a participatory design framework specifically crafted
               to tackle both technical and human factors in the creation of XAI interfaces
               geared towards trust calibration in Human-AI collaboration. The primary
               objective of the C-XAI framework is to assist designers of XAI interfaces in
               minimising trust calibration errors at the design level. This is achieved
               through the adoption of a participatory design approach, which includes
               providing templates, guidance, and involving diverse stakeholders in the
               design process. The efficacy of C-XAI is evaluated through a two-stage
               evaluation study, demonstrating its potential to aid designers in
               constructing user interfaces with trust calibration in mind. Through this
               work, we aspire to offer systematic guidance to practitioners, fostering a
               responsible approach to eXplainable AI at the user interface level.
               },
  keywords  = {Explainable ai,Human-AI teaming,Human-centred design,Participatory design}
}

@article{nakao2022towards,
  title     = {
               Towards Responsible AI: A Design Space Exploration of Human-Centered
               Artificial Intelligence User Interfaces to Investigate Fairness
               },
  author    = {
               Yuri Nakao and Lorenzo Strappelli and Simone Stumpf and Aisha Naseer and
               Daniele Regoli and Giulia Del Gamba
               },
  year      = 2022,
  month     = 6,
  journal   = {International Journal of Human-Computer Interaction},
  publisher = {Taylor and Francis Ltd.},
  volume    = 39,
  pages     = {1762--1788},
  doi       = {10.1080/10447318.2022.2067936},
  issn      = 15327590,
  url       = {https://arxiv.org/abs/2206.00474v1},
  abstract  = {
               With Artificial intelligence (AI) to aid or automate decision-making
               advancing rapidly, a particular concern is its fairness. In order to create
               reliable, safe and trustworthy systems through human-centred artificial
               intelligence (HCAI) design, recent efforts have produced user interfaces
               (UIs) for AI experts to investigate the fairness of AI models. In this work,
               we provide a design space exploration that supports not only data scientists
               but also domain experts to investigate AI fairness. Using loan applications
               as an example, we held a series of workshops with loan officers and data
               scientists to elicit their requirements. We instantiated these requirements
               into FairHIL, a UI to support human-in-the-loop fairness investigations, and
               describe how this UI could be generalized to other use cases. We evaluated
               FairHIL through a think-aloud user study. Our work contributes better designs
               to investigate an AI model's fairness-and move closer towards responsible AI.
               },
  issue     = 9,
  keywords  = {
               Human-centred AI,data scientists,domain
               experts,fairness,human-in-the-loop,user interfaces,visualizations
               }
}

@inproceedings{napolitano2023learning,
  title     = {Learning Confidence Intervals for Feature Importance: A Fast Shapley-based Approach.},
  author    = {Napolitano, Davide and Vaiani, Lorenzo and Cagliero, Luca et al.},
  booktitle = {EDBT/ICDT Workshops},
  year      = {2023}
}

@article{nyawa2023transparent,
  title     = {
               Transparent machine learning models for predicting decisions to undertake
               energy retrofits in residential buildings
               },
  author    = {Serge Nyawa and Christian Gnekpe and Dieudonn\'{e} Tchuente},
  year      = 2023,
  month     = 2,
  journal   = {Annals of Operations Research},
  publisher = {Springer},
  pages     = {1--29},
  doi       = {10.1007/s10479-023-05217-5/tables/5},
  issn      = 15729338,
  url       = {https://link.springer.com/article/10.1007/s10479-023-05217-5},
  abstract  = {
               Transparent and explainable machine learning (ML) models are essential in
               various domains, e.g., energy consumption, where decarbonization is the main
               challenge. The European Union is focusing on energy efficiency retrofits in
               residential buildings to help reach its 2050 carbon emissions target. The
               cost of these investments is often a strong factor, requiring decision-makers
               to understand the motivations driving homeowners' decisions to undertake
               energy retrofits. Instead of hedonic models commonly used in operational
               management research studies, we rely on ML methods to predict homeowners'
               decisions to undertake energy retrofits, using data from 51,000 households in
               France. We describe the data preparation, model training, and evaluation;
               results show that artificial neural networks outperform other popular ML
               techniques (91.4\%). Our post hoc method based on sensitivity analysis and
               feature importance contributes to the transparency and interpretability of
               the results. We show that the type of public aid used, head of household
               gender, family size, prior knowledge of aid, urban vs. rural area,
               geographical location, occupancy status, and working status are the most
               important factors in the decision to undertake energy efficiency retrofits.
               Our predictive methods help decision-makers to make optimal decisions about
               the level, type, beneficiaries of public incentives for energy retrofits, and
               expected outcomes; companies in the construction sector can understand
               homeowners' key motivations and optimally calibrate their strategic
               investments and operations.
               },
  keywords  = {
               Classification,Deep learning,Energy efficiency
               retrofits,France,Interpretability,Machine learning transparency
               }
}

@article{ozer2023explainable,
  title   = {Explainable image quality assessment for medical imaging},
  author  = {Ozer, Caner and Guler, Arda and Cansever, Aysel Turkvatan and Oksuz, Ilkay},
  journal = {arXiv preprint arXiv:2303.14479},
  year    = {2023}
}


@inproceedings{palacio2021xai,
  title     = {Xai handbook: towards a unified framework for explainable AI},
  author    = {Palacio, Sebastian and Lucieri, Adriano and Munir, Mohsin and Ahmed, Sheraz and Hees, J{\"o}rn and Dengel, Andreas},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {3766--3775},
  year      = {2021}
}


@misc{paszke2019pytorchimperativestylehighperformance,
  title         = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author        = {Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  year          = {2019},
  eprint        = {1912.01703},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/1912.01703}
}

@article{peters2019artificial,
  title     = {Artificial intelligence, human evolution, and the speed of learning},
  author    = {Peters, Michael A and Jandri{\'c}, Petar},
  journal   = {Artificial Intelligence and Inclusive Education: speculative futures and emerging practices},
  pages     = {195--206},
  year      = {2019},
  publisher = {Springer}
}


@book{pinedo2012scheduling,
  title     = {Scheduling},
  author    = {Pinedo, Michael L},
  volume    = {29},
  year      = {2012},
  publisher = {Springer}
}


@inproceedings{pmlr-v238-neuhof24a,
  title     = {Confident Feature Ranking},
  author    = {Neuhof, Bitya and Benjamini, Yuval},
  booktitle = {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages     = {1468--1476},
  year      = {2024},
  editor    = {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume    = {238},
  series    = {Proceedings of Machine Learning Research},
  month     = {02--04 May},
  publisher = {PMLR},
  pdf       = {https://proceedings.mlr.press/v238/neuhof24a/neuhof24a.pdf},
  url       = {https://proceedings.mlr.press/v238/neuhof24a.html},
  abstract  = {Machine learning models are widely applied in various fields. Stakeholders often use post-hoc feature importance methods to better understand the input features’ contribution to the models’ predictions. The interpretation of the importance values provided by these methods is frequently based on the relative order of the features (their ranking) rather than the importance values themselves. Since the order may be unstable, we present a framework for quantifying the uncertainty in global importance values. We propose a novel method for the post-hoc interpretation of feature importance values that is based on the framework and pairwise comparisons of the feature importance values. This method produces simultaneous confidence intervals for the features’ ranks, which include the “true” (infinite sample) ranks with high probability, and enables the selection of the set of top-k important features.}
}

@article{popper1963science,
  title   = {Science as falsification},
  author  = {Popper, Karl R},
  year    = 1963,
  journal = {Conjectures and refutations},
  volume  = 1,
  number  = 1963,
  pages   = {33--39}
}

@article{Portoleau2024,
  abstract  = {In this paper, we advocate the use of robust decision trees for a multi-mode resource constrained project scheduling problem with uncertain activity duration and a resource investment objective, coming from an industrial assembly line scheduling application. This work takes place in the context of multi-stage optimization where uncertainty is revealed progressively across a succession of decision time points. In a robust decision tree, a node represents a robust partial schedule from the time origin to a specific decision time point. At this point, the decision maker has access to some information, which partitions the uncertainty scenario set, yielding for each scenario subset a child node and an associated extended partial robust schedule up to the next decision point. Considering that the level of uncertainty is lowered, the new partial schedule is less conservative and improves the robustness guarantee. However, since all accessible information may not be relevant, we turned the information selection part into an optimization problem. An algorithm is proposed to solve the robust decision tree problem. Experimentation is provided to study the influence of decision tree parameters as well as highlighted recommendations. The interest of the decision tree is shown through an experimental comparison with classical approaches of the literature on benchmark instances and industrial instances.},
  author    = {Tom Portoleau and Christian Artigues and Romain Guillaume},
  doi       = {10.1016/J.EJOR.2023.07.035},
  issn      = {0377-2217},
  issue     = {2},
  journal   = {European Journal of Operational Research},
  keywords  = {Decision trees,Multi-mode resource-constrained project scheduling,Multistage robust optimization,Resource investment,Scheduling},
  month     = {1},
  pages     = {525-540},
  publisher = {North-Holland},
  title     = {Robust decision trees for the multi-mode project scheduling problem with a resource investment objective and uncertain activity duration},
  volume    = {312},
  year      = {2024}
}

@article{preece2018stakeholders,
  title    = {Stakeholders in Explainable AI},
  author   = {
              Alun Preece and Dan Harborne and Dave Braines and Richard Tomsett and Supriyo
              Chakraborty
              },
  year     = 2018,
  month    = 9,
  url      = {https://arxiv.org/abs/1810.00184v1},
  abstract = {
              There is general consensus that it is important for artificial intelligence
              (AI) and machine learning systems to be explainable and/or interpretable.
              However, there is no general consensus over what is meant by 'explainable'
              and 'interpretable'. In this paper, we argue that this lack of consensus is
              due to there being several distinct stakeholder communities. We note that,
              while the concerns of the individual communities are broadly compatible, they
              are not identical, which gives rise to different intents and requirements for
              explainability/interpretability. We use the software engineering distinction
              between validation and verification, and the epistemological distinctions
              between knowns/unknowns, to tease apart the concerns of the stakeholder
              communities and highlight the areas where their foci overlap or diverge. It
              is not the purpose of the authors of this paper to 'take sides' - we count
              ourselves as members, to varying degrees, of multiple communities - but
              rather to help disambiguate what stakeholders mean when they ask 'Why?' of an
              AI.
              }
}

@inproceedings{puiutta2020explainable,
  title        = {Explainable reinforcement learning: A survey},
  author       = {Puiutta, Erika and Veith, Eric MSP},
  booktitle    = {International cross-domain conference for machine learning and knowledge extraction},
  pages        = {77--95},
  year         = {2020},
  organization = {Springer}
}


@article{quinlan1986induction,
  title     = {Induction of decision trees},
  author    = {Quinlan, J. Ross},
  journal   = {Machine learning},
  volume    = {1},
  pages     = {81--106},
  year      = {1986},
  publisher = {Springer}
}

@inproceedings{ribeiro2016should,
  title     = {" Why should i trust you?" Explaining the predictions of any classifier},
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  booktitle = {Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},
  pages     = {1135--1144},
  year      = {2016}
}

@article{ribera2019can,
  title    = {Can we do better explanations? A proposal of User-Centered Explainable AI},
  author   = {Mireia Ribera and Agata Lapedriza},
  year     = 2019,
  abstract = {
              Artificial Intelligence systems are spreading to multiple applications and
              they are used by a more diverse audience. With this change of the use
              scenario, AI users will increasingly require explanations. The first part of
              this paper makes a review of the state of the art of Explainable AI and
              highlights how the current research is not paying enough attention to whom
              the explanations are targeted. In the second part of the paper, it is
              suggested a new explainability pipeline, where users are classified in three
              main groups (developers or AI researchers, domain experts and lay users).
              Inspired by the cooperative principles of conversations, it is discussed how
              creating different explanations for each of the targeted groups can overcome
              some of the difficulties related to creating good explanations and evaluating
              them. CCS CONCEPTS \textbullet{} Computing methodologies \rightarrow{}
              Artificial intelligence; \textbullet{} Human-centered computing \rightarrow{}
              HCI theory, concepts and models.
              },
  keywords = {Conversational interfaces,Explainability,HCI,User centered design,XAI}
}
@article{riveiro2021s,
  title     = {“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems},
  author    = {Riveiro, Maria and Thill, Serge},
  journal   = {Artificial Intelligence},
  volume    = {298},
  pages     = {103507},
  year      = {2021},
  publisher = {Elsevier}
}

@article{sado2023explainable,
  title     = {Explainable goal-driven agents and robots-a comprehensive review},
  author    = {Sado, Fatai and Loo, Chu Kiong and Liew, Wei Shiung and Kerzel, Matthias and Wermter, Stefan},
  journal   = {ACM Computing Surveys},
  volume    = {55},
  number    = {10},
  pages     = {1--41},
  year      = {2023},
  publisher = {ACM New York, NY}
}

@article{samek2017explainable,
  title   = {
             Explainable Artificial Intelligence: Understanding, Visualizing and
             Interpreting Deep Learning Models
             },
  author  = {W. Samek and T. Wiegand and K. M\"{u}ller},
  year    = 2017,
  journal = {ArXiv}
}

@article{schoonderwoerd2021human,
  title     = {
               Human-centered XAI: Developing design patterns for explanations of clinical
               decision support systems
               },
  author    = {
               Tjeerd A.J. Schoonderwoerd and Wiard Jorritsma and Mark A. Neerincx and Karel
               van den Bosch
               },
  year      = 2021,
  month     = 10,
  journal   = {International Journal of Human-Computer Studies},
  publisher = {Academic Press},
  volume    = 154,
  pages     = 102684,
  doi       = {10.1016/j.ijhcs.2021.102684},
  issn      = {1071-5819},
  abstract  = {
               Much of the research on eXplainable Artificial Intelligence (XAI) has
               centered on providing transparency of machine learning models. More recently,
               the focus on human-centered approaches to XAI has increased. Yet, there is a
               lack of practical methods and examples on the integration of human factors
               into the development processes of AI-generated explanations that humans prove
               to uptake for better performance. This paper presents a case study of an
               application of a human-centered design approach for AI-generated
               explanations. The approach consists of three components: Domain analysis to
               define the concept \& context of explanations, Requirements elicitation \&
               assessment to derive the use cases \& explanation requirements, and the
               consequential Multi-modal interaction design \& evaluation to create a
               library of design patterns for explanations. In a case study, we adopt the
               DoReMi-approach to design explanations for a Clinical Decision Support System
               (CDSS) for child health. In the requirements elicitation \& assessment, a
               user study with experienced paediatricians uncovered what explanations the
               CDSS should provide. In the interaction design \& evaluation, a second user
               study tested the consequential interaction design patterns. This case study
               provided a first set of user requirements and design patterns for an
               explainable decision support system in medical diagnosis, showing how to
               involve expert end users in the development process and how to develop, more
               or less, generic solutions for general design problems in XAI.
               },
  keywords  = {
               Causability,Clinical decision making,Decision-support system,Design
               patterns,Explainability,Explainable AI,Human-centered design,User study
               }
}

@article{senoner2021using,
  title     = {
               Using Explainable Artificial Intelligence to Improve Process Quality:
               Evidence from Semiconductor Manufacturing
               },
  author    = {Julian Senoner and Torbj\o{}rn Netland and Stefan Feuerriegel},
  year      = 2021,
  month     = 12,
  journal   = {https://doi.org/10.1287/mnsc.2021.4190},
  publisher = {Informs},
  volume    = 68,
  pages     = {5704--5723},
  doi       = {10.1287/mnsc.2021.4190},
  issn      = 15265501,
  url       = {https://pubsonline.informs.org/doi/abs/10.1287/mnsc.2021.4190},
  abstract  = {
               We develop a data-driven decision model to improve process quality in
               manufacturing. A challenge for traditional methods in quality management is
               to handle high-dimensional and nonlinear manufactur...
               },
  issue     = 8,
  keywords  = {SHAP value method,artificial intelligence,manufacturing,quality management}
}

@article{SERRANORUIZ2024100582,
  title    = {Job shop smart manufacturing scheduling by deep reinforcement learning},
  journal  = {Journal of Industrial Information Integration},
  volume   = {38},
  pages    = {100582},
  year     = {2024},
  issn     = {2452-414X},
  doi      = {https://doi.org/10.1016/j.jii.2024.100582},
  url      = {https://www.sciencedirect.com/science/article/pii/S2452414X24000268},
  author   = {Julio C. Serrano-Ruiz and Josefa Mula and Raul Poler},
  keywords = {Smart manufacturing scheduling, Job shop, Digital twin, Deep reinforcement learning, Industry 4.0},
  abstract = {Smart manufacturing scheduling (SMS) requires a high degree of flexibility to successfully cope with changes in operational decision level planning processes in today's production environments, which are usually subject to high uncertainty. In such a unique and complex scenario as the real job shop, the modelling of SMS as a Markov decision process (MDP), and its approach by deep reinforcement learning (DRL), is a research field of growing interest given its characteristics. It allows us to consider achieving high flexibility levels by promoting process automation, autonomy in decision making, and the ability to act in real time when faced with disturbances and disruptions in a highly dynamic environment. This paper addresses the problem of scheduling a quasi-realistic job shop environment characterised by machines receiving jobs from buffers that accumulate numerous jobs using a wide variety of parts and multimachine routes with a diverse number of operation phases by developing a digital twin of the job shop based on a MDP with the DRL methodology. This is approached by: modelling the job shop scheduling environment with OpenAI Gym; designing an observation space with 18 job features; designing an action space composed of three priority heuristic rules; shaping a single reward function with a multi-objective characteristic; using the implementation of the proximal policy optimisation (PPO) algorithm from the Stable Baselines 3 library. This modelling approach, dubbed as job shop smart manufacturing scheduling (JS-SMS), is characterised by deterministic formulation and implementation. The model is subjected to validation by comparing it to several of the best-known heuristic priority rules. The main findings of this methodology allow to replicate, to a great extent, the positive aspects of heuristic rules and to mitigate the negative ones, which achieves more balanced behaviour in most of the measures established as performance indicators and outperforms heuristic rules from this multi-objective perspective. Finally, further research is oriented to dynamic and stochastic approaches to address the job shop reality in an Industry 4.0 context.}
}
@article{shannon1948mathematical,
  title     = {A mathematical theory of communication},
  author    = {Shannon, Claude Elwood},
  journal   = {The Bell system technical journal},
  volume    = {27},
  number    = {3},
  pages     = {379--423},
  year      = {1948},
  publisher = {Nokia Bell Labs}
}
@incollection{shapley1953value,
  title     = {A Value for n-Person Games},
  author    = {Shapley, Lloyd S.},
  booktitle = {Contributions to the Theory of Games, Volume II},
  editor    = {Kuhn, Harold W. and Tucker, Albert W.},
  year      = {1953},
  publisher = {Princeton University Press},
  address   = {Princeton},
  pages     = {307--317}
}
@inproceedings{shi2023explainable,
  title     = {An Explainable AI User Interface for Facilitating Collaboration between Domain Experts and AI Researchers.},
  author    = {Shi, Meng and Savur, Celal and Watkins, Elizabeth and Manuvinakurike, Ramesh and Mejia, Gesem Gudino and Beckwith, Richard and Raffa, Giuseppe},
  booktitle = {xAI (Late-breaking Work, Demos, Doctoral Consortium)},
  pages     = {112--116},
  year      = {2023}
}
@inproceedings{shrikumar2016not,
  title        = {Learning important features through propagating activation differences},
  author       = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  booktitle    = {International conference on machine learning},
  pages        = {3145--3153},
  year         = {2017},
  organization = {PMlR}
}
@article{singh2022rubric,
  title     = {A Rubric for Implementing Explainable AI in Production Logistics},
  author    = {Amita Singh and Erik Flores Garcia and Yongkuk Jeong and Magnus Wiktorsson},
  year      = 2022,
  journal   = {IFIP Advances in Information and Communication Technology},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  volume    = {663 Ifip},
  pages     = {190--197},
  doi       = {10.1007/978-3-031-16407-1\_23/tables/1},
  isbn      = 9783031164064,
  issn      = {1868422x},
  url       = {https://link.springer.com/chapter/10.1007/978-3-031-16407-1\_23},
  abstract  = {
               With the advent of Industry 4.0, the world is witnessing increasing use of
               data and data-driven services. This phenomenon has penetrated through
               different sectors of production including logistics. The purpose of this
               study is to explore the use of Artificial Intelligence (AI) and Machine
               Learning (ML) in production logistics. This paper is the first step in the
               direction of understanding the complexity of AI and ML algorithms and thus
               explaining the black-box-like characteristics of these algorithms. This is
               coupled with the definition of eXplainable AI (XAI) in the domain. The paper
               furthers describes the needs for XAI and consequently presents a rubric for
               implementing XAI in the domain of production logistics and discusses it in
               detail.
               },
  keywords  = {
               Explainability,Explainable artificial
               intelligence,Interpretability,Production logistics
               }
}

@article{slack2019fooling,
  title     = {Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods},
  author    = {
               Dylan Slack and Sophie Hilgard and Emily Jia and Sameer Singh and Himabindu
               Lakkaraju
               },
  year      = 2019,
  month     = 11,
  journal   = {
               AIES 2020 - Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society
               },
  publisher = {Association for Computing Machinery, Inc},
  pages     = {180--186},
  doi       = {10.1145/3375627.3375830},
  isbn      = 9781450371100,
  url       = {https://arxiv.org/abs/1911.02508v2},
  abstract  = {
               As machine learning black boxes are increasingly being deployed in domains
               such as healthcare and criminal justice, there is growing emphasis on
               building tools and techniques for explaining these black boxes in an
               interpretable manner. Such explanations are being leveraged by domain experts
               to diagnose systematic errors and underlying biases of black boxes. In this
               paper, we demonstrate that post hoc explanations techniques that rely on
               input perturbations, such as LIME and SHAP, are not reliable. Specifically,
               we propose a novel scaffolding technique that effectively hides the biases of
               any given classifier by allowing an adversarial entity to craft an arbitrary
               desired explanation. Our approach can be used to scaffold any biased
               classifier in such a way that its predictions on the input data distribution
               still remain biased, but the post hoc explanations of the scaffolded
               classifier look innocuous. Using extensive evaluation with multiple
               real-world datasets (including COMPAS), we demonstrate how extremely biased
               (racist) classifiers crafted by our framework can easily fool popular
               explanation techniques such as LIME and SHAP into generating innocuous
               explanations which do not reflect the underlying biases.
               },
  keywords  = {
               Adversarial attacks,Bias detection,Black box explanations,Model
               interpretability
               }
}

@article{srinivasan2020explanation,
  title    = {Explanation Perspectives from the Cognitive Sciences---A Survey},
  author   = {Ramya Srinivasan and Ajay Chander},
  year     = 2020,
  abstract = {
              With growing adoption of AI across fields such as healthcare, finance, and
              the justice system, explaining an AI decision has become more important than
              ever before. Development of human-centric explainable AI (XAI) systems
              necessitates an understanding of the requirements of the human-in-the-loop
              seeking the explanation. This includes the cognitive behavioral purpose that
              the explanation serves for its recipients, and the structure that the
              explanation uses to reach those ends. An understanding of the psychological
              foundations of explanations is thus vital for the development of effective
              human-centric XAI systems. Towards this end, we survey papers from the
              cognitive science literature that address the following broad questions: (1)
              what is an explanation, (2) what are explanations for, and 3) what are the
              characteristics of good and bad explanations. We organize the insights gained
              therein by means of highlighting the advantages and shortcomings of various
              explanation structures and theories, discuss their applicability across
              different domains, and analyze their utility to various types of
              humans-in-the-loop. We summarize the key takeaways for human-centric design
              of XAI systems, and recommend strategies to bridge the existing gap between
              XAI research and practical needs. We hope this work will spark the
              development of novel human-centric XAI systems.
              },
  keywords = {Human aspects in AI: general,Safe \& Explainable \& Trustworthy AI: general}
}

@misc{sundararajan2016gradients,
  title         = {Gradients of Counterfactuals},
  author        = {Mukund Sundararajan and Ankur Taly and Qiqi Yan},
  year          = {2016},
  eprint        = {1611.02639},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG}
}

@article{suresh2021expertise,
  title     = {
               Beyond expertise and roles: A framework to characterize the stakeholders of
               interpretable machine learning and their needs
               },
  author    = {Harini Suresh and Steven R. Gomez and Kevin K. Nam and Arvind Satyanarayan},
  year      = 2021,
  month     = 5,
  journal   = {Conference on Human Factors in Computing Systems - Proceedings},
  publisher = {Association for Computing Machinery},
  volume    = 16,
  doi       = {10.1145/3411764.3445088},
  isbn      = 9781450380966,
  url       = {https://dl.acm.org/doi/10.1145/3411764.3445088},
  abstract  = {
               To ensure accountability and mitigate harm, it is critical that diverse
               stakeholders can interrogate black-box automated systems and fnd information
               that is understandable, relevant, and useful to them. In this paper, we
               eschew prior expertise- and role-based categorizations of interpretability
               stakeholders in favor of a more granular framework that decouples
               stakeholders' knowledge from their interpretability needs. We characterize
               stakeholders by their formal, instrumental, and personal knowledge and how it
               manifests in the contexts of machine learning, the data domain, and the
               general milieu. We additionally distill a hierarchical typology of
               stakeholder needs that distinguishes higher-level domain goals from
               lower-level interpretability tasks. In assessing the descriptive, evaluative,
               and generative powers of our framework, we fnd our more nuanced treatment of
               stakeholders reveals gaps and opportunities in the interpretability
               literature, adds precision to the design and comparison of user studies, and
               facilitates a more refexive approach to conducting this research.
               },
  issue     = 21,
  keywords  = {
               Expertise,Explainability,Framework,Goals,Interpretability,Knowledge,Machine
               learning,Needs
               }
}

@book{Sutton2018,
  title     = {{Reinforcement Learning: An Introduction}},
  year      = {2018},
  author    = {Sutton, Richard S. and Barto, Andrew G.},
  publisher = {The MIT Press}
}

@book{sutton2018reinforcement,
  title     = {Reinforcement learning: An introduction},
  author    = {Sutton, Richard S and Barto, Andrew G},
  year      = {2018},
  publisher = {MIT press}
}

@article{taghipourdrl,
  author    = {Sharareh Taghipour, Hamed A. Namoura, Mani Sharifi and Mageed Ghaleb},
  title     = {Real-time production scheduling using a deep reinforcement learning-based multi-agent approach},
  journal   = {INFOR: Information Systems and Operational Research},
  volume    = {62},
  number    = {2},
  pages     = {186--210},
  year      = {2024},
  publisher = {Taylor \& Francis},
  doi       = {10.1080/03155986.2023.2287996},
  url       = { 
               
               https://doi.org/10.1080/03155986.2023.2287996
               
               
               
               },
  eprint    = { 
               
               https://doi.org/10.1080/03155986.2023.2287996
               
               
               
               }
}

@article{tchuente2024methodological,
  title     = {
               A methodological and theoretical framework for implementing explainable
               artificial intelligence (XAI) in business applications
               },
  author    = {Dieudonn\'{e} Tchuente and Jerry Lonlac and Bernard Kamsu-Foguem},
  year      = 2024,
  month     = 2,
  journal   = {Computers in Industry},
  publisher = {Elsevier},
  volume    = 155,
  pages     = 104044,
  doi       = {10.1016/j.compind.2023.104044},
  issn      = {0166-3615},
  note      = {Startpaper},
  abstract  = {
               Artificial Intelligence (AI) is becoming fundamental in almost all activity
               sectors in our society. However, most of the modern AI techniques (e.g.,
               Machine Learning – ML) have a black box nature, which hinder their adoption
               by practitioners in many application fields. This issue raises a recent
               emergence of a new research area in AI called Explainable artificial
               intelligence (XAI), aiming at providing AI-based decision-making processes
               and outcomes to be easily understood, interpreted, and justified by humans.
               Since 2018, there has been an exponential growth of research studies on XAI,
               which has justified some review studies. However, these reviews currently
               focus on proposing taxonomies of XAI methods. Yet, XAI is by nature a highly
               applicative research field, and beyond XAI methods, it is also very important
               to investigate how XAI is concretely used in industries, and consequently
               derive the best practices to follow for better implementations and adoptions.
               There is a lack of studies on this latter point. To fill this research gap,
               we first propose a holistic review of business applications of XAI, by
               following the Theory, Context, Characteristics, and Methodology (TCCM)
               protocol. Based on the findings of this review, we secondly propose a
               methodological and theoretical framework in six steps that can be followed by
               all practitioners or stakeholders for improving the implementation and
               adoption of XAI in their business applications. We particularly highlight the
               need to rely on domain field and analytical theories to explain the whole
               analytical process, from the relevance of the business question to the
               robustness checking and the validation of explanations provided by XAI
               methods. Finally, we propose seven important future research avenues.
               },
  keywords  = {
               Business applications,Deep learning,Explainable artificial
               intelligence,Framework,Interpretable artificial intelligence,Interpretable
               machine learning,Management,TCCM,XAI
               }
}

@article{tsymbal2004problem,
  title     = {The problem of concept drift: definitions and related work},
  author    = {Tsymbal, Alexey},
  year      = 2004,
  journal   = {Computer Science Department, Trinity College Dublin},
  publisher = {Citeseer},
  volume    = 106,
  number    = 2,
  pages     = 58
}

@book{tukey1977exploratory,
  title     = {Exploratory data analysis},
  author    = {Tukey, John Wilder et al.},
  volume    = {2},
  year      = {1977},
  publisher = {Springer}
}

@article{turner2022next,
  title     = {
               Next generation DES simulation: A research agenda for human centric
               manufacturing systems
               },
  author    = {Chris J. Turner and Wolfgang Garn},
  year      = 2022,
  month     = 7,
  journal   = {Journal of Industrial Information Integration},
  publisher = {Elsevier},
  volume    = 28,
  pages     = 100354,
  doi       = {10.1016/j.jii.2022.100354},
  issn      = {2452-414x},
  abstract  = {
               In this paper we introduce a research agenda to guide the development of the
               next generation of Discrete Event Simulation (DES) systems. Interfaces to
               digital twins are projected to go beyond physical representations to become
               blueprints for the actual ``objects'' and an active dashboard for their
               control. The role and importance of real-time interactive animations
               presented in an Extended Reality (XR) format will be explored. The need for
               using game engines, particularly their physics engines and AI within
               interactive simulated Extended Reality is expanded on. Importing and scanning
               real-world environments is assumed to become more efficient when using AR.
               Exporting to VR and AR is recommended to be a default feature. A technology
               framework for the next generation simulators is presented along with a
               proposed set of implementation guidelines. The need for more human centric
               technology approaches, nascent in Industry 4.0, are now central to the
               emerging Industry 5.0 paradigm; an agenda that is discussed in this research
               as part of a human in the loop future, supported by DES. The potential role
               of Explainable Artificial Intelligence is also explored along with an audit
               trail approach to provide a justification of complex and automated
               decision-making systems with relation to DES. A technology framework is
               proposed, which brings the above together and can serve as a guide for the
               next generation of holistic simulators for manufacturing.
               },
  keywords  = {
               Agent based simulation,Discrete event simulation (DES),Explainable artificial
               intelligence (XAI),Extended reality (XR),Human centric manufacturing,Human in
               the loop,Industry 4.0,Industry 5.0
               }
}



@article{tversky1974judgment,
  title     = {Judgment under Uncertainty: Heuristics and Biases: Biases in judgments reveal some heuristics of thinking under uncertainty.},
  author    = {Tversky, Amos and Kahneman, Daniel},
  journal   = {science},
  volume    = {185},
  number    = {4157},
  pages     = {1124--1131},
  year      = {1974},
  publisher = {American association for the advancement of science}
}

@article{vassiliadis2002conceptual,
  title     = {Conceptual modeling for ETL processes},
  author    = {Panos Vassiliadis and Alkis Simitsis and Spiros Skiadopoulos},
  year      = 2002,
  journal   = {ACM International Workshop on Data Warehousing and OLAP (DOLAP)},
  publisher = {Association for Computing Machinery (ACM)},
  pages     = {14--21},
  doi       = {10.1145/583890.583893},
  url       = {https://dl.acm.org/doi/10.1145/583890.583893},
  abstract  = {
               Extraction-Transformation-Loading (ETL) tools are pieces of software
               responsible for the extraction of data from several sources, their cleansing,
               customization and insertion into a data warehouse. In this paper, we focus on
               the problem of the definition of ETL activities and provide formal
               foundations for their conceptual representation. The proposed conceptual
               model is (a) customized for the tracing of inter-attribute relationships and
               the respective ETL activities in the early stages of a data warehouse
               project; (b) enriched with a 'palette' of a set of frequently used ETL
               activities, like the assignment of surrogate keys, the check for null values,
               etc; and (c) constructed in a customizable and extensible manner, so that the
               designer can enrich it with his own re-occurring patterns for ETL activities.
               },
  keywords  = {
               Conceptual modeling,Data warehousing,ETL,schema and subschema General Terms
               Design Keywords Data warehousing
               }
}

@article{vermeire2021how,
  title     = {
               How to~Choose an~Explainability Method? Towards a~Methodical Implementation
               of~XAI in~Practice
               },
  author    = {
               Tom Vermeire and Thibault Laugel and Xavier Renard and David Martens and
               Marcin Detyniecki
               },
  year      = 2021,
  journal   = {Communications in Computer and Information Science},
  publisher = {Springer Science and Business Media Deutschland GmbH},
  volume    = {1524 Ccis},
  pages     = {521--533},
  doi       = {10.1007/978-3-030-93736-2\_39/figures/1},
  isbn      = 9783030937355,
  issn      = 18650937,
  url       = {https://link.springer.com/chapter/10.1007/978-3-030-93736-2\_39},
  abstract  = {
               Explainability is becoming an important requirement for organizations that
               make use of automated decision-making due to regulatory initiatives and a
               shift in public awareness. Various and significantly different algorithmic
               methods to provide this explainability have been introduced in the field, but
               the existing literature in the machine learning community has paid little
               attention to the stakeholder whose needs are rather studied in the
               human-computer interface community. Therefore, organizations that want or
               need to provide this explainability are confronted with the selection of an
               appropriate method for their use case. In this paper, we argue there is a
               need for a methodology to bridge the gap between stakeholder needs and
               explanation methods. We present our ongoing work on creating this methodology
               to help data scientists in the process of providing explainability to
               stakeholders. In particular, our contributions include documents used to
               characterize XAI methods and user requirements (shown in Appendix), which our
               methodology builds upon.
               },
  keywords  = {
               Explainable artificial intelligence,Interpretable machine
               learning,Methodology,Stakeholder needs
               }
}

@article{watkins1992q,
  title     = {Q-learning},
  author    = {Watkins, Christopher JCH and Dayan, Peter},
  journal   = {Machine learning},
  volume    = {8},
  pages     = {279--292},
  year      = {1992},
  publisher = {Springer}
}

@article{wells2021explainable,
  title     = {Explainable ai and reinforcement learning—a systematic review of current approaches and trends},
  author    = {Wells, Lindsay and Bednarz, Tomasz},
  journal   = {Frontiers in artificial intelligence},
  volume    = {4},
  pages     = {550030},
  year      = {2021},
  publisher = {Frontiers Media SA}
}
@inproceedings{wirth2000crisp,
  title        = {CRISP-DM: Towards a standard process model for data mining},
  author       = {Wirth, R{\"u}diger and Hipp, Jochen},
  year         = 2000,
  booktitle    = {
                  Proceedings of the 4th international conference on the practical applications
                  of knowledge discovery and data mining
                  },
  volume       = 1,
  pages        = {29--39},
  organization = {Manchester}
}


%angepasst: 
@article{wooditch2021normal,
  title     = {The normal distribution and single-sample significance tests},
  author    = {Wooditch, Alese and Johnson, Nicole J and Solymosi, Reka and Medina Ariza, Juanjo and Langton, Samuel},
  journal   = {A Beginner’s Guide to Statistics for Criminology and Criminal Justice Using R},
  pages     = {155--168},
  year      = {2021},
  publisher = {Springer}
}



@article{wu2006effective,
  title     = {An effective application of decision tree to stock trading},
  author    = {Wu, Muh-Cherng and Lin, Sheng-Yu and Lin, Chia-Hsin},
  journal   = {Expert Systems with applications},
  volume    = {31},
  number    = {2},
  pages     = {270--274},
  year      = {2006},
  publisher = {Elsevier}
}

@article{xiong2024xrl,
  title   = {XRL-Bench: A Benchmark for Evaluating and Comparing Explainable Reinforcement Learning Techniques},
  author  = {Xiong, Yu and Hu, Zhipeng and Huang, Ye and Wu, Runze and Guan, Kai and Fang, Xingchen and Jiang, Ji and Zhou, Tianze and Hu, Yujing and Liu, Haoyu and others},
  journal = {arXiv preprint arXiv:2402.12685},
  year    = {2024}
}

@article{yang2003application,
  title     = {Application of decision tree technology for image classification using remote sensing data},
  author    = {Yang, Chun-Chieh and Prasher, Shiv O and Enright, Peter and Madramootoo, Chandra and Burgess, Magdalena and Goel, Pradeep K and Callum, Ian},
  journal   = {Agricultural Systems},
  volume    = {76},
  number    = {3},
  pages     = {1101--1117},
  year      = {2003},
  publisher = {Elsevier}
}

@inproceedings{yang2018explaining,
  title        = {Explaining therapy predictions with layer-wise relevance propagation in neural networks},
  author       = {Yang, Yinchong and Tresp, Volker and Wunderle, Marius and Fasching, Peter A},
  booktitle    = {2018 IEEE International Conference on Healthcare Informatics (ICHI)},
  pages        = {152--162},
  year         = {2018},
  organization = {IEEE}
}

@article{yuan2022empirical,
  title   = {An empirical study of the effect of background data size on the stability of SHapley Additive exPlanations (SHAP) for deep learning models},
  author  = {Yuan, Han and Liu, Mingxuan and Kang, Lican and Miao, Chenkui and Wu, Ying},
  journal = {arXiv preprint arXiv:2204.11351},
  year    = {2022}
}

@article{yurrita2022towards,
  title     = {
               Towards a multi-stakeholder value-based assessment framework for algorithmic
               systems
               },
  author    = {Mireia Yurrita and Dave Murray-Rust and Agathe Balayn and Alessandro Bozzon},
  year      = 2022,
  month     = 6,
  journal   = {ACM International Conference Proceeding Series},
  publisher = {Association for Computing Machinery},
  volume    = 22,
  pages     = {535--563},
  doi       = {10.1145/3531146.3533118},
  isbn      = 9781450393522,
  url       = {https://dl.acm.org/doi/10.1145/3531146.3533118},
  abstract  = {
               In an effort to regulate Machine Learning-driven (ML) systems, current
               auditing processes mostly focus on detecting harmful algorithmic biases.
               While these strategies have proven to be impactful, some values outlined in
               documents dealing with ethics in ML-driven systems are still underrepresented
               in auditing processes. Such unaddressed values mainly deal with contextual
               factors that cannot be easily quantified. In this paper, we develop a
               value-based assessment framework that is not limited to bias auditing and
               that covers prominent ethical principles for algorithmic systems. Our
               framework presents a circular arrangement of values with two bipolar
               dimensions that make common motivations and potential tensions explicit. In
               order to operationalize these high-level principles, values are then broken
               down into specific criteria and their manifestations. However, some of these
               value-specific criteria are mutually exclusive and require negotiation. As
               opposed to some other auditing frameworks that merely rely on ML researchers'
               and practitioners' input, we argue that it is necessary to include
               stakeholders that present diverse standpoints to systematically negotiate and
               consolidate value and criteria tensions. To that end, we map stakeholders
               with different insight needs, and assign tailored means for communicating
               value manifestations to them. We, therefore, contribute to current ML
               auditing practices with an assessment framework that visualizes closeness and
               tensions between values and we give guidelines on how to operationalize them,
               while opening up the evaluation and deliberation process to a wide range of
               stakeholders.
               },
  keywords  = {
               ML development and deployment pipeline,algorithm
               assessment,multi-stakeholder,values
               }
}


@inproceedings{zhang2019robust,
  title     = {Building robust machine learning systems: Current progress, research challenges, and opportunities},
  author    = {Zhang, Jeff Jun and Liu, Kang and Khalid, Faiq and Hanif, Muhammad Abdullah and Rehman, Semeen and Theocharides, Theocharis and Artussi, Alessandro and Shafique, Muhammad and Garg, Siddharth},
  booktitle = {Proceedings of the 56th Annual Design Automation Conference 2019},
  pages     = {1--4},
  year      = {2019}
}