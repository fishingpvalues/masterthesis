


\title[Demystifying RL in Production Scheduling via xAI]{Demystifying Reinforcement Learning in Production Scheduling via Explainable AI}


\author*[1]{\fnm{Daniel} \sur{Fischer}
%\orcid{0009-0004-9311-5436}
}
\email{daniel.fischer@hsbi.de}

\author[1]{\fnm{Hannah M.} \sur{Hüsener}
%\orcid{0000-0001-7544-7825}
}
\email{hannah\_maria.huesener@hsbi.de}

\author[1]{\fnm{Felix} \sur{Grumbach}}
\email{felix.grumbach@hsbi.de}

\author[1]{\fnm{Lukas} \sur{Vollenkemper}}
\email{lukas.vollenkemper@hsbi.de}

\author[2]{\fnm{Arthur} \sur{Müller}}
\email{arthur.mueller@iosb-ina.fraunhofer.de}

\author[1]{\fnm{Pascal} \sur{Reusch}}
\email{pascal.reusch@hsbi.de}

\affil[1]{
\orgdiv{Center for Applied Data Science (CfADS)},
\orgname{Hochschule Bielefeld}
\orgaddress{
%\street{Schulstr. 10}, 
\city{Gütersloh}, 
%\postcode{33330},
%\state{NW},
\country{Germany}}}
\affil[2]{
\orgdiv{Department of Machine Intelligence},
\orgname{Fraunhofer IOSB-INA},
\orgaddress{
%\street{Strenzfelder Allee 28},
\city{Lemgo},
%\postcode{06406}, 
%\state{ST}, 
\country{Germany}}}

\keywords{Deep Reinforcement Learning, Explainable AI (xAI), Production Scheduling, Captum, SHAP, Hypotheses-based workflow}

\abstract{
Deep Reinforcement Learning (DRL) is a frequently employed technique to solve scheduling problems. Although DRL agents ace at delivering viable results in short computing times, their reasoning remains opaque. We conduct a case study where we systematically apply two explainable AI (xAI) frameworks, namely SHAP (DeepSHAP) and Captum (Input x Gradient), to describe the reasoning behind scheduling decisions of a specialized DRL agent in a flow production. We find that methods in the xAI literature lack falsifiability and consistent terminology, do not adequately consider domain-knowledge, the target audience or real-world scenarios, and typically provide simple input-output explanations rather than causal interpretations. To resolve this issue, we introduce a hypotheses-based workflow. This approach enables us to inspect whether explanations align with domain knowledge and match the reward hypotheses of the agent. We furthermore tackle the challenge of communicating these insights to third parties by tailoring hypotheses to the target audience, which can serve as interpretations of the agent's behavior after verification. Our proposed workflow emphasizes the repeated verification of explanations and may be applicable to various DRL-based scheduling use cases.
}



\maketitle