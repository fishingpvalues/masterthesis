\section{Introduction}

\subsection{Background and Key Concepts}
Artificial Intelligence (AI) is transforming various business domains, with production scheduling emerging as a prominent application area. Scheduling is a critical decision-making process that “deals with the allocation of resources to tasks over given time periods” \shortcite[p. 1]{pinedo2012scheduling}. This involves establishing various rules, such as determining the order in which machines process jobs, to maximize efficiency and effectiveness in production workflows. While many algorithms provide high-quality scheduling solutions, their underlying reasoning often remains obscure \shortcite{heuillet2021explainability}. Gaining insights into these decision processes could yield significant benefits, including improved operational profits and streamlined workflows.

One of the ways to address the challenges of scheduling problems is through Reinforcement Learning (RL). In RL, an agent interacts with its environment and learns to make optimal decisions through experience. Typically, RL problems are framed as Markov Decision Processes (MDPs), defined by a tuple \((\mathcal{S}, \mathcal{A}, P, R, \gamma)\). Here, the set of states \(\mathcal{S}\) represents all possible situations the agent might encounter, with each state \(s_t\) encoding information about the environment at a given time step \(t\). The set of actions \(\mathcal{A}\) encompasses all possible choices available to the agent in any state. The transition probability function \(P\) captures the dynamics of the environment, defining the likelihood of moving from one state to another based on a chosen action. The reward function \(R\) provides immediate feedback by assigning a numerical value \(r_{t+1}\) to each action, indicating its desirability. Lastly, the discount factor \(\gamma\) (ranging from 0 to 1) helps balance the importance of immediate and future rewards in the agent's decision-making process \shortcite{Sutton2018}. RL agents serve as powerful decision-support tools, capable of determining near-optimal job sequences in minimal computing time, even for highly customized configurations, thus facilitating near real-time (re-)scheduling \shortcite{Grumbach2023Memetic}. To further optimize RL, the integration of deep learning techniques has proven particularly effective, leading to the development of Deep Reinforcement Learning (DRL) \shortcite{Kayhan2021, Grumbach2022Robust}.
DRL has emerged as a particularly effective approach for complex scheduling tasks, consistently outperforming traditional methods in handling high-dimensional state spaces \shortcite{Kayhan2021, Grumbach2022Robust}. By leveraging deep neural networks (DNNs) to approximate the functions that map states to actions and estimate the corresponding rewards, DRL agents can manage complex environments where conventional RL methods may fail. The integration of deep learning techniques enhances the scalability and flexibility of RL, enabling solutions to more sophisticated scheduling challenges \shortcite{Kayhan2021, Grumbach2022Robust}. Despite its impressive capabilities, DRL is often referred as a ``black box', obscuring the underlying motives and reasoning of the agent. This opacity poses challenges for companies and users, harming trust and adoption \shortcite{arrieta2020explainable}.
There is a pressing need for explanations that illustrate the inner workings of models like DNNs and DRL. This need has sped up the development of explainable AI (xAI), which aims to enhance understanding and transparency of these models. Additionally, xAI can reveal biases, assess prediction robustness, and support logical validation, balancing various stakeholder objectives \shortcite{heuillet2021explainability, dwivedi2023explainable, mohseni202124, bekkemoen2023explainable}. However, in manufacturing and scheduling contexts, xAI remains underutilized, with existing approaches often neglecting the domain knowledge of target users \shortcite{chen2023explainable, chen2023applications}.
Within the xAI domain, explainable Reinforcement Learning (xRL) \shortcite{puiutta2020explainable} focuses on explaining the actions of (D)RL agents during decision-making. However, research on xRL is relatively sparse, primarily due to its close ties to interpretable machine learning, which has overshadowed specific investigations in this area \shortcite{dazeley2023explainable}.

In manufacturing and scheduling, xRL must fulfill two crucial roles: convincing production planners and decision-makers of the agent's trustworthiness and ensuring the model's robustness \shortcite{heuillet2021explainability}. To achieve these objectives, explanations should be tailored to consider the domain, audience (including non-experts in AI), and application of the AI model, avoiding complex explanations that could alienate various stakeholders \shortcite{heuillet2021explainability, dwivedi2023explainable, bekkemoen2023explainable, chen2023explainable}.

To address these challenges, we analyze workflows for xAI in practice. Transparent workflows that keep management, engineers, production planners, and other stakeholders in mind are scarce \shortcite{Clement2023, tchuente2024methodological}. While xAI is gaining increasing scientific interest, a universal solution for making AI—specifically DRL—interpretable remains hidden \shortcite{heuillet2021explainability}. Additionally, many xAI methods currently lack falsifiability \cite{leavitt2020towards}.

\subsection{Contribution and Research Questions}
We adapt the holistic approach of \citeA{tchuente2024methodological} to guide empirical investigations with xAI in business applications. Our novel xAI approach involves formulating hypotheses in natural language based on domain knowledge, analyzing the reward function of the agent, and inspecting descriptive statistics of use-case data to create falsifiable explanations tailored to AI laymen in a real-world scheduling scenario.
Secondly, we combine two xAI methods and analyze their plausibility in our use case to ensure that the generated explanations can be effectively communicated within the procution domain.

The scope of the paper is to answer the following questions:
\begin{itemize}
    \item \textbf{(RQ1.1)} How can xAI methods, specifically DeepSHAP and Input x Gradient, be systematically implemented, applied, and validated to describe the decisions of a DRL agent in a real-world flow production context?
    \item \textbf{(RQ1.2)} How suitable are the chosen xAI methods for the specified use case and what are their advantages and disadvantages?
    \item \textbf{(RQ2.1)} How can a workflow be developed to integrate hypotheses derived from domain knowledge with xAI methods for scheduling applications, ensuring falsifiability?
    \item \textbf{(RQ2.2)} How can these xAI-explanations be processed and communicated to stakeholders using domain knowledge?
\end{itemize}