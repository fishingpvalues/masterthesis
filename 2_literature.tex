\section{Literature Review}
\label{literature}
We start by laying a common ground on RL in production and scheduling. Afterwards, we introduce xAI methods and workflows before we stress the research gap for falsifiable and systematic xAI approaches in production planning. In the following, we differentiate between frameworks (software tools which \textit{contain} xAI methods) and the methods themselves that can be mathematically formulated.

\subsection{Reinforcement Learning Applications in Production and Scheduling}
RL offers an efficient alternative to heuristic methods by leveraging ML to solve problems more effectively, particularly in environments with continuous feedback from sensors \shortcite{khadivi2023deeprlmanufacturing, sutton2018reinforcement}. Various approaches, such as Q-Learning \shortcite{watkins1992q} and DRL (\hyperlink{RL}{see Introduction}) \shortcite{mnih2015drl}, have proven effective for scheduling problems. DRL is especially advantageous in large state spaces where traditional RL algorithms fail. It even outperforms classical RL and Q-Learning in several scheduling tasks. For instance, \citeA{SERRANORUIZ2024100582} utilized OpenAI Gym to simulate a quasi-realistic job shop scheduling environment using a digital twin based on an MDP and DRL with the proximal policy optimization algorithm. This model incorporated a rich observation space and heuristic priority rules, showing superior multi-objective performance compared to traditional methods. The strength of DRL lies in its capacity to model complex state spaces, although its black-box nature poses challenges for interpretability. Even with well-crafted reward functions, the reasons behind the agent's decisions often remains unclear. Recent research focuses on improving the explainability of DRL models to show the hidden decision-making processes \shortcite{kuhnle2022explainable}.

\subsection{State of the Art xAI Frameworks and Methods}
Before discussing xAI approaches, key terminology must be clarified, as proposed by \citeA{palacio2021xai}. This helps discuss the different methodologies.
In this framework, an \textit{explanation} refers to the information provided to support human understanding. xAI \textit{methods} define how these explanations are generated, specifying inputs and outputs. \textit{Interpretation} refers to the meaning attached to the explanation, including the reasoning behind it, such as causal links or context \shortcite{palacio2021xai}.

Some researchers argue that in AI, interpretability implies lower model complexity, enabling components to be easily understood, as in small decision trees. Such models are considered ‘transparent.’ While xAI can enhance transparency during model development, post-hoc explainability is often used for black-box models, where additional algorithms explain outcomes after the model is executed \shortcite{mohseni202124, kim2021multi, heuillet2021explainability, milani2024explainable}.
xAI methods are categorized as model-specific or model-agnostic, and explanations as global or local. Model-specific methods apply to a particular algorithm, while model-agnostic methods can be used across different ML models. Explanations can address either the entire model (global) or specific predictions, such as local explanations for individual actions \shortcite{heuillet2021explainability, dwivedi2023explainable, mohseni202124, chen2023explainable, milani2024explainable}.

\subsubsection{The Choice of Explanation Type}
Various xAI methods generate different types of explanations tailored to specific use cases (see \citeA{mohseni202124} or \citeA{chen2023explainable}. The selection of explanation type largely depends on the target audience and their background knowledge. For instance, an AI expert may seek detailed explanations for debugging purposes, while laypersons might only require enough information to gauge the reliability of the AI system \shortcite{mohseni202124, kim2021multi, heuillet2021explainability, dwivedi2023explainable}. Researchers can utilize xAI to provide global explanations that describe the model's overall functioning \shortcite{mohseni202124, chen2023explainable}. In the context of xRL, certain approaches assist developers in debugging models by clarifying states, actions, and their outcomes \shortcite{dazeley2023explainable}. Example-based explanations focus on specific instances within the data, utilizing local explanations to elucidate a single action taken by the agent \shortcite{chen2023explainable}. However, such singular explanations have been criticized for inadequately conveying the workings of complex models \shortcite{leavitt2020towards}.
Other xAI types include what-if explanations, which illustrate how changes affect outputs; contrastive explanations, which clarify why certain outcomes did not occur (e.g., ``Why not another action?'); and counterfactual explanations, which indicate what modifications in the input or model would result in different outcomes (e.g., ``What needs to be observed for a different action?'). While these explanations are generally intuitive for humans, they can be challenging to apply in large state spaces or may require specifically generated features \shortcite{dazeley2023explainable, mohseni202124, kim2021multi, chen2023explainable}.
\textit{Why} explanations focus on elucidating the reasons behind a specific outcome based on input \shortcite{mohseni202124}. Within this category, feature-based techniques reveal how input features influence model output by assigning importance values \shortcite{dwivedi2023explainable}.

In the following sections, we will look into feature-based xAI methods, although this discussion will not deliver an exhaustive overview, as such is beyond the scope of this paper.

\subsubsection{xAI Methods for the Target Audience}

In selecting xAI methods, it is essential to consider both the goal and the target audience \shortcite{mohseni202124, kim2021multi, heuillet2021explainability, dwivedi2023explainable}. In our case, we are not focused on interpreting static model components or creating a self-explaining model, which is a general aim in some xAI research \shortcite{mohseni202124, kim2021multi}. Instead, we are utilizing an existing DRL model to explain (post-hoc) the rationale behind the agent's decisions. It's important to note that debugging or explaining the technical details of the entire model is not suited for our target audience \shortcite{mohseni202124, dazeley2023explainable}. Our focus is on domain experts in production scheduling, prioritizing user trust and acceptance. The objective is to provide straightforward explanations for why the DRL agent generated a specific production plan. Consequently, we avoid contrastive or counterfactual explanations, concentrating instead on the reasons behind specific production decisions—aligning with what \citeA{mohseni202124} refers to as 'why'-explanations. Given that features such as demand or buffer content are central to scheduling, it is intuitive to utilize them in generating domain-based explanations. Thus, we opt for feature-based xAI techniques that illustrate how input features impact model outputs through assigned importance values \shortcite{dwivedi2023explainable}. For RL in real-world applications, \citeA{heuillet2021explainability} recommend broad xAI approaches. We adhere to this by employing model-agnostic xAI frameworks, specifically SHAP and Captum.
In summary, 'why'-explanations focused on features in the data, combined with model-agnostic xAI frameworks, are most appropriate for our use case.

To offer alternative explanation styles that may benefit the explainee \shortcite{kim2021multi}, we selected one method — DeepSHAP — that emphasizes visualization, and another method — Input X Gradient — that is more quantifiable and can be represented in table format. We generate explanations for each class of action rather than examining single instances.

\paragraph{DeepSHAP}
The DeepSHAP explainer is speciﬁc for DL models, utilizing their compositional structure \shortcite{lundberg2017unified}. It builds on DeepLIFT \shortcite{shrikumar2016not} and SHAP \shortcite{lundberg2017unified}.
If the input features are independent, the neural network is linearized, shapley values from cooperative game theory \shortcite{shapley1953value} are chosen as attribution values and the reference value is taken to be E[x], DeepLIFT approximates SHAP values. SHAP values are "Shapley values of a conditional expectation function of the original model" \shortcite[p. 4]{lundberg2017unified}, where every feature is assigned an importance value that shows how the expected model prediction changes when conditioning on that specific feature. The starting point of this additive attribution is the so called base value, which represents the expected prediction if all feature values were unknown \shortcite{lundberg2017unified}. Making use of DeepLIFT’s form of back-propagation (multipliers \textit{m}), DeepSHAP combines the SHAP values for a feature i and the prediction y for the network components (e.g., \( f_3 \)) – that can be solved analytically if linear – into SHAP values for the entire network \shortcite{lundberg2017unified}.

$$\phi_i(f_3, y) \approx m_{y_{if_3}}(y_i - E[y_i])
$$

While SHAP simplifies computations by assuming feature independence, this can lead to unrealistic SHAP values for correlated features or non-linear relationships \shortcite{aas2021explaining,kamath2021explainable}. Another limitation of DeepSHAP lies in its dependence on the background input that must be chosen to compute the mean prediction \shortcite{fernando2019study}. Stability improves with larger background sample sizes \shortcite{yuan2022empirical} .
Recently, DeepSHAP has been used to explain the results of a DNN in the field of condition monitoring for hydraulic systems \shortcite{keleko2023health}.

\paragraph{Input X Gradient}
The Input X Gradient method (or \textit{gradient*input}) computes the contribution of each input \( x_i \) by taking the partial derivatives of the output with respect to the input and multiplying these derivatives by the input itself \shortcite{kindermans2016investigating}:

\[
  R_{i}^{c}(x) = x_i \cdot \frac{\partial S_c(x)}{\partial x_i}
\]

Here, \( R_{i}^{c}(x) \) represents the contribution of input \( x_i \), \( S_c(x) \) is the output function, and \( \frac{\partial S_c(x)}{\partial x_i} \) denotes the partial derivative of the output with respect to the input. This method enhances the information gained from the gradient by also detecting gradient saturation, making it more informative than considering gradients alone \shortcite{adebayo2018sanity}.
One advantage of Input X Gradient is its monitorability, allowing the definition of decision boundaries and critical thresholds for attribution values. For instance, if the attribution for a specific action violates the 68–95–99.7 rule \shortcite{wooditch2021normal}, an alert can be triggered. However, Input X Gradient is sensitive to scaling \shortcite{sundararajan2016gradients}. \citeA{leavitt2020towards} note that it may lead to unreliable explanations when examining single neuron activations, as it may falsely imply that a single neuron is \textit{causally} responsible for a specific agent action, ignoring the complex interactions among neurons within the network.
Applications of Input X Gradient include \citeA{chatterjee2024exploration}, who use it to color specific lung areas in patients potentially suffering from COVID-19, and \citeA{ozer2023explainable}, who apply it to x-ray images.

Since we are explaining an RL-model, we researched general xAI methods as well as specific methods in xRL. Different xRL methods exist, but as \citeA{heuillet2021explainability} point out many of those are not applicable to RL in real-world scenarios: One reason being that in RL, there are specific assumptions, algorithms and environments with different constrains to consider. The authors propose to focus research on global xAI approaches.
In fact, we find approaches in literature where DRL results were explained using general xAI methods (such as SHAP or even decision trees) that are not specific to RL \shortcite{10566218}. The next section will thus focus on application frameworks for xAI.

\subsubsection{Frameworks for xAI Workflow and Design Decisions}
Asides from the methods themselves, we must also consider the procedure of generating xAI in a complex business setting. This includes the workflow, design choices as well as other aspects to look out for. We draw on frameworks from both the xAI community as well as other domains.

\citeA{leavitt2020towards} point out that approaches of xAI, especially for deep learning, often rely on visualization or single examples and lack falsification via hypotheses, quantifiability and human as well as general verification of validity. \citeA{chen2023explainable} also highlight that xAI results need to be validated. This is something that should be kept in mind in the workflow and design of explanations, in order to ensure a scientific standard.
\citeA{dazeley2023explainable} highlight the importance of causality and introduce the Causal xRL Framework to create causal explanations in RL. They draw on two existing theories.
First, \citeA{dazeley2021levels} suggest that xRL can take place at different orders and that higher level explanations should be incorporated for acceptance of AI systems. Zero-order explanations only consider how an input resulted in an output (base case in xAI). First-order explanations that consider an agent’s objective to maximise a reward signal are also relevant. Explaining the agent's intentionality (e.g., the objectives behind the behavior) might improve understanding of the agent, in comparison to only giving zero-order explanations \shortcite{dazeley2021levels, dazeley2023explainable}.
Secondly, due to the temporal sequence of transitions in RL, \citeA{dazeley2023explainable} propose to provide causal explanations. They draw on the Casual Explanation Network by \citeA{bohm2015people} , which aims at explaining intentionality, cause and reason of behavior as well as implications for the future.
In our setting, the Causal xRL Framework can be understood as following: The agent is expected to make production decisions based on two objectives (e.g., goals). For this, the agent observes the state (perception, zero-order) and depending on the attributes of the features one objective might be prioritized (disposition, first-order). To fulfill the objective (first-order), the agent will then make an action that will in return cause an outcome (zero-order).
If an agent has more than one objective the explanation of disposition plays a role, which \citeA{dazeley2023explainable} point out as an outlook for future research.
Goal-driven xAI approaches are currently an emerging research field \shortcite{dazeley2023explainable}. However, it is mainly a focus in autonomous agents and robots, while research on xRL and reactive agents in particular is limited to policy retention without including domain knowledge \shortcite{sado2023explainable}.
\citeA{shi2023explainable} widen the circle of people involved by incorporating domain experts and researchers together. Their approach is suited for manufacturing and consists of a prototype of a task guiding system. With a graphical user interface they summarize information about different models and agent behaviour. This makes it easier for third parties to interact with the explanations. \citeA{langer2021what} build upon that and create a model to focus on the different \textit{stakeholder desiderata}. Their model classifies different stakeholders and their demands regarding xAI explanations. Different stakeholder classes like deployers, regulators, users and "affected" are presented. One key insight are the highly versatile expectations of the different stakeholders. These expectations are already affected by the different domain knowledge of the parties. The intersection of these interests has to be found to provide a reasonable xAI model.
\citeA{tchuente2024methodological} specifically reviewed xAI in business settings and proposed a workflow for xAI in business applications, which is discussed in detail in our approach \ref{systematic workflow}. As limitations of their workflow, the authors highlight that it has to be adapted to specific contexts and further work is need regarding robustness and validation of explanations. Later on, we directly take up these limitations with our approach.

\subsection{Research Gap}
Established frameworks for xAI in scheduling are scarce, and the number of xRL methods applicable in real-world scenarios is limited \shortcite{bekkemoen2023explainable, heuillet2021explainability}. While many xAI methods reviewed are suitable for debugging, they often fall short for non-AI experts. This raises the question of how xAI methods can effectively describe the decisions made by a DRL agent in a real-world flow production context (RQ 1.1) and how these methods can be systematically implemented, applied, and validated (RQ 1.2)\shortcite{chi2022quantitative}.
Many efforts in xRL only provide zero-order explanations, which are not comprehensive enough to create trust and acceptance in the AI system and only few approaches include utilizing an agent's objectives or dispositions to create casual explanations \shortcite{dazeley2021levels, dazeley2023explainable}. Adding domain knowledge to xRL is important \shortcite{milani2024explainable}, but considering it in xAI approaches for manufacturing is rare \citeA{chen2023explainable}. Here, the questions arise how domain knowledge as well as context can be included into an explanation. How can xAI-results be processed and presented to stakeholders using this knowledge (RQ 2.2)?
In the xAI community, researchers have criticized the lack of unified terminology \shortcite{palacio2021xai} as well as falsification and missing validity checks \shortcite{leavitt2020towards, chen2023explainable}. How can falsifiablity be ensured (RQ 1.1, 2.1)?

We fill this gap by developing a workflow based on hypotheses (RQ 2.1), utilizing the domain knowledge (RQ 2.2) of the users and knowledge of the agent's workings. The framework builds upon existing xAI methods and -frameworks and focuses on real world scheduling applications (RQ 1.1, 1.2).
